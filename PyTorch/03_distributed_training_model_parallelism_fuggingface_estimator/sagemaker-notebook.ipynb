{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface Sagemaker-sdk - Distributed Training Demo\n",
    "\n",
    "### Model Parallelism using `SageMakerTrainer` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)  \n",
    "2. [Development Environment and Permissions](#Development-Environment-and-Permissions)\n",
    "    1. [Installation](#Installation)  \n",
    "    2. [Development environment](#Development-environment)  \n",
    "    3. [Permissions](#Permissions)\n",
    "3. [Processing](#Preprocessing)   \n",
    "    1. [Tokenization](#Tokenization)  \n",
    "    2. [Uploading data to sagemaker_session_bucket](#Uploading-data-to-sagemaker_session_bucket)  \n",
    "4. [Fine-tuning & starting Sagemaker Training Job](#Fine-tuning-\\&-starting-Sagemaker-Training-Job)  \n",
    "    1. [Creating an Estimator and start a training job](#Creating-an-Estimator-and-start-a-training-job)  \n",
    "    2. [Estimator Parameters](#Estimator-Parameters)   \n",
    "    3. [Download fine-tuned model from s3](#Download-fine-tuned-model-from-s3)\n",
    "    3. [Attach to old training job to an estimator ](#Attach-to-old-training-job-to-an-estimator)  \n",
    "5. [_Coming soon_:Push model to the Hugging Face hub](#Push-model-to-the-Hugging-Face-hub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Welcome to our end-to-end distributed Text-Classification example. In this demo, we will use the Hugging Face `transformers` and `datasets` library together with a Amazon sagemaker-sdk extension to run GLUE `mnli` benchmark on a multi-node multi-gpu cluster using [SageMaker Model Parallelism Library](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-intro.html). The demo will use the new smdistributed library to run training on multiple gpus. We extended the `Trainer` API to a the `SageMakerTrainer` to use the model parallelism library. Therefore you only have to change the imports in your `train.py`.\n",
    "\n",
    "```python\n",
    "from transformers.sagemaker import SageMakerTrainingArguments as TrainingArguments\n",
    "from transformers.sagemaker import SageMakerTrainer as Trainer\n",
    "```\n",
    "\n",
    "_**NOTE: You can run this demo in Sagemaker Studio, your local machine or Sagemaker Notebook Instances**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development Environment and Permissions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "_*Note:* we only install the required libraries from Hugging Face and AWS. You also need PyTorch or Tensorflow, if you haven´t it installed_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install \"sagemaker>=2.48.0\" --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker.huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::815969174475:role/service-role/AmazonSageMaker-ExecutionRole-20190909T195854\n",
      "sagemaker bucket: sagemaker-us-east-1-815969174475\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = \"arn:aws:iam::815969174475:role/service-role/AmazonSageMaker-ExecutionRole-20190909T195854\"\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning & starting Sagemaker Training Job\n",
    "\n",
    "In order to create a sagemaker training job we need an `HuggingFace` Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. In a Estimator we define, which fine-tuning script should be used as `entry_point`, which `instance_type` should be used, which `hyperparameters` are passed in .....\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "huggingface_estimator = HuggingFace(entry_point='train.py',\n",
    "                            source_dir='./scripts',\n",
    "                            base_job_name='huggingface-sdk-extension',\n",
    "                            instance_type='ml.p3.2xlarge',\n",
    "                            instance_count=1,\n",
    "                            transformers_version='4.4',\n",
    "                            pytorch_version='1.6',\n",
    "                            py_version='py36',\n",
    "                            role=role,\n",
    "                            hyperparameters = {'epochs': 1,\n",
    "                                               'train_batch_size': 32,\n",
    "                                               'model_name':'distilbert-base-uncased'\n",
    "                                                })\n",
    "```\n",
    "\n",
    "When we create a SageMaker training job, SageMaker takes care of starting and managing all the required ec2 instances for us with the `huggingface` container, uploads the provided fine-tuning script `train.py` and downloads the data from our `sagemaker_session_bucket` into the container at `/opt/ml/input/data`. Then, it starts the training job by running. \n",
    "\n",
    "```python\n",
    "/opt/conda/bin/python train.py --epochs 1 --model_name distilbert-base-uncased --train_batch_size 32\n",
    "```\n",
    "\n",
    "The `hyperparameters` you define in the `HuggingFace` estimator are passed in as named arguments. \n",
    "\n",
    "Sagemaker is providing useful properties about the training environment through various environment variables, including the following:\n",
    "\n",
    "* `SM_MODEL_DIR`: A string that represents the path where the training job writes the model artifacts to. After training, artifacts in this directory are uploaded to S3 for model hosting.\n",
    "\n",
    "* `SM_NUM_GPUS`: An integer representing the number of GPUs available to the host.\n",
    "\n",
    "* `SM_CHANNEL_XXXX:` A string that represents the path to the directory that contains the input data for the specified channel. For example, if you specify two input channels in the HuggingFace estimator’s fit call, named `train` and `test`, the environment variables `SM_CHANNEL_TRAIN` and `SM_CHANNEL_TEST` are set.\n",
    "\n",
    "\n",
    "To run your training job locally you can define `instance_type='local'` or `instance_type='local_gpu'` for gpu usage. _Note: this does not working within SageMaker Studio_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Estimator and start a training job\n",
    "\n",
    "In this example we are going to use the `run_glue.py` from the transformers example scripts. We modified it and included `SageMakerTrainer` instead of the `Trainer` to enable model-parallelism. You can find the code [here](https://github.com/huggingface/transformers/tree/master/examples/pytorch/text-classification).\n",
    "\n",
    "```python\n",
    "from transformers.sagemaker import SageMakerTrainingArguments as TrainingArguments, SageMakerTrainer as Trainer\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={\n",
    "    'model_name_or_path':'roberta-large',\n",
    "    'task_name': 'mnli',\n",
    "    'per_device_train_batch_size': 16,\n",
    "    'per_device_eval_batch_size': 16,\n",
    "    'do_train': True,\n",
    "    'do_eval': True,\n",
    "    'do_predict': True,\n",
    "    'num_train_epochs': 2,\n",
    "    'output_dir':'/opt/ml/model',\n",
    "    'max_steps': 500,\n",
    "}\n",
    "\n",
    "# configuration for running training on smdistributed Model Parallel\n",
    "mpi_options = {\n",
    "    \"enabled\" : True,\n",
    "    \"processes_per_host\" : 8,\n",
    "}\n",
    "smp_options = {\n",
    "    \"enabled\":True,\n",
    "    \"parameters\": {\n",
    "        \"microbatches\": 4,\n",
    "        \"placement_strategy\": \"spread\",\n",
    "        \"pipeline\": \"interleaved\",\n",
    "        \"optimize\": \"speed\",\n",
    "        \"partitions\": 4,\n",
    "        \"ddp\": True,\n",
    "    }\n",
    "}\n",
    "\n",
    "distribution={\n",
    "    \"smdistributed\": {\"modelparallel\": smp_options},\n",
    "    \"mpi\": mpi_options\n",
    "}\n",
    "\n",
    "# git configuration to download our fine-tuning script\n",
    "git_config = {'repo': 'https://github.com/huggingface/transformers.git','branch': 'v4.6.1'}\n",
    "\n",
    "# instance configurations\n",
    "instance_type='ml.p3.16xlarge'\n",
    "instance_count=1\n",
    "volume_size=200\n",
    "\n",
    "# metric definition to extract the results\n",
    "metric_definitions=[\n",
    "     {'Name': 'train_runtime', 'Regex':\"train_runtime.*=\\D*(.*?)$\"},\n",
    "     {'Name': 'train_samples_per_second', 'Regex': \"train_samples_per_second.*=\\D*(.*?)$\"},\n",
    "     {'Name': 'epoch', 'Regex': \"epoch.*=\\D*(.*?)$\"},\n",
    "     {'Name': 'f1', 'Regex': \"f1.*=\\D*(.*?)$\"},\n",
    "     {'Name': 'exact_match', 'Regex': \"exact_match.*=\\D*(.*?)$\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimator\n",
    "huggingface_estimator = HuggingFace(entry_point='run_glue.py',\n",
    "                                    source_dir='./examples/pytorch/text-classification',\n",
    "                                    git_config=git_config,\n",
    "                                    metrics_definition=metric_definitions,\n",
    "                                    instance_type=instance_type,\n",
    "                                    instance_count=instance_count,\n",
    "                                    volume_size=volume_size,\n",
    "                                    role=role,\n",
    "                                    transformers_version='4.6',\n",
    "                                    pytorch_version='1.7',\n",
    "                                    py_version='py36',\n",
    "                                    distribution= distribution,\n",
    "                                    hyperparameters = hyperparameters,\n",
    "                                    debugger_hook_config=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name_or_path': '\"roberta-large\"',\n",
       " 'task_name': '\"mnli\"',\n",
       " 'per_device_train_batch_size': '16',\n",
       " 'per_device_eval_batch_size': '16',\n",
       " 'do_train': 'true',\n",
       " 'do_eval': 'true',\n",
       " 'do_predict': 'true',\n",
       " 'num_train_epochs': '2',\n",
       " 'output_dir': '\"/opt/ml/model\"',\n",
       " 'max_steps': '500',\n",
       " 'sagemaker_mpi_enabled': 'true',\n",
       " 'sagemaker_mpi_num_of_processes_per_host': '8',\n",
       " 'sagemaker_mpi_custom_mpi_options': '\"\"',\n",
       " 'mp_parameters': '{\"microbatches\": 4, \"placement_strategy\": \"spread\", \"pipeline\": \"interleaved\", \"optimize\": \"speed\", \"partitions\": 4, \"ddp\": true}',\n",
       " 'sagemaker_distributed_dataparallel_enabled': 'false',\n",
       " 'sagemaker_instance_type': '\"ml.p3.16xlarge\"'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_estimator.hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into '/tmp/tmpjmr959qx'...\n",
      "Note: checking out 'v4.6.1'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by performing another checkout.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -b with the checkout command again. Example:\n",
      "\n",
      "  git checkout -b <new-branch-name>\n",
      "\n",
      "HEAD is now at fb27b276e Release: v4.6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-02 13:38:58 Starting - Starting the training job...ProfilerReport-1651498737: InProgress\n",
      "...\n",
      "2022-05-02 13:39:56 Starting - Preparing the instances for training............\n",
      "2022-05-02 13:41:43 Downloading - Downloading input data\n",
      "2022-05-02 13:41:43 Training - Downloading the training image..................\n",
      "2022-05-02 13:44:52 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-05-02 13:44:55,341 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-05-02 13:44:55,418 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-05-02 13:44:55,425 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-05-02 13:44:55,845 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting accelerate\n",
      "  Downloading accelerate-0.7.1-py3-none-any.whl (79 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets>=1.1.3 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (1.6.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (0.1.91)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (3.17.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.3 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 5)) (1.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (2.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=1.0.0<4.0.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (4.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (0.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (0.70.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (1.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (20.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (4.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (2.25.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (2021.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm<4.50.0,>=4.27 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (4.49.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<0.1.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (0.0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from torch>=1.3->-r requirements.txt (line 5)) (3.10.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from huggingface-hub<0.1.0->datasets>=1.1.3->-r requirements.txt (line 2)) (3.0.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 2)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 2)) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 2)) (2020.12.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 2)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.6/site-packages (from accelerate->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.9 in /opt/conda/lib/python3.6/site-packages (from protobuf->-r requirements.txt (line 4)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->datasets>=1.1.3->-r requirements.txt (line 2)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->datasets>=1.1.3->-r requirements.txt (line 2)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets>=1.1.3->-r requirements.txt (line 2)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets>=1.1.3->-r requirements.txt (line 2)) (2021.1)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: accelerate\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.7.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2022-05-02 13:44:58,894 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2022-05-02 13:44:58,894 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2022-05-02 13:44:58,897 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2022-05-02 13:44:58,897 sagemaker-training-toolkit INFO     Env Hosts: ['algo-1'] Hosts: ['algo-1:8'] process_per_hosts: 8 num_processes: 8\u001b[0m\n",
      "\u001b[34m2022-05-02 13:44:58,898 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2022-05-02 13:44:58,975 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": false,\n",
      "        \"sagemaker_instance_type\": \"ml.p3.16xlarge\",\n",
      "        \"sagemaker_mpi_custom_mpi_options\": \"\",\n",
      "        \"sagemaker_mpi_enabled\": true,\n",
      "        \"sagemaker_mpi_num_of_processes_per_host\": 8\n",
      "    },\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"do_eval\": true,\n",
      "        \"do_predict\": true,\n",
      "        \"do_train\": true,\n",
      "        \"max_steps\": 500,\n",
      "        \"model_name_or_path\": \"roberta-large\",\n",
      "        \"mp_parameters\": {\n",
      "            \"microbatches\": 4,\n",
      "            \"placement_strategy\": \"spread\",\n",
      "            \"pipeline\": \"interleaved\",\n",
      "            \"optimize\": \"speed\",\n",
      "            \"partitions\": 4,\n",
      "            \"ddp\": true\n",
      "        },\n",
      "        \"num_train_epochs\": 2,\n",
      "        \"output_dir\": \"/opt/ml/model\",\n",
      "        \"per_device_eval_batch_size\": 16,\n",
      "        \"per_device_train_batch_size\": 16,\n",
      "        \"task_name\": \"mnli\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2022-05-02-13-38-52-867\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-815969174475/huggingface-pytorch-training-2022-05-02-13-38-52-867/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_glue\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.16xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.16xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_glue.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"do_eval\":true,\"do_predict\":true,\"do_train\":true,\"max_steps\":500,\"model_name_or_path\":\"roberta-large\",\"mp_parameters\":{\"ddp\":true,\"microbatches\":4,\"optimize\":\"speed\",\"partitions\":4,\"pipeline\":\"interleaved\",\"placement_strategy\":\"spread\"},\"num_train_epochs\":2,\"output_dir\":\"/opt/ml/model\",\"per_device_eval_batch_size\":16,\"per_device_train_batch_size\":16,\"task_name\":\"mnli\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_glue.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p3.16xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.16xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_glue\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-815969174475/huggingface-pytorch-training-2022-05-02-13-38-52-867/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p3.16xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"do_eval\":true,\"do_predict\":true,\"do_train\":true,\"max_steps\":500,\"model_name_or_path\":\"roberta-large\",\"mp_parameters\":{\"ddp\":true,\"microbatches\":4,\"optimize\":\"speed\",\"partitions\":4,\"pipeline\":\"interleaved\",\"placement_strategy\":\"spread\"},\"num_train_epochs\":2,\"output_dir\":\"/opt/ml/model\",\"per_device_eval_batch_size\":16,\"per_device_train_batch_size\":16,\"task_name\":\"mnli\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-pytorch-training-2022-05-02-13-38-52-867\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-815969174475/huggingface-pytorch-training-2022-05-02-13-38-52-867/source/sourcedir.tar.gz\",\"module_name\":\"run_glue\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.16xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_glue.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--do_eval\",\"True\",\"--do_predict\",\"True\",\"--do_train\",\"True\",\"--max_steps\",\"500\",\"--model_name_or_path\",\"roberta-large\",\"--mp_parameters\",\"ddp=True,microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\",\"--num_train_epochs\",\"2\",\"--output_dir\",\"/opt/ml/model\",\"--per_device_eval_batch_size\",\"16\",\"--per_device_train_batch_size\",\"16\",\"--task_name\",\"mnli\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_DO_EVAL=true\u001b[0m\n",
      "\u001b[34mSM_HP_DO_PREDICT=true\u001b[0m\n",
      "\u001b[34mSM_HP_DO_TRAIN=true\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_STEPS=500\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=roberta-large\u001b[0m\n",
      "\u001b[34mSM_HP_MP_PARAMETERS={\"ddp\":true,\"microbatches\":4,\"optimize\":\"speed\",\"partitions\":4,\"pipeline\":\"interleaved\",\"placement_strategy\":\"spread\"}\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=2\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=16\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=16\u001b[0m\n",
      "\u001b[34mSM_HP_TASK_NAME=mnli\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1:8 -np 8 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/opt/conda/lib/python3.6/site-packages/gethostname.cpython-36m-x86_64-linux-gnu.so -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_HP_DO_EVAL -x SM_HP_DO_PREDICT -x SM_HP_DO_TRAIN -x SM_HP_MAX_STEPS -x SM_HP_MODEL_NAME_OR_PATH -x SM_HP_MP_PARAMETERS -x SM_HP_NUM_TRAIN_EPOCHS -x SM_HP_OUTPUT_DIR -x SM_HP_PER_DEVICE_EVAL_BATCH_SIZE -x SM_HP_PER_DEVICE_TRAIN_BATCH_SIZE -x SM_HP_TASK_NAME -x PYTHONPATH /opt/conda/bin/python3.6 -m mpi4py run_glue.py --do_eval True --do_predict True --do_train True --max_steps 500 --model_name_or_path roberta-large --mp_parameters ddp=True,microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread --num_train_epochs 2 --output_dir /opt/ml/model --per_device_eval_batch_size 16 --per_device_train_batch_size 16 --task_name mnli\u001b[0m\n",
      "\u001b[34m Data for JOB [41157,1] offset 0 Total slots allocated 8\n",
      " ========================   JOB MAP   ========================\n",
      " Data for node: algo-1#011Num slots: 8#011Max slots: 0#011Num procs: 8\n",
      " #011Process OMPI jobid: [41157,1] App: 0 Process rank: 0 Bound: N/A\n",
      " #011Process OMPI jobid: [41157,1] App: 0 Process rank: 1 Bound: N/A\n",
      " #011Process OMPI jobid: [41157,1] App: 0 Process rank: 2 Bound: N/A\n",
      " #011Process OMPI jobid: [41157,1] App: 0 Process rank: 3 Bound: N/A\n",
      " #011Process OMPI jobid: [41157,1] App: 0 Process rank: 4 Bound: N/A\n",
      " #011Process OMPI jobid: [41157,1] App: 0 Process rank: 5 Bound: N/A\n",
      " #011Process OMPI jobid: [41157,1] App: 0 Process rank: 6 Bound: N/A\n",
      " #011Process OMPI jobid: [41157,1] App: 0 Process rank: 7 Bound: N/A\n",
      " =============================================================\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:40 [0] NCCL INFO Bootstrap : Using [0]eth0:10.2.89.240<0>\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:40 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:40 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:40 [0] NCCL INFO NET/Socket : Using [0]eth0:10.2.89.240<0>\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:40 [0] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:NCCL version 2.7.8+cuda11.0\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:41:41 [1] NCCL INFO Bootstrap : Using [0]eth0:10.2.89.240<0>\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:42 [2] NCCL INFO Bootstrap : Using [0]eth0:10.2.89.240<0>\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:43:43 [3] NCCL INFO Bootstrap : Using [0]eth0:10.2.89.240<0>\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Bootstrap : Using [0]eth0:10.2.89.240<0>\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:46 [6] NCCL INFO Bootstrap : Using [0]eth0:10.2.89.240<0>\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:41:41 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:42 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:43:43 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:46 [6] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:43:43 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:42 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:41:41 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:46 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:42 [2] NCCL INFO NET/Socket : Using [0]eth0:10.2.89.240<0>\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:42 [2] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:41:41 [1] NCCL INFO NET/Socket : Using [0]eth0:10.2.89.240<0>\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:43:43 [3] NCCL INFO NET/Socket : Using [0]eth0:10.2.89.240<0>\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO NET/Socket : Using [0]eth0:10.2.89.240<0>\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:46 [6] NCCL INFO NET/Socket : Using [0]eth0:10.2.89.240<0>\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:41:41 [1] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:43:43 [3] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:46 [6] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:45:45 [5] NCCL INFO Bootstrap : Using [0]eth0:10.2.89.240<0>\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:45:45 [5] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:45:45 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:45:45 [5] NCCL INFO NET/Socket : Using [0]eth0:10.2.89.240<0>\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:45:45 [5] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:47:47 [7] NCCL INFO Bootstrap : Using [0]eth0:10.2.89.240<0>\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:47:47 [7] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:47:47 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:47:47 [7] NCCL INFO NET/Socket : Using [0]eth0:10.2.89.240<0>\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:47:47 [7] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:43:789 [3] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:43:789 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:43:789 [3] NCCL INFO Trees [0] 2/-1/-1->3->0|0->3->2/-1/-1 [1] 2/-1/-1->3->0|0->3->2/-1/-1 [2] -1/-1/-1->3->2|2->3->-1/-1/-1 [3] -1/-1/-1->3->2|2->3->-1/-1/-1 [4] 7/-1/-1->3->1|1->3->7/-1/-1 [5] 1/-1/-1->3->7|7->3->1/-1/-1 [6] 2/-1/-1->3->0|0->3->2/-1/-1 [7] 2/-1/-1->3->0|0->3->2/-1/-1 [8] -1/-1/-1->3->2|2->3->-1/-1/-1 [9] -1/-1/-1->3->2|2->3->-1/-1/-1 [10] 7/-1/-1->3->1|1->3->7/-1/-1 [11] 1/-1/-1->3->7|7->3->1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:790 [4] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:790 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:790 [4] NCCL INFO Trees [0] -1/-1/-1->4->7|7->4->-1/-1/-1 [1] -1/-1/-1->4->7|7->4->-1/-1/-1 [2] 7/-1/-1->4->0|0->4->7/-1/-1 [3] 7/-1/-1->4->0|0->4->7/-1/-1 [4] 6/-1/-1->4->5|5->4->6/-1/-1 [5] 5/-1/-1->4->6|6->4->5/-1/-1 [6] -1/-1/-1->4->7|7->4->-1/-1/-1 [7] -1/-1/-1->4->7|7->4->-1/-1/-1 [8] 7/-1/-1->4->0|0->4->7/-1/-1 [9] 7/-1/-1->4->0|0->4->7/-1/-1 [10] 6/-1/-1->4->5|5->4->6/-1/-1 [11] 5/-1/-1->4->6|6->4->5/-1/-1\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:45:792 [5] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:45:792 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:47:793 [7] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:791 [6] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:45:792 [5] NCCL INFO Trees [0] 6/-1/-1->5->1|1->5->6/-1/-1 [1] 6/-1/-1->5->1|1->5->6/-1/-1 [2] 1/-1/-1->5->6|6->5->1/-1/-1 [3] 1/-1/-1->5->6|6->5->1/-1/-1 [4] 4/-1/-1->5->7|7->5->4/-1/-1 [5] 7/-1/-1->5->4|4->5->7/-1/-1 [6] 6/-1/-1->5->1|1->5->6/-1/-1 [7] 6/-1/-1->5->1|1->5->6/-1/-1 [8] 1/-1/-1->5->6|6->5->1/-1/-1 [9] 1/-1/-1->5->6|6->5->1/-1/-1 [10] 4/-1/-1->5->7|7->5->4/-1/-1 [11] 7/-1/-1->5->4|4->5->7/-1/-1\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:791 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:47:793 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:791 [6] NCCL INFO Trees [0] 7/-1/-1->6->5|5->6->7/-1/-1 [1] 7/-1/-1->6->5|5->6->7/-1/-1 [2] 5/-1/-1->6->7|7->6->5/-1/-1 [3] 5/-1/-1->6->7|7->6->5/-1/-1 [4] 2/-1/-1->6->4|4->6->2/-1/-1 [5] 4/-1/-1->6->2|2->6->4/-1/-1 [6] 7/-1/-1->6->5|5->6->7/-1/-1 [7] 7/-1/-1->6->5|5->6->7/-1/-1 [8] 5/-1/-1->6->7|7->6->5/-1/-1 [9] 5/-1/-1->6->7|7->6->5/-1/-1 [10] 2/-1/-1->6->4|4->6->2/-1/-1 [11] 4/-1/-1->6->2|2->6->4/-1/-1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:47:793 [7] NCCL INFO Trees [0] 4/-1/-1->7->6|6->7->4/-1/-1 [1] 4/-1/-1->7->6|6->7->4/-1/-1 [2] 6/-1/-1->7->4|4->7->6/-1/-1 [3] 6/-1/-1->7->4|4->7->6/-1/-1 [4] 5/-1/-1->7->3|3->7->5/-1/-1 [5] 3/-1/-1->7->5|5->7->3/-1/-1 [6] 4/-1/-1->7->6|6->7->4/-1/-1 [7] 4/-1/-1->7->6|6->7->4/-1/-1 [8] 6/-1/-1->7->4|4->7->6/-1/-1 [9] 6/-1/-1->7->4|4->7->6/-1/-1 [10] 5/-1/-1->7->3|3->7->5/-1/-1 [11] 3/-1/-1->7->5|5->7->3/-1/-1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:786 [0] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:787 [2] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:41:788 [1] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:786 [0] NCCL INFO Channel 00/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:41:788 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:786 [0] NCCL INFO Channel 01/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:786 [0] NCCL INFO Channel 02/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:41:788 [1] NCCL INFO Trees [0] 5/-1/-1->1->2|2->1->5/-1/-1 [1] 5/-1/-1->1->2|2->1->5/-1/-1 [2] 2/-1/-1->1->5|5->1->2/-1/-1 [3] 2/-1/-1->1->5|5->1->2/-1/-1 [4] 3/-1/-1->1->0|0->1->3/-1/-1 [5] -1/-1/-1->1->3|3->1->-1/-1/-1 [6] 5/-1/-1->1->2|2->1->5/-1/-1 [7] 5/-1/-1->1->2|2->1->5/-1/-1 [8] 2/-1/-1->1->5|5->1->2/-1/-1 [9] 2/-1/-1->1->5|5->1->2/-1/-1 [10] 3/-1/-1->1->0|0->1->3/-1/-1 [11] -1/-1/-1->1->3|3->1->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:787 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:787 [2] NCCL INFO Trees [0] 1/-1/-1->2->3|3->2->1/-1/-1 [1] 1/-1/-1->2->3|3->2->1/-1/-1 [2] 3/-1/-1->2->1|1->2->3/-1/-1 [3] 3/-1/-1->2->1|1->2->3/-1/-1 [4] -1/-1/-1->2->6|6->2->-1/-1/-1 [5] 6/-1/-1->2->0|0->2->6/-1/-1 [6] 1/-1/-1->2->3|3->2->1/-1/-1 [7] 1/-1/-1->2->3|3->2->1/-1/-1 [8] 3/-1/-1->2->1|1->2->3/-1/-1 [9] 3/-1/-1->2->1|1->2->3/-1/-1 [10] -1/-1/-1->2->6|6->2->-1/-1/-1 [11] 6/-1/-1->2->0|0->2->6/-1/-1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:786 [0] NCCL INFO Channel 03/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:786 [0] NCCL INFO Channel 04/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:786 [0] NCCL INFO Channel 05/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:786 [0] NCCL INFO Channel 06/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:786 [0] NCCL INFO Channel 07/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:786 [0] NCCL INFO Channel 08/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:786 [0] NCCL INFO Channel 09/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:786 [0] NCCL INFO Channel 10/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:786 [0] NCCL INFO Channel 11/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:786 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:786 [0] NCCL INFO Trees [0] 3/-1/-1->0->-1|-1->0->3/-1/-1 [1] 3/-1/-1->0->-1|-1->0->3/-1/-1 [2] 4/-1/-1->0->-1|-1->0->4/-1/-1 [3] 4/-1/-1->0->-1|-1->0->4/-1/-1 [4] 1/-1/-1->0->-1|-1->0->1/-1/-1 [5] 2/-1/-1->0->-1|-1->0->2/-1/-1 [6] 3/-1/-1->0->-1|-1->0->3/-1/-1 [7] 3/-1/-1->0->-1|-1->0->3/-1/-1 [8] 4/-1/-1->0->-1|-1->0->4/-1/-1 [9] 4/-1/-1->0->-1|-1->0->4/-1/-1 [10] 1/-1/-1->0->-1|-1->0->1/-1/-1 [11] 2/-1/-1->0->-1|-1->0->2/-1/-1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:43:789 [3] NCCL INFO Channel 00 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:45:792 [5] NCCL INFO Channel 00 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:790 [4] NCCL INFO Channel 00 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:791 [6] NCCL INFO Channel 00 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:47:793 [7] NCCL INFO Channel 00 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:787 [2] NCCL INFO Channel 00 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:41:788 [1] NCCL INFO Channel 00 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:786 [0] NCCL INFO Channel 00 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:790 [4] NCCL INFO Channel 00 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:43:789 [3] NCCL INFO Channel 00 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:45:792 [5] NCCL INFO Channel 00 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:791 [6] NCCL INFO Channel 00 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:47:793 [7] NCCL INFO Channel 00 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:41:788 [1] NCCL INFO Channel 00 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:787 [2] NCCL INFO Channel 00 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:790 [4] NCCL INFO Channel 01 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:786 [0] NCCL INFO Channel 01 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:43:789 [3] NCCL INFO Channel 01 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:45:792 [5] NCCL INFO Channel 01 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:791 [6] NCCL INFO Channel 01 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:47:793 [7] NCCL INFO Channel 01 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:41:788 [1] NCCL INFO Channel 01 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:787 [2] NCCL INFO Channel 01 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:790 [4] NCCL INFO Channel 01 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:43:789 [3] NCCL INFO Channel 01 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:45:792 [5] NCCL INFO Channel 01 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:791 [6] NCCL INFO Channel 01 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:47:793 [7] NCCL INFO Channel 01 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:41:788 [1] NCCL INFO Channel 01 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:787 [2] NCCL INFO Channel 01 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:790 [4] NCCL INFO Channel 02 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:786 [0] NCCL INFO Channel 02 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:45:792 [5] NCCL INFO Channel 02 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:43:789 [3] NCCL INFO Channel 02 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:791 [6] NCCL INFO Channel 02 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:47:793 [7] NCCL INFO Channel 02 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:41:788 [1] NCCL INFO Channel 02 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:787 [2] NCCL INFO Channel 02 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:43:789 [3] NCCL INFO Channel 02 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:790 [4] NCCL INFO Channel 02 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:45:792 [5] NCCL INFO Channel 02 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:791 [6] NCCL INFO Channel 02 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:47:793 [7] NCCL INFO Channel 02 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:41:788 [1] NCCL INFO Channel 02 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:787 [2] NCCL INFO Channel 02 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:786 [0] NCCL INFO Channel 03 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:43:789 [3] NCCL INFO Channel 03 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:790 [4] NCCL INFO Channel 03 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:45:792 [5] NCCL INFO Channel 03 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:47:793 [7] NCCL INFO Channel 03 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:791 [6] NCCL INFO Channel 03 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:41:788 [1] NCCL INFO Channel 03 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:787 [2] NCCL INFO Channel 03 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:43:789 [3] NCCL INFO Channel 03 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:790 [4] NCCL INFO Channel 03 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:45:792 [5] NCCL INFO Channel 03 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:47:793 [7] NCCL INFO Channel 03 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:791 [6] NCCL INFO Channel 03 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:41:788 [1] NCCL INFO Channel 03 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:787 [2] NCCL INFO Channel 03 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:43:789 [3] NCCL INFO Channel 04 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:786 [0] NCCL INFO Channel 04 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:790 [4] NCCL INFO Channel 04 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:45:792 [5] NCCL INFO Channel 04 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:791 [6] NCCL INFO Channel 04 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:47:793 [7] NCCL INFO Channel 04 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:41:788 [1] NCCL INFO Channel 04 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:787 [2] NCCL INFO Channel 04 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:787 [2] NCCL INFO Channel 04 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:43:789 [3] NCCL INFO Channel 04 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:790 [4] NCCL INFO Channel 04 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:45:792 [5] NCCL INFO Channel 04 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:47:793 [7] NCCL INFO Channel 04 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:791 [6] NCCL INFO Channel 04 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:41:788 [1] NCCL INFO Channel 04 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:787 [2] NCCL INFO Channel 05 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:43:789 [3] NCCL INFO Channel 05 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:786 [0] NCCL INFO Channel 05 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:790 [4] NCCL INFO Channel 05 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:45:792 [5] NCCL INFO Channel 05 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:47:793 [7] NCCL INFO Channel 05 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:791 [6] NCCL INFO Channel 05 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:41:788 [1] NCCL INFO Channel 05 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:787 [2] NCCL INFO Channel 05 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:41:788 [1] NCCL INFO Channel 05 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:43:789 [3] NCCL INFO Channel 05 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:790 [4] NCCL INFO Channel 05 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:45:792 [5] NCCL INFO Channel 05 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:47:793 [7] NCCL INFO Channel 05 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:791 [6] NCCL INFO Channel 05 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:41:788 [1] NCCL INFO Channel 06 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:786 [0] NCCL INFO Channel 06 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:787 [2] NCCL INFO Channel 06 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:43:789 [3] NCCL INFO Channel 06 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:45:792 [5] NCCL INFO Channel 06 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:790 [4] NCCL INFO Channel 06 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:47:793 [7] NCCL INFO Channel 06 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:791 [6] NCCL INFO Channel 06 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:790 [4] NCCL INFO Channel 06 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:41:788 [1] NCCL INFO Channel 06 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:787 [2] NCCL INFO Channel 06 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:43:789 [3] NCCL INFO Channel 06 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:45:792 [5] NCCL INFO Channel 06 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:47:793 [7] NCCL INFO Channel 06 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:791 [6] NCCL INFO Channel 06 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:790 [4] NCCL INFO Channel 07 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:786 [0] NCCL INFO Channel 07 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:41:788 [1] NCCL INFO Channel 07 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:787 [2] NCCL INFO Channel 07 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:43:789 [3] NCCL INFO Channel 07 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:45:792 [5] NCCL INFO Channel 07 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:47:793 [7] NCCL INFO Channel 07 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:791 [6] NCCL INFO Channel 07 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:790 [4] NCCL INFO Channel 07 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:41:788 [1] NCCL INFO Channel 07 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:787 [2] NCCL INFO Channel 07 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:43:789 [3] NCCL INFO Channel 07 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:45:792 [5] NCCL INFO Channel 07 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:47:793 [7] NCCL INFO Channel 07 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:791 [6] NCCL INFO Channel 07 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:790 [4] NCCL INFO Channel 08 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:786 [0] NCCL INFO Channel 08 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:41:788 [1] NCCL INFO Channel 08 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:787 [2] NCCL INFO Channel 08 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:43:789 [3] NCCL INFO Channel 08 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:45:792 [5] NCCL INFO Channel 08 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:47:793 [7] NCCL INFO Channel 08 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:791 [6] NCCL INFO Channel 08 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:43:789 [3] NCCL INFO Channel 08 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:790 [4] NCCL INFO Channel 08 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:41:788 [1] NCCL INFO Channel 08 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:787 [2] NCCL INFO Channel 08 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:45:792 [5] NCCL INFO Channel 08 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:47:793 [7] NCCL INFO Channel 08 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:791 [6] NCCL INFO Channel 08 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:43:789 [3] NCCL INFO Channel 09 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:786 [0] NCCL INFO Channel 09 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:41:788 [1] NCCL INFO Channel 09 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:790 [4] NCCL INFO Channel 09 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:787 [2] NCCL INFO Channel 09 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:45:792 [5] NCCL INFO Channel 09 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:47:793 [7] NCCL INFO Channel 09 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:791 [6] NCCL INFO Channel 09 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:43:789 [3] NCCL INFO Channel 09 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:41:788 [1] NCCL INFO Channel 09 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:790 [4] NCCL INFO Channel 09 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:787 [2] NCCL INFO Channel 09 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:45:792 [5] NCCL INFO Channel 09 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:47:793 [7] NCCL INFO Channel 09 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:791 [6] NCCL INFO Channel 09 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:43:789 [3] NCCL INFO Channel 10 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:786 [0] NCCL INFO Channel 10 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:41:788 [1] NCCL INFO Channel 10 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:790 [4] NCCL INFO Channel 10 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:787 [2] NCCL INFO Channel 10 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:45:792 [5] NCCL INFO Channel 10 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:791 [6] NCCL INFO Channel 10 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:47:793 [7] NCCL INFO Channel 10 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:787 [2] NCCL INFO Channel 10 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:43:789 [3] NCCL INFO Channel 10 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:41:788 [1] NCCL INFO Channel 10 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:790 [4] NCCL INFO Channel 10 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:45:792 [5] NCCL INFO Channel 10 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:47:793 [7] NCCL INFO Channel 10 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:791 [6] NCCL INFO Channel 10 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:787 [2] NCCL INFO Channel 11 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:786 [0] NCCL INFO Channel 11 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:41:788 [1] NCCL INFO Channel 11 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:43:789 [3] NCCL INFO Channel 11 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:790 [4] NCCL INFO Channel 11 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:45:792 [5] NCCL INFO Channel 11 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:47:793 [7] NCCL INFO Channel 11 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:791 [6] NCCL INFO Channel 11 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:41:788 [1] NCCL INFO Channel 11 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:787 [2] NCCL INFO Channel 11 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:43:789 [3] NCCL INFO Channel 11 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:790 [4] NCCL INFO Channel 11 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:45:792 [5] NCCL INFO Channel 11 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:791 [6] NCCL INFO Channel 11 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:47:793 [7] NCCL INFO Channel 11 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:41:788 [1] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:41:788 [1] NCCL INFO comm 0x7fe510001060 rank 1 nranks 8 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:786 [0] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:786 [0] NCCL INFO comm 0x7f22b4001060 rank 0 nranks 8 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:40 [0] NCCL INFO Launch mode Parallel\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:787 [2] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:43:789 [3] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:790 [4] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:787 [2] NCCL INFO comm 0x7f3b30001060 rank 2 nranks 8 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:43:789 [3] NCCL INFO comm 0x7f0314001060 rank 3 nranks 8 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:790 [4] NCCL INFO comm 0x7faa1c001060 rank 4 nranks 8 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:45:792 [5] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:47:793 [7] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:45:792 [5] NCCL INFO comm 0x7f9884001060 rank 5 nranks 8 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:791 [6] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:47:793 [7] NCCL INFO comm 0x7f1138001060 rank 7 nranks 8 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:791 [6] NCCL INFO comm 0x7f79b4001060 rank 6 nranks 8 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-05-02 13:45:10.464: I smdistributed/modelparallel/torch/state_mod.py:108] [0] Finished initializing torch distributed process groups. pp_rank: 0, dp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2022-05-02 13:45:10.464: I smdistributed/modelparallel/torch/state_mod.py:108] [1] Finished initializing torch distributed process groups. pp_rank: 0, dp_rank: 1\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2022-05-02 13:45:10.464: I smdistributed/modelparallel/torch/state_mod.py:108] [2] Finished initializing torch distributed process groups. pp_rank: 1, dp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2022-05-02 13:45:10.464: I smdistributed/modelparallel/torch/state_mod.py:108] [5] Finished initializing torch distributed process groups. pp_rank: 2, dp_rank: 1\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2022-05-02 13:45:10.464: I smdistributed/modelparallel/torch/state_mod.py:108] [6] Finished initializing torch distributed process groups. pp_rank: 3, dp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2022-05-02 13:45:10.464: I smdistributed/modelparallel/torch/state_mod.py:108] [3] Finished initializing torch distributed process groups. pp_rank: 1, dp_rank: 1\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2022-05-02 13:45:10.464: I smdistributed/modelparallel/torch/state_mod.py:108] [4] Finished initializing torch distributed process groups. pp_rank: 2, dp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2022-05-02 13:45:10.464: I smdistributed/modelparallel/torch/state_mod.py:108] [7] Finished initializing torch distributed process groups. pp_rank: 3, dp_rank: 1\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Downloading and preparing dataset glue/mnli (download: 298.29 MiB, generated: 78.65 MiB, post-processed: Unknown size, total: 376.95 MiB) to /root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|file_utils.py:1532] 2022-05-02 13:45:55,052 >> https://huggingface.co/roberta-large/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp24j39iti\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|file_utils.py:1536] 2022-05-02 13:45:55,090 >> storing https://huggingface.co/roberta-large/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|file_utils.py:1544] 2022-05-02 13:45:55,090 >> creating metadata file for /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|configuration_utils.py:517] 2022-05-02 13:45:55,090 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|configuration_utils.py:553] 2022-05-02 13:45:55,091 >> Model config RobertaConfig {\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    \"RobertaForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  ],\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  \"finetuning_task\": \"mnli\",\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  \"id2label\": {\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    \"0\": \"LABEL_0\",\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    \"1\": \"LABEL_1\",\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    \"2\": \"LABEL_2\"\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  },\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  \"label2id\": {\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    \"LABEL_0\": 0,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    \"LABEL_1\": 1,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    \"LABEL_2\": 2\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  },\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  \"layer_norm_eps\": 1e-05,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  \"max_position_embeddings\": 514,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  \"model_type\": \"roberta\",\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  \"type_vocab_size\": 1,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:}\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[INFO|configuration_utils.py:517] 2022-05-02 13:45:55,108 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[INFO|configuration_utils.py:553] 2022-05-02 13:45:55,108 >> Model config RobertaConfig {\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    \"RobertaForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  ],\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  \"finetuning_task\": \"mnli\",\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  \"id2label\": {\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    \"0\": \"LABEL_0\",\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    \"1\": \"LABEL_1\",\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    \"2\": \"LABEL_2\"\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  },\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  \"label2id\": {\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    \"LABEL_0\": 0,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    \"LABEL_1\": 1,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    \"LABEL_2\": 2\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  },\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  \"layer_norm_eps\": 1e-05,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  \"max_position_embeddings\": 514,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  \"model_type\": \"roberta\",\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  \"type_vocab_size\": 1,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:}\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[INFO|configuration_utils.py:517] 2022-05-02 13:45:55,113 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[INFO|configuration_utils.py:517] 2022-05-02 13:45:55,113 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[INFO|configuration_utils.py:553] 2022-05-02 13:45:55,113 >> Model config RobertaConfig {\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    \"RobertaForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  ],\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  \"finetuning_task\": \"mnli\",\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  \"id2label\": {\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    \"0\": \"LABEL_0\",\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    \"1\": \"LABEL_1\",\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    \"2\": \"LABEL_2\"\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  },\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  \"label2id\": {\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    \"LABEL_0\": 0,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    \"LABEL_1\": 1,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    \"LABEL_2\": 2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  },\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  \"layer_norm_eps\": 1e-05,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  \"max_position_embeddings\": 514,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  \"model_type\": \"roberta\",\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  \"type_vocab_size\": 1,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:}\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[INFO|configuration_utils.py:553] 2022-05-02 13:45:55,114 >> Model config RobertaConfig {\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    \"RobertaForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  ],\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  \"finetuning_task\": \"mnli\",\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  \"id2label\": {\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    \"0\": \"LABEL_0\",\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    \"1\": \"LABEL_1\",\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    \"2\": \"LABEL_2\"\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  },\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  \"label2id\": {\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    \"LABEL_0\": 0,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    \"LABEL_1\": 1,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    \"LABEL_2\": 2\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  },\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  \"layer_norm_eps\": 1e-05,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  \"max_position_embeddings\": 514,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  \"model_type\": \"roberta\",\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  \"type_vocab_size\": 1,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:}\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|configuration_utils.py:517] 2022-05-02 13:45:55,122 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|configuration_utils.py:553] 2022-05-02 13:45:55,123 >> Model config RobertaConfig {\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:    \"RobertaForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  ],\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  \"layer_norm_eps\": 1e-05,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  \"max_position_embeddings\": 514,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  \"model_type\": \"roberta\",\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  \"type_vocab_size\": 1,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:}\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[INFO|configuration_utils.py:517] 2022-05-02 13:45:55,140 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[INFO|configuration_utils.py:553] 2022-05-02 13:45:55,140 >> Model config RobertaConfig {\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:    \"RobertaForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  ],\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  \"layer_norm_eps\": 1e-05,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  \"max_position_embeddings\": 514,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  \"model_type\": \"roberta\",\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  \"type_vocab_size\": 1,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:}\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[INFO|configuration_utils.py:517] 2022-05-02 13:45:55,142 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[INFO|configuration_utils.py:553] 2022-05-02 13:45:55,142 >> Model config RobertaConfig {\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:    \"RobertaForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  ],\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  \"layer_norm_eps\": 1e-05,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  \"max_position_embeddings\": 514,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  \"model_type\": \"roberta\",\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  \"type_vocab_size\": 1,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:}\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[INFO|configuration_utils.py:517] 2022-05-02 13:45:55,144 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[INFO|configuration_utils.py:553] 2022-05-02 13:45:55,145 >> Model config RobertaConfig {\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:    \"RobertaForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  ],\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  \"layer_norm_eps\": 1e-05,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  \"max_position_embeddings\": 514,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  \"model_type\": \"roberta\",\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  \"type_vocab_size\": 1,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:}\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|file_utils.py:1532] 2022-05-02 13:45:55,154 >> https://huggingface.co/roberta-large/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpdzneh0pw\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[INFO|configuration_utils.py:517] 2022-05-02 13:45:55,156 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[INFO|configuration_utils.py:553] 2022-05-02 13:45:55,157 >> Model config RobertaConfig {\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    \"RobertaForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  ],\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  \"finetuning_task\": \"mnli\",\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  \"id2label\": {\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    \"0\": \"LABEL_0\",\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    \"1\": \"LABEL_1\",\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    \"2\": \"LABEL_2\"\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  },\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  \"label2id\": {\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    \"LABEL_0\": 0,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    \"LABEL_1\": 1,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    \"LABEL_2\": 2\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  },\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  \"layer_norm_eps\": 1e-05,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  \"max_position_embeddings\": 514,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  \"model_type\": \"roberta\",\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  \"type_vocab_size\": 1,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:}\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[INFO|configuration_utils.py:517] 2022-05-02 13:45:55,197 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[INFO|configuration_utils.py:553] 2022-05-02 13:45:55,198 >> Model config RobertaConfig {\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:    \"RobertaForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  ],\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  \"layer_norm_eps\": 1e-05,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  \"max_position_embeddings\": 514,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  \"model_type\": \"roberta\",\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  \"type_vocab_size\": 1,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:}\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[INFO|configuration_utils.py:517] 2022-05-02 13:45:55,209 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[INFO|configuration_utils.py:553] 2022-05-02 13:45:55,209 >> Model config RobertaConfig {\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    \"RobertaForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  ],\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  \"finetuning_task\": \"mnli\",\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  \"id2label\": {\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    \"0\": \"LABEL_0\",\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    \"1\": \"LABEL_1\",\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    \"2\": \"LABEL_2\"\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  },\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  \"label2id\": {\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    \"LABEL_0\": 0,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    \"LABEL_1\": 1,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    \"LABEL_2\": 2\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  },\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  \"layer_norm_eps\": 1e-05,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  \"max_position_embeddings\": 514,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  \"model_type\": \"roberta\",\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  \"type_vocab_size\": 1,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:}\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|file_utils.py:1536] 2022-05-02 13:45:55,235 >> storing https://huggingface.co/roberta-large/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|file_utils.py:1544] 2022-05-02 13:45:55,235 >> creating metadata file for /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[INFO|configuration_utils.py:517] 2022-05-02 13:45:55,240 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[INFO|configuration_utils.py:553] 2022-05-02 13:45:55,241 >> Model config RobertaConfig {\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:    \"RobertaForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  ],\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  \"layer_norm_eps\": 1e-05,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  \"max_position_embeddings\": 514,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  \"model_type\": \"roberta\",\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  \"type_vocab_size\": 1,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:}\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|file_utils.py:1532] 2022-05-02 13:45:55,270 >> https://huggingface.co/roberta-large/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp5z11dolu\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[INFO|configuration_utils.py:517] 2022-05-02 13:45:55,271 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[INFO|configuration_utils.py:553] 2022-05-02 13:45:55,272 >> Model config RobertaConfig {\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    \"RobertaForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  ],\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  \"finetuning_task\": \"mnli\",\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  \"id2label\": {\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    \"0\": \"LABEL_0\",\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    \"1\": \"LABEL_1\",\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    \"2\": \"LABEL_2\"\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  },\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  \"label2id\": {\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    \"LABEL_0\": 0,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    \"LABEL_1\": 1,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    \"LABEL_2\": 2\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  },\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  \"layer_norm_eps\": 1e-05,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  \"max_position_embeddings\": 514,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  \"model_type\": \"roberta\",\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  \"type_vocab_size\": 1,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:}\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[INFO|configuration_utils.py:517] 2022-05-02 13:45:55,310 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[INFO|configuration_utils.py:553] 2022-05-02 13:45:55,310 >> Model config RobertaConfig {\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:    \"RobertaForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  ],\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  \"layer_norm_eps\": 1e-05,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  \"max_position_embeddings\": 514,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  \"model_type\": \"roberta\",\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  \"type_vocab_size\": 1,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:}\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|configuration_utils.py:517] 2022-05-02 13:45:55,318 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|configuration_utils.py:553] 2022-05-02 13:45:55,319 >> Model config RobertaConfig {\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    \"RobertaForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  ],\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"finetuning_task\": \"mnli\",\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"id2label\": {\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    \"0\": \"LABEL_0\",\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    \"1\": \"LABEL_1\",\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    \"2\": \"LABEL_2\"\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  },\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"label2id\": {\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    \"LABEL_0\": 0,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    \"LABEL_1\": 1,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    \"LABEL_2\": 2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  },\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"layer_norm_eps\": 1e-05,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"max_position_embeddings\": 514,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"model_type\": \"roberta\",\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"type_vocab_size\": 1,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:}\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|file_utils.py:1536] 2022-05-02 13:45:55,332 >> storing https://huggingface.co/roberta-large/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|file_utils.py:1544] 2022-05-02 13:45:55,332 >> creating metadata file for /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|configuration_utils.py:517] 2022-05-02 13:45:55,349 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|configuration_utils.py:553] 2022-05-02 13:45:55,350 >> Model config RobertaConfig {\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    \"RobertaForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  ],\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"layer_norm_eps\": 1e-05,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"max_position_embeddings\": 514,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"model_type\": \"roberta\",\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"type_vocab_size\": 1,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:}\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|file_utils.py:1532] 2022-05-02 13:45:55,361 >> https://huggingface.co/roberta-large/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmppjinfayn\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|file_utils.py:1536] 2022-05-02 13:45:55,473 >> storing https://huggingface.co/roberta-large/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|file_utils.py:1544] 2022-05-02 13:45:55,473 >> creating metadata file for /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,575 >> loading file https://huggingface.co/roberta-large/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,575 >> loading file https://huggingface.co/roberta-large/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,575 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,575 >> loading file https://huggingface.co/roberta-large/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,575 >> loading file https://huggingface.co/roberta-large/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,575 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer_config.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,585 >> loading file https://huggingface.co/roberta-large/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,585 >> loading file https://huggingface.co/roberta-large/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,585 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,585 >> loading file https://huggingface.co/roberta-large/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,585 >> loading file https://huggingface.co/roberta-large/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,585 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer_config.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,586 >> loading file https://huggingface.co/roberta-large/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,586 >> loading file https://huggingface.co/roberta-large/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,586 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,586 >> loading file https://huggingface.co/roberta-large/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,586 >> loading file https://huggingface.co/roberta-large/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,586 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer_config.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,595 >> loading file https://huggingface.co/roberta-large/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,595 >> loading file https://huggingface.co/roberta-large/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,595 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,595 >> loading file https://huggingface.co/roberta-large/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,596 >> loading file https://huggingface.co/roberta-large/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,596 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer_config.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,600 >> loading file https://huggingface.co/roberta-large/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,601 >> loading file https://huggingface.co/roberta-large/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,601 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,601 >> loading file https://huggingface.co/roberta-large/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,601 >> loading file https://huggingface.co/roberta-large/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,601 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer_config.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,602 >> loading file https://huggingface.co/roberta-large/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,602 >> loading file https://huggingface.co/roberta-large/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,603 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,603 >> loading file https://huggingface.co/roberta-large/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,603 >> loading file https://huggingface.co/roberta-large/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,602 >> loading file https://huggingface.co/roberta-large/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,603 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer_config.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,603 >> loading file https://huggingface.co/roberta-large/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,603 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,603 >> loading file https://huggingface.co/roberta-large/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,603 >> loading file https://huggingface.co/roberta-large/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,603 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer_config.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,618 >> loading file https://huggingface.co/roberta-large/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,618 >> loading file https://huggingface.co/roberta-large/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,619 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,619 >> loading file https://huggingface.co/roberta-large/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,619 >> loading file https://huggingface.co/roberta-large/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,619 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer_config.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[INFO|file_utils.py:1532] 2022-05-02 13:45:55,705 >> https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmppc0y1qta\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[INFO|file_utils.py:1536] 2022-05-02 13:46:21,507 >> storing https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[INFO|file_utils.py:1544] 2022-05-02 13:46:21,507 >> creating metadata file for /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[INFO|modeling_utils.py:1155] 2022-05-02 13:46:21,508 >> loading weights file https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[INFO|modeling_utils.py:1155] 2022-05-02 13:46:21,535 >> loading weights file https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[INFO|modeling_utils.py:1155] 2022-05-02 13:46:21,535 >> loading weights file https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|modeling_utils.py:1155] 2022-05-02 13:46:21,585 >> loading weights file https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|modeling_utils.py:1155] 2022-05-02 13:46:21,636 >> loading weights file https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[INFO|modeling_utils.py:1155] 2022-05-02 13:46:21,686 >> loading weights file https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[INFO|modeling_utils.py:1155] 2022-05-02 13:46:21,736 >> loading weights file https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[INFO|modeling_utils.py:1155] 2022-05-02 13:46:21,786 >> loading weights file https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[WARNING|modeling_utils.py:1331] 2022-05-02 13:46:26,623 >> Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[WARNING|modeling_utils.py:1342] 2022-05-02 13:46:26,623 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[WARNING|modeling_utils.py:1331] 2022-05-02 13:46:26,878 >> Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias']\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[WARNING|modeling_utils.py:1342] 2022-05-02 13:46:26,878 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[WARNING|modeling_utils.py:1331] 2022-05-02 13:46:26,882 >> Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight']\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[WARNING|modeling_utils.py:1342] 2022-05-02 13:46:26,882 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[WARNING|modeling_utils.py:1331] 2022-05-02 13:46:26,885 >> Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.weight']\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[WARNING|modeling_utils.py:1342] 2022-05-02 13:46:26,885 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[WARNING|modeling_utils.py:1331] 2022-05-02 13:46:26,933 >> Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[WARNING|modeling_utils.py:1342] 2022-05-02 13:46:26,933 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[WARNING|modeling_utils.py:1331] 2022-05-02 13:46:26,934 >> Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[WARNING|modeling_utils.py:1342] 2022-05-02 13:46:26,934 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[WARNING|modeling_utils.py:1331] 2022-05-02 13:46:27,224 >> Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[WARNING|modeling_utils.py:1342] 2022-05-02 13:46:27,224 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[WARNING|modeling_utils.py:1331] 2022-05-02 13:46:27,281 >> Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias']\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[WARNING|modeling_utils.py:1342] 2022-05-02 13:46:27,281 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[INFO|trainer.py:398] 2022-05-02 13:47:21,678 >> max_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[INFO|trainer.py:516] 2022-05-02 13:47:21,956 >> The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: premise, hypothesis, idx.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[INFO|trainer.py:1156] 2022-05-02 13:47:22,009 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[INFO|trainer.py:1157] 2022-05-02 13:47:22,021 >>   Num examples = 392702\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[INFO|trainer.py:1158] 2022-05-02 13:47:22,021 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[INFO|trainer.py:1159] 2022-05-02 13:47:22,021 >>   Instantaneous batch size per device = 16\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[INFO|trainer.py:1160] 2022-05-02 13:47:22,021 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[INFO|trainer.py:1161] 2022-05-02 13:47:22,022 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[INFO|trainer.py:1162] 2022-05-02 13:47:22,036 >>   Total optimization steps = 500\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[INFO|trainer.py:398] 2022-05-02 13:47:22,210 >> max_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|trainer.py:398] 2022-05-02 13:47:22,309 >> max_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[INFO|trainer.py:398] 2022-05-02 13:47:22,330 >> max_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[INFO|trainer.py:516] 2022-05-02 13:47:22,377 >> The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[INFO|trainer.py:1156] 2022-05-02 13:47:22,423 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[INFO|trainer.py:1157] 2022-05-02 13:47:22,423 >>   Num examples = 392702\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[INFO|trainer.py:1158] 2022-05-02 13:47:22,423 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[INFO|trainer.py:1159] 2022-05-02 13:47:22,423 >>   Instantaneous batch size per device = 16\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[INFO|trainer.py:1160] 2022-05-02 13:47:22,423 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[INFO|trainer.py:1161] 2022-05-02 13:47:22,423 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[INFO|trainer.py:1162] 2022-05-02 13:47:22,424 >>   Total optimization steps = 500\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[INFO|trainer.py:398] 2022-05-02 13:47:22,539 >> max_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|trainer.py:516] 2022-05-02 13:47:22,555 >> The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: hypothesis, premise, idx.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[INFO|trainer.py:516] 2022-05-02 13:47:22,560 >> The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[INFO|trainer.py:1156] 2022-05-02 13:47:22,613 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[INFO|trainer.py:1157] 2022-05-02 13:47:22,614 >>   Num examples = 392702\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[INFO|trainer.py:1158] 2022-05-02 13:47:22,614 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[INFO|trainer.py:1159] 2022-05-02 13:47:22,614 >>   Instantaneous batch size per device = 16\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[INFO|trainer.py:1160] 2022-05-02 13:47:22,614 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[INFO|trainer.py:1161] 2022-05-02 13:47:22,614 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[INFO|trainer.py:1162] 2022-05-02 13:47:22,614 >>   Total optimization steps = 500\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|trainer.py:1156] 2022-05-02 13:47:22,627 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|trainer.py:1157] 2022-05-02 13:47:22,627 >>   Num examples = 392702\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|trainer.py:1158] 2022-05-02 13:47:22,627 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|trainer.py:1159] 2022-05-02 13:47:22,627 >>   Instantaneous batch size per device = 16\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|trainer.py:1160] 2022-05-02 13:47:22,627 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|trainer.py:1161] 2022-05-02 13:47:22,627 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|trainer.py:1162] 2022-05-02 13:47:22,627 >>   Total optimization steps = 500\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2022-05-02 13:47:22.715: W smdistributed/modelparallel/backend/split.py:167] Non-splittable object of type <class 'NoneType'> passed to smp.step. If this object contains tensors that need to be split across microbatches, implement a 'smp_slice' method for this class. See SMP documentation for further information.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[INFO|trainer.py:516] 2022-05-02 13:47:22,739 >> The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: premise, idx, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[INFO|trainer.py:398] 2022-05-02 13:47:22,829 >> max_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[INFO|trainer.py:1156] 2022-05-02 13:47:22,844 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[INFO|trainer.py:1157] 2022-05-02 13:47:22,844 >>   Num examples = 392702\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[INFO|trainer.py:1158] 2022-05-02 13:47:22,845 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[INFO|trainer.py:1159] 2022-05-02 13:47:22,845 >>   Instantaneous batch size per device = 16\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[INFO|trainer.py:1160] 2022-05-02 13:47:22,845 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[INFO|trainer.py:1161] 2022-05-02 13:47:22,845 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[INFO|trainer.py:1162] 2022-05-02 13:47:22,845 >>   Total optimization steps = 500\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[INFO|trainer.py:398] 2022-05-02 13:47:22,945 >> max_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[INFO|trainer.py:516] 2022-05-02 13:47:22,952 >> The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: premise, idx, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:398] 2022-05-02 13:47:22,955 >> max_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[INFO|trainer.py:1156] 2022-05-02 13:47:23,005 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[INFO|trainer.py:1157] 2022-05-02 13:47:23,005 >>   Num examples = 392702\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[INFO|trainer.py:1158] 2022-05-02 13:47:23,005 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[INFO|trainer.py:1159] 2022-05-02 13:47:23,006 >>   Instantaneous batch size per device = 16\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[INFO|trainer.py:1160] 2022-05-02 13:47:23,006 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[INFO|trainer.py:1161] 2022-05-02 13:47:23,006 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[INFO|trainer.py:1162] 2022-05-02 13:47:23,006 >>   Total optimization steps = 500\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:516] 2022-05-02 13:47:23,096 >> The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: hypothesis, idx, premise.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[INFO|trainer.py:516] 2022-05-02 13:47:23,096 >> The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:1156] 2022-05-02 13:47:23,141 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:1157] 2022-05-02 13:47:23,141 >>   Num examples = 392702\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:1158] 2022-05-02 13:47:23,141 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:1159] 2022-05-02 13:47:23,141 >>   Instantaneous batch size per device = 16\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:1160] 2022-05-02 13:47:23,141 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:1161] 2022-05-02 13:47:23,141 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:1162] 2022-05-02 13:47:23,141 >>   Total optimization steps = 500\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[INFO|trainer.py:1156] 2022-05-02 13:47:23,146 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[INFO|trainer.py:1157] 2022-05-02 13:47:23,146 >>   Num examples = 392702\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[INFO|trainer.py:1158] 2022-05-02 13:47:23,147 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[INFO|trainer.py:1159] 2022-05-02 13:47:23,147 >>   Instantaneous batch size per device = 16\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[INFO|trainer.py:1160] 2022-05-02 13:47:23,147 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[INFO|trainer.py:1161] 2022-05-02 13:47:23,147 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[INFO|trainer.py:1162] 2022-05-02 13:47:23,147 >>   Total optimization steps = 500\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-05-02 13:47:23.222: W smdistributed/modelparallel/backend/split.py:167] Non-splittable object of type <class 'NoneType'> passed to smp.step. If this object contains tensors that need to be split across microbatches, implement a 'smp_slice' method for this class. See SMP documentation for further information.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-05-02 13:47:23.224: I smdistributed/modelparallel/torch/worker.py:281] Tracing on GPU. If the model parameters do not fit in a single GPU, you can set trace_device to `cpu`.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-05-02 13:47:25.705: I smdistributed/modelparallel/torch/model.py:231] Partition assignments:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-05-02 13:47:25.705: I smdistributed/modelparallel/torch/model.py:238] main: 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-05-02 13:47:25.705: I smdistributed/modelparallel/torch/model.py:238] main/module: 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-05-02 13:47:25.705: I smdistributed/modelparallel/torch/model.py:238] main/module/module: 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-05-02 13:47:25.705: I smdistributed/modelparallel/torch/model.py:238] main/module/module/roberta: 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-05-02 13:47:25.706: I smdistributed/modelparallel/torch/model.py:238] main/module/module/classifier: 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-05-02 13:47:25.706: I smdistributed/modelparallel/torch/model.py:238] main/module/module/roberta/embeddings: 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-05-02 13:47:25.706: I smdistributed/modelparallel/torch/model.py:238] main/module/module/roberta/encoder: 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-05-02 13:47:25.706: I smdistributed/modelparallel/torch/model.py:238] main/module/module/roberta/encoder/layer: 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-05-02 13:47:25.706: I smdistributed/modelparallel/torch/model.py:238] main/module/module/roberta/encoder/layer/0: 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-05-02 13:47:25.706: I smdistributed/modelparallel/torch/model.py:238] main/module/module/roberta/encoder/layer/1: 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-05-02 13:47:25.707: I smdistributed/modelparallel/torch/model.py:238] main/module/module/roberta/encoder/layer/2: 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-05-02 13:47:25.707: I smdistributed/modelparallel/torch/model.py:238] main/module/module/roberta/encoder/layer/3: 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-05-02 13:47:25.707: I smdistributed/modelparallel/torch/model.py:238] main/module/module/roberta/encoder/layer/4: 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-05-02 13:47:25.707: I smdistributed/modelparallel/torch/model.py:238] main/module/module/roberta/encoder/layer/5: 3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-05-02 13:47:25.707: I smdistributed/modelparallel/torch/model.py:238] main/module/module/roberta/encoder/layer/6: 3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-05-02 13:47:25.708: I smdistributed/modelparallel/torch/model.py:238] main/module/module/roberta/encoder/layer/7: 3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-05-02 13:47:25.708: I smdistributed/modelparallel/torch/model.py:238] main/module/module/roberta/encoder/layer/8: 3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-05-02 13:47:25.708: I smdistributed/modelparallel/torch/model.py:238] main/module/module/roberta/encoder/layer/9: 3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-05-02 13:47:25.709: I smdistributed/modelparallel/torch/model.py:238] main/module/module/roberta/encoder/layer/10: 3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-05-02 13:47:25.709: I smdistributed/modelparallel/torch/model.py:238] main/module/module/roberta/encoder/layer/11: 1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-05-02 13:47:25.709: I smdistributed/modelparallel/torch/model.py:238] main/module/module/roberta/encoder/layer/12: 1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-05-02 13:47:25.709: I smdistributed/modelparallel/torch/model.py:238] main/module/module/roberta/encoder/layer/13: 1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-05-02 13:47:25.709: I smdistributed/modelparallel/torch/model.py:238] main/module/module/roberta/encoder/layer/14: 1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-05-02 13:47:25.709: I smdistributed/modelparallel/torch/model.py:238] main/module/module/roberta/encoder/layer/15: 1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-05-02 13:47:25.710: I smdistributed/modelparallel/torch/model.py:238] main/module/module/roberta/encoder/layer/16: 1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-05-02 13:47:25.710: I smdistributed/modelparallel/torch/model.py:238] main/module/module/roberta/encoder/layer/17: 2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-05-02 13:47:25.710: I smdistributed/modelparallel/torch/model.py:238] main/module/module/roberta/encoder/layer/18: 2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-05-02 13:47:25.710: I smdistributed/modelparallel/torch/model.py:238] main/module/module/roberta/encoder/layer/19: 2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-05-02 13:47:25.710: I smdistributed/modelparallel/torch/model.py:238] main/module/module/roberta/encoder/layer/20: 2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-05-02 13:47:25.711: I smdistributed/modelparallel/torch/model.py:238] main/module/module/roberta/encoder/layer/21: 2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-05-02 13:47:25.711: I smdistributed/modelparallel/torch/model.py:238] main/module/module/roberta/encoder/layer/22: 2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-05-02 13:47:25.711: I smdistributed/modelparallel/torch/model.py:238] main/module/module/roberta/encoder/layer/23: 2\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2022-05-02 13:47:25.873: I smdistributed/modelparallel/torch/model.py:181] Number of parameters on partition 1 are 96. 96 require grads\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2022-05-02 13:47:25.878: I smdistributed/modelparallel/torch/model.py:181] Number of parameters on partition 3 are 96. 96 require grads\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2022-05-02 13:47:25.897: I smdistributed/modelparallel/torch/model.py:181] Number of parameters on partition 2 are 112. 112 require grads\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-05-02 13:47:26.415: I smdistributed/modelparallel/torch/model.py:181] Number of parameters on partition 0 are 89. 89 require grads\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-05-02 13:47:26.418: I smdistributed/modelparallel/torch/model.py:197] Number of buffers on partition 0 are 1. \u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-05-02 13:47:26.418: I smdistributed/modelparallel/torch/model.py:260] Finished partitioning the model\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:NCCL version 2.7.8+cuda11.0\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:NCCL version 2.7.8+cuda11.0\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:NCCL version 2.7.8+cuda11.0\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:41:1447 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:41:1447 [1] NCCL INFO Trees [0] -1/-1/-1->1->0|0->1->-1/-1/-1 [1] -1/-1/-1->1->0|0->1->-1/-1/-1 [2] -1/-1/-1->1->0|0->1->-1/-1/-1 [3] -1/-1/-1->1->0|0->1->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:1445 [0] NCCL INFO Channel 00/04 :    0   1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:1445 [0] NCCL INFO Channel 01/04 :    0   1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:1445 [0] NCCL INFO Channel 02/04 :    0   1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:1445 [0] NCCL INFO Channel 03/04 :    0   1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:1445 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:1445 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1|-1->0->1/-1/-1 [1] 1/-1/-1->0->-1|-1->0->1/-1/-1 [2] 1/-1/-1->0->-1|-1->0->1/-1/-1 [3] 1/-1/-1->0->-1|-1->0->1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:1443 [6] NCCL INFO Channel 00/04 :    0   1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:47:1449 [7] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:47:1449 [7] NCCL INFO Trees [0] -1/-1/-1->1->0|0->1->-1/-1/-1 [1] 0/-1/-1->1->-1|-1->1->0/-1/-1 [2] -1/-1/-1->1->0|0->1->-1/-1/-1 [3] 0/-1/-1->1->-1|-1->1->0/-1/-1\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:1443 [6] NCCL INFO Channel 01/04 :    0   1\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:1443 [6] NCCL INFO Channel 02/04 :    0   1\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:1443 [6] NCCL INFO Channel 03/04 :    0   1\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:1443 [6] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:1443 [6] NCCL INFO Trees [0] 1/-1/-1->0->-1|-1->0->1/-1/-1 [1] -1/-1/-1->0->1|1->0->-1/-1/-1 [2] 1/-1/-1->0->-1|-1->0->1/-1/-1 [3] -1/-1/-1->0->1|1->0->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:41:1447 [1] NCCL INFO Channel 00 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:1445 [0] NCCL INFO Channel 00 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:47:1449 [7] NCCL INFO Channel 00 : 1[1e0] -> 0[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:1443 [6] NCCL INFO Channel 00 : 0[1d0] -> 1[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:43:1450 [3] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:43:1450 [3] NCCL INFO Trees [0] -1/-1/-1->1->0|0->1->-1/-1/-1 [1] 0/-1/-1->1->-1|-1->1->0/-1/-1 [2] -1/-1/-1->1->0|0->1->-1/-1/-1 [3] 0/-1/-1->1->-1|-1->1->0/-1/-1\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:1448 [2] NCCL INFO Channel 00/04 :    0   1\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:1448 [2] NCCL INFO Channel 01/04 :    0   1\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:1448 [2] NCCL INFO Channel 02/04 :    0   1\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:1448 [2] NCCL INFO Channel 03/04 :    0   1\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:1448 [2] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:1448 [2] NCCL INFO Trees [0] 1/-1/-1->0->-1|-1->0->1/-1/-1 [1] -1/-1/-1->0->1|1->0->-1/-1/-1 [2] 1/-1/-1->0->-1|-1->0->1/-1/-1 [3] -1/-1/-1->0->1|1->0->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:45:1453 [5] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:1452 [4] NCCL INFO Channel 00/04 :    0   1\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:1452 [4] NCCL INFO Channel 01/04 :    0   1\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:45:1453 [5] NCCL INFO Trees [0] -1/-1/-1->1->0|0->1->-1/-1/-1 [1] -1/-1/-1->1->0|0->1->-1/-1/-1 [2] -1/-1/-1->1->0|0->1->-1/-1/-1 [3] -1/-1/-1->1->0|0->1->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:1452 [4] NCCL INFO Channel 02/04 :    0   1\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:1452 [4] NCCL INFO Channel 03/04 :    0   1\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:1452 [4] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:1452 [4] NCCL INFO Trees [0] 1/-1/-1->0->-1|-1->0->1/-1/-1 [1] 1/-1/-1->0->-1|-1->0->1/-1/-1 [2] 1/-1/-1->0->-1|-1->0->1/-1/-1 [3] 1/-1/-1->0->-1|-1->0->1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:43:1450 [3] NCCL INFO Channel 00 : 1[1a0] -> 0[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:1448 [2] NCCL INFO Channel 00 : 0[190] -> 1[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:45:1453 [5] NCCL INFO Channel 00 : 1[1c0] -> 0[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:41:1447 [1] NCCL INFO Channel 01 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:1452 [4] NCCL INFO Channel 00 : 0[1b0] -> 1[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:1445 [0] NCCL INFO Channel 01 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:47:1449 [7] NCCL INFO Channel 01 : 1[1e0] -> 0[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:1443 [6] NCCL INFO Channel 01 : 0[1d0] -> 1[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:43:1450 [3] NCCL INFO Channel 01 : 1[1a0] -> 0[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:1448 [2] NCCL INFO Channel 01 : 0[190] -> 1[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:45:1453 [5] NCCL INFO Channel 01 : 1[1c0] -> 0[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:41:1447 [1] NCCL INFO Channel 02 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:1452 [4] NCCL INFO Channel 01 : 0[1b0] -> 1[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:1445 [0] NCCL INFO Channel 02 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:47:1449 [7] NCCL INFO Channel 02 : 1[1e0] -> 0[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:1443 [6] NCCL INFO Channel 02 : 0[1d0] -> 1[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:43:1450 [3] NCCL INFO Channel 02 : 1[1a0] -> 0[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:1448 [2] NCCL INFO Channel 02 : 0[190] -> 1[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:1452 [4] NCCL INFO Channel 02 : 0[1b0] -> 1[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:45:1453 [5] NCCL INFO Channel 02 : 1[1c0] -> 0[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:41:1447 [1] NCCL INFO Channel 03 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:1445 [0] NCCL INFO Channel 03 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:47:1449 [7] NCCL INFO Channel 03 : 1[1e0] -> 0[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:1443 [6] NCCL INFO Channel 03 : 0[1d0] -> 1[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:41:1447 [1] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:1445 [0] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:41:1447 [1] NCCL INFO comm 0x7fe36c001060 rank 1 nranks 2 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:1445 [0] NCCL INFO comm 0x7f20c0001060 rank 0 nranks 2 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:40:40 [0] NCCL INFO Launch mode Parallel\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:47:1449 [7] NCCL INFO 4 coll channels, 4 p2p channels, 4 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:47:1449 [7] NCCL INFO comm 0x7f0f6c001060 rank 1 nranks 2 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:1443 [6] NCCL INFO 4 coll channels, 4 p2p channels, 4 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:1443 [6] NCCL INFO comm 0x7f77e0001060 rank 0 nranks 2 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:46:46 [6] NCCL INFO Launch mode Parallel\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:43:1450 [3] NCCL INFO Channel 03 : 1[1a0] -> 0[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:1448 [2] NCCL INFO Channel 03 : 0[190] -> 1[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:1452 [4] NCCL INFO Channel 03 : 0[1b0] -> 1[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:45:1453 [5] NCCL INFO Channel 03 : 1[1c0] -> 0[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:43:1450 [3] NCCL INFO 4 coll channels, 4 p2p channels, 4 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:43:1450 [3] NCCL INFO comm 0x7f015c001060 rank 1 nranks 2 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:1448 [2] NCCL INFO 4 coll channels, 4 p2p channels, 4 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:1448 [2] NCCL INFO comm 0x7f3998001060 rank 0 nranks 2 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:42:42 [2] NCCL INFO Launch mode Parallel\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:1452 [4] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:1452 [4] NCCL INFO comm 0x7fa830001060 rank 0 nranks 2 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Launch mode Parallel\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:45:1453 [5] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:45:1453 [5] NCCL INFO comm 0x7f96ac001060 rank 1 nranks 2 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-05-02 13:47:26.589: I smdistributed/modelparallel/torch/model.py:268] Broadcasted parameters and buffers for partition 0\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2022-05-02 13:47:26.591: I smdistributed/modelparallel/torch/model.py:268] Broadcasted parameters and buffers for partition 3\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2022-05-02 13:47:26.597: I smdistributed/modelparallel/torch/model.py:268] Broadcasted parameters and buffers for partition 1\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2022-05-02 13:47:26.599: I smdistributed/modelparallel/torch/model.py:268] Broadcasted parameters and buffers for partition 2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:{'loss': 1.1072, 'learning_rate': 0.0, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2022-05-02 13:58:15.771: I smdistributed/modelparallel/torch/model.py:307] [4] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2022-05-02 13:58:15.865: I smdistributed/modelparallel/torch/model.py:307] [7] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2022-05-02 13:58:15.886: I smdistributed/modelparallel/torch/model.py:307] [2] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2022-05-02 13:58:15.889: I smdistributed/modelparallel/torch/model.py:307] [3] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2022-05-02 13:58:15.936: I smdistributed/modelparallel/torch/model.py:307] [6] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2022-05-02 13:58:15.948: I smdistributed/modelparallel/torch/model.py:307] [5] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-05-02 13:58:16.020: I smdistributed/modelparallel/torch/model.py:307] [0] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2022-05-02 13:58:16.028: I smdistributed/modelparallel/torch/model.py:307] [1] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[INFO|trainer.py:1352] 2022-05-02 13:58:17,227 >> \u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Training completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:1885] 2022-05-02 13:58:17,235 >> Saving model checkpoint to /opt/ml/model/checkpoint-500\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|configuration_utils.py:351] 2022-05-02 13:58:17,236 >> Configuration saved in /opt/ml/model/checkpoint-500/config.json\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2022-05-02 13:58:17.695: I smdistributed/modelparallel/torch/model.py:307] [1] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[INFO|trainer.py:1352] 2022-05-02 13:58:17,942 >> \u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Training completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[INFO|trainer.py:1352] 2022-05-02 13:58:18,196 >> \u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Training completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[INFO|trainer.py:1352] 2022-05-02 13:58:18,212 >> \u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Training completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2022-05-02 13:58:18.233: I smdistributed/modelparallel/torch/optimizers/optimizer.py:83] [4] Gathering optimizer state_dict from different ranks. To prevent hangs, please ensure optimizer.state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2022-05-02 13:58:18.328: I smdistributed/modelparallel/torch/model.py:307] [3] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2022-05-02 13:58:18.371: I smdistributed/modelparallel/torch/optimizers/optimizer.py:83] [2] Gathering optimizer state_dict from different ranks. To prevent hangs, please ensure optimizer.state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2022-05-02 13:58:18.395: I smdistributed/modelparallel/torch/optimizers/optimizer.py:83] [6] Gathering optimizer state_dict from different ranks. To prevent hangs, please ensure optimizer.state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2022-05-02 13:58:18.567: I smdistributed/modelparallel/torch/model.py:307] [7] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2022-05-02 13:58:18.624: I smdistributed/modelparallel/torch/model.py:307] [5] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|modeling_utils.py:889] 2022-05-02 13:58:19,199 >> Model weights saved in /opt/ml/model/checkpoint-500/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|tokenization_utils_base.py:1924] 2022-05-02 13:58:19,200 >> tokenizer config file saved in /opt/ml/model/checkpoint-500/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|tokenization_utils_base.py:1930] 2022-05-02 13:58:19,200 >> Special tokens file saved in /opt/ml/model/checkpoint-500/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[INFO|trainer.py:516] 2022-05-02 13:58:19,702 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[INFO|trainer.py:2115] 2022-05-02 13:58:19,707 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[INFO|trainer.py:2117] 2022-05-02 13:58:19,707 >>   Num examples = 9815\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[INFO|trainer.py:2120] 2022-05-02 13:58:19,707 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-05-02 13:58:20.013: I smdistributed/modelparallel/torch/optimizers/optimizer.py:83] [0] Gathering optimizer state_dict from different ranks. To prevent hangs, please ensure optimizer.state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[INFO|trainer.py:516] 2022-05-02 13:58:20,126 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: premise, hypothesis, idx.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[INFO|trainer.py:2115] 2022-05-02 13:58:20,131 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[INFO|trainer.py:2117] 2022-05-02 13:58:20,131 >>   Num examples = 9815\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[INFO|trainer.py:2120] 2022-05-02 13:58:20,131 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[INFO|trainer.py:516] 2022-05-02 13:58:20,515 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: premise, idx, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[INFO|trainer.py:2115] 2022-05-02 13:58:20,521 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[INFO|trainer.py:2117] 2022-05-02 13:58:20,521 >>   Num examples = 9815\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[INFO|trainer.py:2120] 2022-05-02 13:58:20,521 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[INFO|trainer.py:516] 2022-05-02 13:58:20,524 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[INFO|trainer.py:2115] 2022-05-02 13:58:20,529 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[INFO|trainer.py:2117] 2022-05-02 13:58:20,529 >>   Num examples = 9815\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[INFO|trainer.py:2120] 2022-05-02 13:58:20,529 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[INFO|trainer.py:1352] 2022-05-02 13:58:23,576 >> \u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Training completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|trainer.py:1352] 2022-05-02 13:58:23,597 >> \u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Training completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[INFO|trainer.py:1352] 2022-05-02 13:58:23,616 >> \u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Training completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2022-05-02 13:58:23.841: I smdistributed/modelparallel/torch/model.py:307] [4] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2022-05-02 13:58:23.988: I smdistributed/modelparallel/torch/model.py:307] [2] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2022-05-02 13:58:24.015: I smdistributed/modelparallel/torch/model.py:307] [6] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:1352] 2022-05-02 13:58:27,547 >> \u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Training completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:{'train_runtime': 664.4063, 'train_samples_per_second': 0.753, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-05-02 13:58:27.982: I smdistributed/modelparallel/torch/model.py:307] [0] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:1885] 2022-05-02 13:58:28,778 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|configuration_utils.py:351] 2022-05-02 13:58:28,781 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[INFO|trainer.py:516] 2022-05-02 13:58:29,544 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[INFO|trainer.py:2115] 2022-05-02 13:58:29,548 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[INFO|trainer.py:2117] 2022-05-02 13:58:29,548 >>   Num examples = 9815\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[INFO|trainer.py:2120] 2022-05-02 13:58:29,548 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|trainer.py:516] 2022-05-02 13:58:29,564 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: hypothesis, premise, idx.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|trainer.py:2115] 2022-05-02 13:58:29,569 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|trainer.py:2117] 2022-05-02 13:58:29,569 >>   Num examples = 9815\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|trainer.py:2120] 2022-05-02 13:58:29,569 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[INFO|trainer.py:516] 2022-05-02 13:58:29,579 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: premise, idx, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[INFO|trainer.py:2115] 2022-05-02 13:58:29,584 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[INFO|trainer.py:2117] 2022-05-02 13:58:29,584 >>   Num examples = 9815\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[INFO|trainer.py:2120] 2022-05-02 13:58:29,584 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|modeling_utils.py:889] 2022-05-02 13:58:30,675 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|tokenization_utils_base.py:1924] 2022-05-02 13:58:30,675 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|tokenization_utils_base.py:1930] 2022-05-02 13:58:30,675 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:907] 2022-05-02 13:58:30,805 >> ***** train metrics *****\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2022-05-02 13:58:30,805 >>   epoch                      =       0.04\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2022-05-02 13:58:30,805 >>   init_mem_cpu_alloc_delta   =       -5MB\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2022-05-02 13:58:30,805 >>   init_mem_cpu_peaked_delta  =        4MB\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2022-05-02 13:58:30,805 >>   init_mem_gpu_alloc_delta   =        0MB\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2022-05-02 13:58:30,805 >>   init_mem_gpu_peaked_delta  =        0MB\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2022-05-02 13:58:30,805 >>   train_mem_cpu_alloc_delta  =     1993MB\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2022-05-02 13:58:30,805 >>   train_mem_cpu_peaked_delta =     1863MB\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2022-05-02 13:58:30,805 >>   train_mem_gpu_alloc_delta  =     2213MB\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2022-05-02 13:58:30,805 >>   train_mem_gpu_peaked_delta =     1541MB\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2022-05-02 13:58:30,806 >>   train_runtime              = 0:11:04.40\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2022-05-02 13:58:30,806 >>   train_samples              =     392702\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2022-05-02 13:58:30,806 >>   train_samples_per_second   =      0.753\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:516] 2022-05-02 13:58:30,907 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: hypothesis, idx, premise.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:2115] 2022-05-02 13:58:30,911 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:2117] 2022-05-02 13:58:30,911 >>   Num examples = 9815\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:2120] 2022-05-02 13:58:30,911 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:907] 2022-05-02 14:01:50,579 >> ***** eval metrics *****\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2022-05-02 14:01:50,580 >>   epoch                     =       0.04\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2022-05-02 14:01:50,580 >>   eval_accuracy             =     0.3182\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2022-05-02 14:01:50,580 >>   eval_loss                 =     1.0992\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2022-05-02 14:01:50,580 >>   eval_mem_cpu_alloc_delta  =      103MB\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2022-05-02 14:01:50,580 >>   eval_mem_cpu_peaked_delta =        0MB\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2022-05-02 14:01:50,580 >>   eval_mem_gpu_alloc_delta  =        0MB\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2022-05-02 14:01:50,580 >>   eval_mem_gpu_peaked_delta =      146MB\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2022-05-02 14:01:50,580 >>   eval_runtime              = 0:03:19.47\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2022-05-02 14:01:50,580 >>   eval_samples              =       9815\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2022-05-02 14:01:50,580 >>   eval_samples_per_second   =     49.203\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[INFO|trainer.py:516] 2022-05-02 14:01:50,652 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: premise, idx, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[INFO|trainer.py:2115] 2022-05-02 14:01:50,656 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[INFO|trainer.py:2117] 2022-05-02 14:01:50,656 >>   Num examples = 9832\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[INFO|trainer.py:2120] 2022-05-02 14:01:50,656 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[INFO|trainer.py:516] 2022-05-02 14:01:50,671 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: premise, idx, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[INFO|trainer.py:2115] 2022-05-02 14:01:50,675 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[INFO|trainer.py:2117] 2022-05-02 14:01:50,676 >>   Num examples = 9832\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[INFO|trainer.py:2120] 2022-05-02 14:01:50,676 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[INFO|trainer.py:516] 2022-05-02 14:01:50,708 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[INFO|trainer.py:2115] 2022-05-02 14:01:50,711 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[INFO|trainer.py:2117] 2022-05-02 14:01:50,712 >>   Num examples = 9832\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[INFO|trainer.py:2120] 2022-05-02 14:01:50,712 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[INFO|trainer.py:516] 2022-05-02 14:01:50,736 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: premise, hypothesis, idx.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[INFO|trainer.py:2115] 2022-05-02 14:01:50,741 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[INFO|trainer.py:2117] 2022-05-02 14:01:50,741 >>   Num examples = 9832\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[INFO|trainer.py:2120] 2022-05-02 14:01:50,742 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:516] 2022-05-02 14:01:50,750 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: hypothesis, idx, premise.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:2115] 2022-05-02 14:01:50,754 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:2117] 2022-05-02 14:01:50,754 >>   Num examples = 9832\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:2120] 2022-05-02 14:01:50,754 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|trainer.py:516] 2022-05-02 14:01:50,821 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: hypothesis, premise, idx.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|trainer.py:2115] 2022-05-02 14:01:50,824 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|trainer.py:2117] 2022-05-02 14:01:50,824 >>   Num examples = 9832\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|trainer.py:2120] 2022-05-02 14:01:50,825 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[INFO|trainer.py:516] 2022-05-02 14:01:50,842 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[INFO|trainer.py:2115] 2022-05-02 14:01:50,846 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[INFO|trainer.py:2117] 2022-05-02 14:01:50,847 >>   Num examples = 9832\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[INFO|trainer.py:2120] 2022-05-02 14:01:50,847 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[INFO|trainer.py:516] 2022-05-02 14:01:50,872 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[INFO|trainer.py:2115] 2022-05-02 14:01:50,875 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[INFO|trainer.py:2117] 2022-05-02 14:01:50,876 >>   Num examples = 9832\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[INFO|trainer.py:2120] 2022-05-02 14:01:50,876 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[INFO|trainer.py:516] 2022-05-02 14:05:10,090 >> The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[INFO|trainer.py:516] 2022-05-02 14:05:10,095 >> The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[INFO|trainer.py:2115] 2022-05-02 14:05:10,095 >> ***** Running Prediction *****\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[INFO|trainer.py:2117] 2022-05-02 14:05:10,095 >>   Num examples = 9796\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[INFO|trainer.py:2120] 2022-05-02 14:05:10,096 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[INFO|trainer.py:2115] 2022-05-02 14:05:10,098 >> ***** Running Prediction *****\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[INFO|trainer.py:2117] 2022-05-02 14:05:10,098 >>   Num examples = 9796\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[INFO|trainer.py:2120] 2022-05-02 14:05:10,099 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[INFO|trainer.py:516] 2022-05-02 14:05:10,135 >> The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: premise, idx, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[INFO|trainer.py:2115] 2022-05-02 14:05:10,139 >> ***** Running Prediction *****\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[INFO|trainer.py:2117] 2022-05-02 14:05:10,139 >>   Num examples = 9796\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[INFO|trainer.py:2120] 2022-05-02 14:05:10,139 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:907] 2022-05-02 14:05:10,175 >> ***** eval metrics *****\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2022-05-02 14:05:10,176 >>   epoch                     =       0.04\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2022-05-02 14:05:10,176 >>   eval_accuracy             =     0.3182\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2022-05-02 14:05:10,176 >>   eval_loss                 =     1.0992\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2022-05-02 14:05:10,176 >>   eval_mem_cpu_alloc_delta  =       29MB\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2022-05-02 14:05:10,176 >>   eval_mem_cpu_peaked_delta =        0MB\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2022-05-02 14:05:10,176 >>   eval_mem_gpu_alloc_delta  =        0MB\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2022-05-02 14:05:10,176 >>   eval_mem_gpu_peaked_delta =      150MB\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2022-05-02 14:05:10,176 >>   eval_runtime              = 0:03:19.19\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2022-05-02 14:05:10,176 >>   eval_samples              =       9832\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2022-05-02 14:05:10,176 >>   eval_samples_per_second   =     49.358\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|trainer.py:516] 2022-05-02 14:05:10,227 >> The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: hypothesis, premise, idx.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|trainer.py:2115] 2022-05-02 14:05:10,230 >> ***** Running Prediction *****\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|trainer.py:2117] 2022-05-02 14:05:10,231 >>   Num examples = 9796\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|trainer.py:2120] 2022-05-02 14:05:10,231 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[INFO|trainer.py:516] 2022-05-02 14:05:10,322 >> The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: premise, idx, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[INFO|trainer.py:2115] 2022-05-02 14:05:10,326 >> ***** Running Prediction *****\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[INFO|trainer.py:2117] 2022-05-02 14:05:10,326 >>   Num examples = 9796\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[INFO|trainer.py:2120] 2022-05-02 14:05:10,327 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:516] 2022-05-02 14:05:10,375 >> The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: hypothesis, idx, premise.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:2115] 2022-05-02 14:05:10,379 >> ***** Running Prediction *****\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:2117] 2022-05-02 14:05:10,380 >>   Num examples = 9796\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:2120] 2022-05-02 14:05:10,380 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[INFO|trainer.py:516] 2022-05-02 14:05:10,432 >> The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[INFO|trainer.py:2115] 2022-05-02 14:05:10,436 >> ***** Running Prediction *****\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[INFO|trainer.py:2117] 2022-05-02 14:05:10,436 >>   Num examples = 9796\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[INFO|trainer.py:2120] 2022-05-02 14:05:10,436 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[INFO|trainer.py:516] 2022-05-02 14:05:10,456 >> The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: premise, hypothesis, idx.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[INFO|trainer.py:2115] 2022-05-02 14:05:10,460 >> ***** Running Prediction *****\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[INFO|trainer.py:2117] 2022-05-02 14:05:10,461 >>   Num examples = 9796\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[INFO|trainer.py:2120] 2022-05-02 14:05:10,461 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[INFO|trainer.py:516] 2022-05-02 14:08:26,758 >> The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[INFO|trainer.py:2115] 2022-05-02 14:08:26,762 >> ***** Running Prediction *****\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[INFO|trainer.py:2117] 2022-05-02 14:08:26,762 >>   Num examples = 9847\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[INFO|trainer.py:2120] 2022-05-02 14:08:26,762 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[INFO|trainer.py:516] 2022-05-02 14:08:26,780 >> The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[INFO|trainer.py:2115] 2022-05-02 14:08:26,784 >> ***** Running Prediction *****\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[INFO|trainer.py:2117] 2022-05-02 14:08:26,784 >>   Num examples = 9847\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[INFO|trainer.py:2120] 2022-05-02 14:08:26,784 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[INFO|trainer.py:516] 2022-05-02 14:08:26,807 >> The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: premise, idx, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[INFO|trainer.py:2115] 2022-05-02 14:08:26,811 >> ***** Running Prediction *****\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[INFO|trainer.py:2117] 2022-05-02 14:08:26,811 >>   Num examples = 9847\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[INFO|trainer.py:2120] 2022-05-02 14:08:26,811 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|trainer.py:516] 2022-05-02 14:08:26,827 >> The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: hypothesis, premise, idx.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|trainer.py:2115] 2022-05-02 14:08:26,831 >> ***** Running Prediction *****\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|trainer.py:2117] 2022-05-02 14:08:26,831 >>   Num examples = 9847\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[INFO|trainer.py:2120] 2022-05-02 14:08:26,831 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[INFO|trainer.py:516] 2022-05-02 14:08:26,856 >> The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: premise, hypothesis, idx.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[INFO|trainer.py:2115] 2022-05-02 14:08:26,860 >> ***** Running Prediction *****\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[INFO|trainer.py:2117] 2022-05-02 14:08:26,861 >>   Num examples = 9847\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[INFO|trainer.py:2120] 2022-05-02 14:08:26,861 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[INFO|trainer.py:516] 2022-05-02 14:08:26,879 >> The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[INFO|trainer.py:2115] 2022-05-02 14:08:26,882 >> ***** Running Prediction *****\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[INFO|trainer.py:2117] 2022-05-02 14:08:26,882 >>   Num examples = 9847\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[INFO|trainer.py:2120] 2022-05-02 14:08:26,882 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[INFO|trainer.py:516] 2022-05-02 14:08:26,909 >> The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: premise, idx, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[INFO|trainer.py:2115] 2022-05-02 14:08:26,914 >> ***** Running Prediction *****\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[INFO|trainer.py:2117] 2022-05-02 14:08:26,914 >>   Num examples = 9847\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[INFO|trainer.py:2120] 2022-05-02 14:08:26,914 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:516] 2022-05-02 14:08:26,914 >> The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: hypothesis, idx, premise.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:2115] 2022-05-02 14:08:26,919 >> ***** Running Prediction *****\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:2117] 2022-05-02 14:08:26,919 >>   Num examples = 9847\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:2120] 2022-05-02 14:08:26,919 >>   Batch size = 16\u001b[0m\n",
      "\n",
      "2022-05-02 14:11:51 Uploading - Uploading generated training model\u001b[34m[1,2]<stderr>:WARNING:__main__:Process rank: -1, device: cuda:2, n_gpu: 1distributed training: False, 16-bits training: False\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:WARNING:__main__:Process rank: -1, device: cuda:3, n_gpu: 1distributed training: False, 16-bits training: False\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:WARNING:__main__:Process rank: -1, device: cuda:4, n_gpu: 1distributed training: False, 16-bits training: False\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:WARNING:__main__:Process rank: -1, device: cuda:7, n_gpu: 1distributed training: False, 16-bits training: False\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:WARNING:__main__:Process rank: -1, device: cuda:6, n_gpu: 1distributed training: False, 16-bits training: False\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:__main__:Training/evaluation parameters TrainingArguments(output_dir=/opt/ml/model, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=16, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=2.0, max_steps=500, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/May02_13-45-11_algo-1, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/opt/ml/model, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=[], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=1, mp_parameters=ddp=True,microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:__main__:Training/evaluation parameters TrainingArguments(output_dir=/opt/ml/model, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=16, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=2.0, max_steps=500, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/May02_13-45-11_algo-1, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/opt/ml/model, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=[], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=1, mp_parameters=ddp=True,microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread)\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:INFO:__main__:Training/evaluation parameters TrainingArguments(output_dir=/opt/ml/model, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=16, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=2.0, max_steps=500, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/May02_13-45-11_algo-1, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/opt/ml/model, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=[], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=1, mp_parameters=ddp=True,microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:WARNING:__main__:Process rank: -1, device: cuda:1, n_gpu: 1distributed training: False, 16-bits training: False\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:INFO:__main__:Training/evaluation parameters TrainingArguments(output_dir=/opt/ml/model, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=16, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=2.0, max_steps=500, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/May02_13-45-11_algo-1, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/opt/ml/model, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=[], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=1, mp_parameters=ddp=True,microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:__main__:Training/evaluation parameters TrainingArguments(output_dir=/opt/ml/model, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=16, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=2.0, max_steps=500, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/May02_13-45-11_algo-1, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/opt/ml/model, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=[], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=1, mp_parameters=ddp=True,microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread)\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:INFO:__main__:Training/evaluation parameters TrainingArguments(output_dir=/opt/ml/model, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=16, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=2.0, max_steps=500, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/May02_13-45-11_algo-1, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/opt/ml/model, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=[], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=1, mp_parameters=ddp=True,microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:__main__:Training/evaluation parameters TrainingArguments(output_dir=/opt/ml/model, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=16, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=2.0, max_steps=500, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/May02_13-45-11_algo-1, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/opt/ml/model, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=[], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=1, mp_parameters=ddp=True,microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread)\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:WARNING:__main__:Process rank: -1, device: cuda:5, n_gpu: 1distributed training: False, 16-bits training: False\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:INFO:__main__:Training/evaluation parameters TrainingArguments(output_dir=/opt/ml/model, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=16, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=2.0, max_steps=500, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/May02_13-45-11_algo-1, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/opt/ml/model, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=[], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=1, mp_parameters=ddp=True,microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread)\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:#015Downloading:   0%|          | 0.00/7.78k [00:00<?, ?B/s][1,6]<stderr>:#015Downloading: 28.8kB [00:00, 14.6MB/s]                   \u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015Downloading:   0%|          | 0.00/7.78k [00:00<?, ?B/s][1,0]<stderr>:#015Downloading: 28.8kB [00:00, 15.8MB/s]                   [1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:#015Downloading:   0%|          | 0.00/7.78k [00:00<?, ?B/s][1,3]<stderr>:#015Downloading: 28.8kB [00:00, 15.3MB/s]                   \u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:#015Downloading:   0%|          | 0.00/4.47k [00:00<?, ?B/s][1,6]<stderr>:#015Downloading: 28.7kB [00:00, 18.7MB/s]                   \u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:#015Downloading:   0%|          | 0.00/7.78k [00:00<?, ?B/s][1,4]<stderr>:#015Downloading: 28.8kB [00:00, 13.5MB/s]                   \u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015Downloading:   0%|          | 0.00/4.47k [00:00<?, ?B/s][1,0]<stderr>:#015Downloading: 28.7kB [00:00, 20.6MB/s]                   \u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:#015Downloading:   0%|          | 0.00/7.78k [00:00<?, ?B/s][1,7]<stderr>:#015Downloading: 28.8kB [00:00, 16.8MB/s]                   \u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:#015Downloading:   0%|          | 0.00/7.78k [00:00<?, ?B/s][1,2]<stderr>:#015Downloading: 28.8kB [00:00, 16.1MB/s]                   [1,2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:#015Downloading:   0%|          | 0.00/7.78k [00:00<?, ?B/s][1,1]<stderr>:#015Downloading: 28.8kB [00:00, 16.0MB/s]                   \u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:#015Downloading:   0%|          | 0.00/7.78k [00:00<?, ?B/s][1,5]<stderr>:#015Downloading: 28.8kB [00:00, 16.9MB/s]                   \u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:#015Downloading:   0%|          | 0.00/313M [00:00<?, ?B/s][1,6]<stderr>:#015Downloading:   0%|          | 52.2k/313M [00:00<17:43, 294kB/s][1,6]<stderr>:#015Downloading:   0%|          | 209k/313M [00:00<14:12, 367kB/s] [1,6]<stderr>:#015Downloading:   0%|          | 296k/313M [00:00<11:44, 444kB/s][1,6]<stderr>:#015Downloading:   0%|          | 453k/313M [00:00<09:56, 524kB/s][1,6]<stderr>:#015Downloading:   0%|          | 609k/313M [00:00<08:45, 595kB/s][1,6]<stderr>:#015Downloading:   0%|          | 923k/313M [00:00<07:01, 740kB/s][1,6]<stderr>:#015Downloading:   1%|          | 1.85M/313M [00:01<05:04, 1.02MB/s][1,6]<stderr>:#015Downloading:   1%|          | 3.09M/313M [00:01<03:39, 1.41MB/s][1,6]<stderr>:#015Downloading:   2%|▏         | 4.96M/313M [00:01<02:37, 1.95MB/s][1,6]<stderr>:#015Downloading:   3%|▎         | 9.12M/313M [00:01<01:51, 2.73MB/s][1,6]<stderr>:#015Downloading:   4%|▍         | 13.9M/313M [00:01<01:18, 3.81MB/s][1,6]<stderr>:#015Downloading:   6%|▌         | 17.3M/313M [00:01<00:56, 5.20MB/s][1,6]<stderr>:#015Downloading:   7%|▋         | 20.8M/313M [00:01<00:41, 6.98MB/s][1,6]<stderr>:#015Downloading:   8%|▊         | 25.1M/313M [00:01<00:30, 9.32MB/s][1,6]<stderr>:#015Downloading:   9%|▉         | 29.2M/313M [00:01<00:23, 12.1MB/s][1,6]<stderr>:#015Downloading:  11%|█         | 33.5M/313M [00:02<00:18, 15.1MB/s][1,6]<stderr>:#015Downloading:  12%|█▏        | 37.6M/313M [00:02<00:14, 18.7MB/s][1,6]<stderr>:#015Downloading:  13%|█▎        | 41.8M/313M [00:02<00:12, 22.4MB/s][1,6]<stderr>:#015Downloading:  15%|█▍        | 45.9M/313M [00:02<00:10, 25.9MB/s][1,6]<stderr>:#015Downloading:  16%|█▌        | 50.2M/313M [00:02<00:08, 29.5MB/s][1,6]<stderr>:#015Downloading:  17%|█▋        | 54.3M/313M [00:02<00:08, 31.4MB/s][1,6]<stderr>:#015Downloading:  19%|█▉        | 58.9M/313M [00:02<00:07, 34.8MB/s][1,6]<stderr>:#015Downloading:  20%|██        | 63.1M/313M [00:02<00:07, 35.7MB/s][1,6]<stderr>:#015Downloading:  21%|██▏       | 67.2M/313M [00:02<00:06, 37.2MB/s][1,6]<stderr>:#015Downloading:  23%|██▎       | 71.5M/313M [00:02<00:06, 38.6MB/s][1,6]<stderr>:#015Downloading:  24%|██▍       | 75.6M/313M [00:03<00:06, 38.3MB/s][1,6]<stderr>:#015Downloading:  25%|██▌       | 79.7M/313M [00:03<00:05, 39.1MB/s][1,6]<stderr>:#015Downloading:  27%|██▋       | 83.9M/313M [00:03<00:05, 39.9MB/s][1,6]<stderr>:#015Downloading:  28%|██▊       | 88.1M/313M [00:03<00:05, 40.5MB/s][1,6]<stderr>:#015Downloading:  29%|██▉       | 92.2M/313M [00:03<00:05, 40.6MB/s][1,6]<stderr>:#015Downloading:  31%|███       | 96.8M/313M [00:03<00:05, 42.1MB/s][1,6]<stderr>:#015Downloading:  32%|███▏      | 101M/313M [00:03<00:05, 40.5MB/s] [1,6]<stderr>:#015Downloading:  34%|███▎      | 105M/313M [00:03<00:05, 37.4MB/s][1,6]<stderr>:#015Downloading:  35%|███▍      | 109M/313M [00:03<00:05, 38.8MB/s][1,6]<stderr>:#015Downloading:  36%|███▌      | 113M/313M [00:03<00:05, 38.0MB/s][1,6]<stderr>:#015Downloading:  38%|███▊      | 118M/313M [00:04<00:04, 40.4MB/s][1,6]<stderr>:#015Downloading:  39%|███▉      | 122M/313M [00:04<00:04, 41.0MB/s][1,6]<stderr>:#015Downloading:  40%|████      | 126M/313M [00:04<00:04, 41.0MB/s][1,6]<stderr>:#015Downloading:  42%|████▏     | 131M/313M [00:04<00:04, 42.2MB/s][1,6]<stderr>:#015Downloading:  43%|████▎     | 135M/313M [00:04<00:04, 41.8MB/s][1,6]<stderr>:#015Downloading:  45%|████▍     | 139M/313M [00:04<00:04, 38.2MB/s][1,6]<stderr>:#015Downloading:  46%|████▌     | 144M/313M [00:04<00:04, 39.1MB/s][1,6]<stderr>:#015Downloading:  47%|████▋     | 148M/313M [00:04<00:04, 40.2MB/s][1,6]<stderr>:#015Downloading:  49%|████▊     | 152M/313M [00:04<00:04, 39.3MB/s][1,6]<stderr>:#015Downloading:  50%|████▉     | 156M/313M [00:05<00:04, 38.8MB/s][1,6]<stderr>:#015Downloading:  51%|█████     | 160M/313M [00:05<00:03, 39.6MB/s][1,6]<stderr>:#015Downloading:  53%|█████▎    | 165M/313M [00:05<00:03, 42.3MB/s][1,6]<stderr>:#015Downloading:  54%|█████▍    | 169M/313M [00:05<00:03, 37.6MB/s][1,6]<stderr>:#015Downloading:  56%|█████▌    | 174M/313M [00:05<00:03, 39.0MB/s][1,6]<stderr>:#015Downloading:  57%|█████▋    | 178M/313M [00:05<00:03, 40.6MB/s][1,6]<stderr>:#015Downloading:  59%|█████▊    | 183M/313M [00:05<00:03, 42.9MB/s][1,6]<stderr>:#015Downloading:  60%|█████▉    | 188M/313M [00:05<00:02, 43.3MB/s][1,6]<stderr>:#015Downloading:  61%|██████▏   | 192M/313M [00:05<00:02, 42.9MB/s][1,6]<stderr>:#015Downloading:  63%|██████▎   | 196M/313M [00:06<00:02, 40.1MB/s][1,6]<stderr>:#015Downloading:  64%|██████▍   | 200M/313M [00:06<00:02, 39.9MB/s][1,6]<stderr>:#015Downloading:  65%|██████▌   | 204M/313M [00:06<00:02, 38.1MB/s][1,6]<stderr>:#015Downloading:  67%|██████▋   | 209M/313M [00:06<00:02, 39.7MB/s][1,6]<stderr>:#015Downloading:  68%|██████▊   | 213M/313M [00:06<00:02, 40.3MB/s][1,6]<stderr>:#015Downloading:  70%|██████▉   | 217M/313M [00:06<00:02, 41.6MB/s][1,6]<stderr>:#015Downloading:  71%|███████   | 222M/313M [00:06<00:02, 43.5MB/s][1,6]<stderr>:#015Downloading:  72%|███████▏  | 227M/313M [00:06<00:02, 40.7MB/s][1,6]<stderr>:#015Downloading:  74%|███████▍  | 231M/313M [00:06<00:01, 41.4MB/s][1,6]<stderr>:#015Downloading:  75%|███████▌  | 235M/313M [00:06<00:01, 42.2MB/s][1,6]<stderr>:#015Downloading:  77%|███████▋  | 240M/313M [00:07<00:01, 41.2MB/s][1,6]<stderr>:#015Downloading:  78%|███████▊  | 244M/313M [00:07<00:01, 41.0MB/s][1,6]<stderr>:#015Downloading:  79%|███████▉  | 248M/313M [00:07<00:01, 41.4MB/s][1,6]<stderr>:#015Downloading:  81%|████████  | 252M/313M [00:07<00:01, 41.5MB/s][1,6]<stderr>:#015Downloading:  82%|████████▏ | 258M/313M [00:07<00:01, 44.5MB/s][1,6]<stderr>:#015Downloading:  84%|████████▍ | 262M/313M [00:07<00:01, 40.8MB/s][1,6]<stderr>:#015Downloading:  85%|████████▌ | 266M/313M [00:07<00:01, 38.7MB/s][1,6]<stderr>:#015Downloading:  87%|████████▋ | 271M/313M [00:07<00:01, 40.7MB/s][1,6]<stderr>:#015Downloading:  88%|████████▊ | 275M/313M [00:07<00:00, 37.7MB/s][1,6]<stderr>:#015Downloading:  89%|████████▉ | 280M/313M [00:08<00:00, 39.3MB/s][1,6]<stderr>:#015Downloading:  91%|█████████ | 284M/313M [00:08<00:00, 40.3MB/s][1,6]<stderr>:#015Downloading:  92%|█████████▏| 288M/313M [00:08<00:00, 41.0MB/s][1,6]<stderr>:#015Downloading:  94%|█████████▎| 293M/313M [00:08<00:00, 42.1MB/s][1,6]<stderr>:#015Downloading:  95%|█████████▍| 297M/313M [00:08<00:00, 42.3MB/s][1,6]<stderr>:#015Downloading:  96%|█████████▋| 301M/313M [00:08<00:00, 40.8MB/s][1,6]<stderr>:#015Downloading:  98%|█████████▊| 305M/313M [00:08<00:00, 40.7MB/s][1,6]<stderr>:#015Downloading:  99%|█████████▉| 310M/313M [00:08<00:00, 42.6MB/s][1,6]<stderr>:#015Downloading: 100%|██████████| 313M/313M [00:08<00:00, 35.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:#0150 examples [00:00, ? examples/s][1,6]<stderr>:#0151917 examples [00:00, 19161.16 examples/s][1,6]<stderr>:#0153859 examples [00:00, 19234.52 examples/s][1,6]<stderr>:#0155859 examples [00:00, 19456.11 examples/s][1,6]<stderr>:#0157850 examples [00:00, 19590.08 examples/s][1,6]<stderr>:#0159847 examples [00:00, 19699.94 examples/s][1,6]<stderr>:#01511496 examples [00:00, 18165.48 examples/s][1,6]<stderr>:#01513529 examples [00:00, 18763.71 examples/s][1,6]<stderr>:#01515526 examples [00:00, 19106.95 examples/s][1,6]<stderr>:#01517438 examples [00:00, 19108.90 examples/s][1,6]<stderr>:#01519466 examples [00:01, 19444.37 examples/s][1,6]<stderr>:#01521366 examples [00:01, 19215.58 examples/s][1,6]<stderr>:#01523401 examples [00:01, 19541.23 examples/s][1,6]<stderr>:#01525461 examples [00:01, 19846.33 examples/s][1,6]<stderr>:#01527496 examples [00:01, 19992.63 examples/s][1,6]<stderr>:#01529505 examples [00:01, 20020.74 examples/s][1,6]<stderr>:#01531501 examples [00:01, 19387.32 examples/s][1,6]<stderr>:#01533477 examples [00:01, 19497.30 examples/s][1,6]<stderr>:#01535508 examples [00:01, 19732.34 examples/s][1,6]<stderr>:#01537528 examples [00:01, 19868.97 examples/s][1,6]<stderr>:#01539565 examples [00:02, 20014.14 examples/s][1,6]<stderr>:#01541568 examples [00:02, 19654.75 examples/s][1,6]<stderr>:#01543603 examples [00:02, 19855.88 examples/s][1,6]<stderr>:#01545658 examples [00:02, 20056.96 examples/s][1,6]<stderr>:#01547693 examples [00:02, 20141.82 examples/s][1,6]<stderr>:#01549711 examples [00:02, 20152.50 examples/s][1,6]<stderr>:#01551728 examples [00:02, 19757.66 examples/s][1,6]<stderr>:#01553796 examples [00:02, 20021.89 examples/s][1,6]<stderr>:#01555823 examples [00:02, 20095.29 examples/s][1,6]<stderr>:#01557849 examples [00:02, 20143.38 examples/s][1,6]<stderr>:#01559895 examples [00:03, 20236.85 examples/s][1,6]<stderr>:#01561920 examples [00:03, 19818.01 examples/s][1,6]<stderr>:#01563971 examples [00:03, 20018.69 examples/s][1,6]<stderr>:#01565976 examples [00:03, 20012.54 examples/s][1,6]<stderr>:#01567992 examples [00:03, 20056.17 examples/s][1,6]<stderr>:#01570000 examples [00:03, 19567.06 examples/s][1,6]<stderr>:#01572030 examples [00:03, 19779.37 examples/s][1,6]<stderr>:#01574030 examples [00:03, 19844.56 examples/s][1,6]<stderr>:#01576073 examples [00:03, 20014.78 examples/s][1,6]<stderr>:#01578079 examples [00:03, 20025.34 examples/s][1,6]<stderr>:#01580083 examples [00:04, 19605.95 examples/s][1,6]<stderr>:#01582117 examples [00:04, 19819.01 examples/s][1,6]<stderr>:#01584159 examples [00:04, 19994.21 examples/s][1,6]<stderr>:#01586176 examples [00:04, 20045.26 examples/s][1,6]<stderr>:#01588183 examples [00:04, 20041.11 examples/s][1,6]<stderr>:#01590189 examples [00:04, 19527.60 examples/s][1,6]<stderr>:#01592207 examples [00:04, 19716.26 examples/s][1,6]<stderr>:#01594234 examples [00:04, 19876.83 examples/s][1,6]<stderr>:#01596229 examples [00:04, 19897.11 examples/s][1,6]<stderr>:#01598258 examples [00:04, 20012.15 examples/s][1,6]<stderr>:#015100261 examples [00:05, 19595.78 examples/s][1,6]<stderr>:#015102325 examples [00:05, 19896.22 examples/s][1,6]<stderr>:#015104358 examples [00:05, 20022.99 examples/s][1,6]<stderr>:#015106363 examples [00:05, 20010.50 examples/s][1,6]<stderr>:#015108387 examples [00:05, 20075.23 examples/s][1,6]<stderr>:#015110396 examples [00:05, 19603.71 examples/s][1,6]<stderr>:#015112394 examples [00:05, 19714.16 examples/s][1,6]<stderr>:#015114457 examples [00:05, 19977.69 examples/s][1,6]<stderr>:#015116518 examples [00:05, 20160.89 examples/s][1,6]<stderr>:#015118537 examples [00:05, 20147.26 examples/s][1,6]<stderr>:#015120554 examples [00:06, 19714.03 examples/s][1,6]<stderr>:#015122580 examples [00:06, 19872.75 examples/s][1,6]<stderr>:#015124600 examples [00:06, 19967.54 examples/s][1,6]<stderr>:#015126607 examples [00:06, 19998.03 examples/s][1,6]<stderr>:#015128623 examples [00:06, 20045.20 examples/s][1,6]<stderr>:#015130629 examples [00:06, 19503.45 examples/s][1,6]<stderr>:#015132635 examples [00:06, 19665.27 examples/s][1,6]<stderr>:#015134696 examples [00:06, 19938.95 examples/s][1,6]<stderr>:#015136765 examples [00:06, 20156.53 examples/s][1,6]<stderr>:#015138808 examples [00:06, 20235.38 examples/s][1,6]<stderr>:#015140834 examples [00:07, 19807.02 examples/s][1,6]<stderr>:#015142845 examples [00:07, 19895.38 examples/s][1,6]<stderr>:#015144921 examples [00:07, 20144.34 examples/s][1,6]<stderr>:#015146971 examples [00:07, 20249.54 examples/s][1,6]<stderr>:#015148998 examples [00:07, 20241.30 examples/s][1,6]<stderr>:#015151024 examples [00:07, 19783.51 examples/s][1,6]<stderr>:#015153054 examples [00:07, 19934.49 examples/s][1,6]<stderr>:#015155100 examples [00:07, 20088.57 examples/s][1,6]<stderr>:#015157111 examples [00:07, 19989.76 examples/s][1,6]<stderr>:#015159154 examples [00:08, 20117.00 examples/s][1,6]<stderr>:#015161167 examples [00:08, 19736.26 examples/s][1,6]<stderr>:#015163221 examples [00:08, 19968.68 examples/s][1,6]<stderr>:#015165249 examples [00:08, 20061.03 examples/s][1,6]<stderr>:#015167267 examples [00:08, 20095.45 examples/s][1,6]<stderr>:#015169300 examples [00:08, 20163.15 examples/s][1,6]<stderr>:#015171318 examples [00:08, 19762.74 examples/s][1,6]<stderr>:#015173348 examples [00:08, 19918.26 examples/s][1,6]<stderr>:#015175361 examples [00:08, 19980.24 examples/s][1,6]<stderr>:#015177375 examples [00:08, 20026.10 examples/s][1,6]<stderr>:#015179395 examples [00:09, 20076.40 examples/s][1,6]<stderr>:#015181404 examples [00:09, 19644.99 examples/s][1,6]<stderr>:#015183440 examples [00:09, 19852.26 examples/s][1,6]<stderr>:#015185485 examples [00:09, 20027.87 examples/s][1,6]<stderr>:#015187539 examples [00:09, 20176.69 examples/s][1,6]<stderr>:#015189572 examples [00:09, 20221.57 examples/s][1,6]<stderr>:#015191596 examples [00:09, 19701.20 examples/s][1,6]<stderr>:#015193629 examples [00:09, 19883.86 examples/s][1,6]<stderr>:#015195687 examples [00:09, 20086.49 examples/s][1,6]<stderr>:#015197708 examples [00:09, 20122.95 examples/s][1,6]<stderr>:#015199734 examples [00:10, 20161.98 examples/s][1,6]<stderr>:#015201752 examples [00:10, 19580.05 examples/s][1,6]<stderr>:#015203767 examples [00:10, 19746.75 examples/s][1,6]<stderr>:#015205746 examples [00:10, 19704.03 examples/s][1,6]<stderr>:#015207742 examples [00:10, 19777.54 examples/s][1,6]<stderr>:#015209764 examples [00:10, 19907.73 examples/s][1,6]<stderr>:#015211757 examples [00:10, 19525.56 examples/s][1,6]<stderr>:#015213764 examples [00:10, 19685.18 examples/s][1,6]<stderr>:#015215760 examples [00:10, 19763.85 examples/s][1,6]<stderr>:#015217779 examples [00:10, 19886.45 examples/s][1,6]<stderr>:#015219799 examples [00:11, 19979.06 examples/s][1,6]<stderr>:#015221798 examples [00:11, 19586.19 examples/s][1,6]<stderr>:#015223788 examples [00:11, 19678.99 examples/s][1,6]<stderr>:#015225758 examples [00:11, 19674.31 examples/s][1,6]<stderr>:#015227750 examples [00:11, 19745.89 examples/s][1,6]<stderr>:#015229751 examples [00:11, 19821.75 examples/s][1,6]<stderr>:#015231734 examples [00:11, 19429.27 examples/s][1,6]<stderr>:#015233730 examples [00:11, 19583.67 examples/s][1,6]<stderr>:#015235740 examples [00:11, 19734.43 examples/s][1,6]<stderr>:#015237761 examples [00:11, 19871.40 examples/s][1,6]<stderr>:#015239801 examples [00:12, 20026.89 examples/s][1,6]<stderr>:#015241805 examples [00:12, 19610.16 examples/s][1,6]<stderr>:#015243830 examples [00:12, 19797.22 examples/s][1,6]<stderr>:#015245850 examples [00:12, 19915.71 examples/s][1,6]<stderr>:#015247863 examples [00:12, 19979.31 examples/s][1,6]<stderr>:#015249869 examples [00:12, 19999.96 examples/s][1,6]<stderr>:#015251870 examples [00:12, 19631.11 examples/s][1,6]<stderr>:#015253885 examples [00:12, 19782.51 examples/s][1,6]<stderr>:#015255888 examples [00:12, 19854.70 examples/s][1,6]<stderr>:#015257903 examples [00:12, 19941.89 examples/s][1,6]<stderr>:#015259907 examples [00:13, 19969.97 examples/s][1,6]<stderr>:#015261905 examples [00:13, 19293.70 examples/s][1,6]<stderr>:#015263896 examples [00:13, 19472.48 examples/s][1,6]<stderr>:#015265895 examples [00:13, 19623.79 examples/s][1,6]<stderr>:#015267888 examples [00:13, 19714.32 examples/s][1,6]<stderr>:#015269904 examples [00:13, 19844.49 examples/s][1,6]<stderr>:#015271891 examples [00:13, 19373.23 examples/s][1,6]<stderr>:#015273932 examples [00:13, 19672.19 examples/s][1,6]<stderr>:#015275941 examples [00:13, 19794.29 examples/s][1,6]<stderr>:#015277961 examples [00:14, 19914.00 examples/s][1,6]<stderr>:#015279955 examples [00:14, 19838.48 examples/s][1,6]<stderr>:#015281941 examples [00:14, 19381.85 examples/s][1,6]<stderr>:#015283963 examples [00:14, 19623.46 examples/s][1,6]<stderr>:#015285944 examples [00:14, 19676.97 examples/s][1,6]<stderr>:#015287946 examples [00:14, 19776.59 examples/s][1,6]<stderr>:#015289941 examples [00:14, 19827.26 examples/s][1,6]<stderr>:#015291925 examples [00:14, 19388.58 examples/s][1,6]<stderr>:#015293938 examples [00:14, 19604.26 examples/s][1,6]<stderr>:#015295949 examples [00:14, 19752.23 examples/s][1,6]<stderr>:#015297953 examples [00:15, 19836.43 examples/s][1,6]<stderr>:#015299976 examples [00:15, 19947.91 examples/s][1,6]<stderr>:#015301973 examples [00:15, 19436.72 examples/s][1,6]<stderr>:#015303988 examples [00:15, 19644.03 examples/s][1,6]<stderr>:#015305956 examples [00:15, 19606.43 examples/s][1,6]<stderr>:#015307926 examples [00:15, 19633.56 examples/s][1,6]<stderr>:#015309932 examples [00:15, 19758.62 examples/s][1,6]<stderr>:#015311910 examples [00:15, 19489.52 examples/s][1,6]<stderr>:#015313919 examples [00:15, 19665.04 examples/s][1,6]<stderr>:#015315947 examples [00:15, 19845.46 examples/s][1,6]<stderr>:#015317939 examples [00:16, 19866.08 examples/s][1,6]<stderr>:#015319950 examples [00:16, 19936.80 examples/s][1,6]<stderr>:#015321945 examples [00:16, 19495.53 examples/s][1,6]<stderr>:#015323953 examples [00:16, 19664.88 examples/s][1,6]<stderr>:#015325944 examples [00:16, 19735.76 examples/s][1,6]<stderr>:#015327962 examples [00:16, 19866.91 examples/s][1,6]<stderr>:#015329971 examples [00:16, 19930.69 examples/s][1,6]<stderr>:#015331966 examples [00:16, 19437.91 examples/s][1,6]<stderr>:#015333944 examples [00:16, 19536.89 examples/s][1,6]<stderr>:#015335936 examples [00:16, 19650.25 examples/s][1,6]<stderr>:#015337967 examples [00:17, 19843.61 examples/s][1,6]<stderr>:#015339980 examples [00:17, 19928.35 examples/s][1,6]<stderr>:#015341975 examples [00:17, 19518.50 examples/s][1,6]<stderr>:#015343937 examples [00:17, 19548.23 examples/s][1,6]<stderr>:#015345894 examples [00:17, 19515.95 examples/s][1,6]<stderr>:#015347906 examples [00:17, 19691.63 examples/s][1,6]<stderr>:#015349877 examples [00:17, 19677.80 examples/s][1,6]<stderr>:#015351846 examples [00:17, 19249.18 examples/s][1,6]<stderr>:#015353848 examples [00:17, 19470.92 examples/s][1,6]<stderr>:#015355830 examples [00:17, 19573.91 examples/s][1,6]<stderr>:#015357814 examples [00:18, 19652.32 examples/s][1,6]<stderr>:#015359800 examples [00:18, 19713.63 examples/s][1,6]<stderr>:#015361773 examples [00:18, 19353.09 examples/s][1,6]<stderr>:#015363751 examples [00:18, 19476.57 examples/s][1,6]<stderr>:#015365745 examples [00:18, 19611.51 examples/s][1,6]<stderr>:#015367754 examples [00:18, 19750.42 examples/s][1,6]<stderr>:#015369792 examples [00:18, 19934.52 examples/s][1,6]<stderr>:#015371787 examples [00:18, 19479.56 examples/s][1,6]<stderr>:#015373788 examples [00:18, 19633.26 examples/s][1,6]<stderr>:#015375787 examples [00:18, 19736.86 examples/s][1,6]<stderr>:#015377799 examples [00:19, 19847.80 examples/s][1,6]<stderr>:#015379819 examples [00:19, 19951.11 examples/s][1,6]<stderr>:#015381816 examples [00:19, 19494.92 examples/s][1,6]<stderr>:#015383830 examples [00:19, 19682.18 examples/s][1,6]<stderr>:#015385827 examples [00:19, 19767.00 examples/s][1,6]<stderr>:#015387806 examples [00:19, 19766.85 examples/s][1,6]<stderr>:#015389801 examples [00:19, 19818.94 examples/s][1,6]<stderr>:#015391784 examples [00:19, 19405.41 examples/s][1,6]<stderr>:#015                                            #015[1,6]<stderr>:#0150 examples [00:00, ? examples/s][1,6]<stderr>:#0151863 examples [00:00, 18623.05 examples/s][1,6]<stderr>:#0153747 examples [00:00, 18686.06 examples/s][1,6]<stderr>:#0155643 examples [00:00, 18764.21 examples/s][1,6]<stderr>:#0157594 examples [00:00, 18978.56 examples/s][1,6]<stderr>:#0159558 examples [00:00, 19171.02 examples/s][1,6]<stderr>:#015                                          #015[1,6]<stderr>:#0150 examples [00:00, ? examples/s][1,6]<stderr>:#0151929 examples [00:00, 19283.77 examples/s][1,6]<stderr>:#0153842 examples [00:00, 19236.12 examples/s][1,6]<stderr>:#0155795 examples [00:00, 19321.18 examples/s][1,6]<stderr>:#0157722 examples [00:00, 19302.74 examples/s][1,6]<stderr>:#0159650 examples [00:00, 19294.55 examples/s][1,6]<stderr>:#015                                          #015[1,6]<stderr>:#0150 examples [00:00, ? examples/s][1,6]<stderr>:#0151995 examples [00:00, 19941.89 examples/s][1,6]<stderr>:#0154189 examples [00:00, 20500.04 examples/s][1,6]<stderr>:#0156427 examples [00:00, 21029.16 examples/s][1,6]<stderr>:#0158642 examples [00:00, 21352.50 examples/s][1,6]<stderr>:#015                                          #015[1,6]<stderr>:#0150 examples [00:00, ? examples/s][1,6]<stderr>:#0152163 examples [00:00, 21627.55 examples/s][1,6]<stderr>:#0154331 examples [00:00, 21641.36 examples/s][1,6]<stderr>:#0156512 examples [00:00, 21689.23 examples/s][1,6]<stderr>:#0158733 examples [00:00, 21839.89 examples/s][1,6]<stderr>:#015                                          #015[1,1]<stderr>:WARNING:datasets.builder:Reusing dataset glue (/root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:WARNING:datasets.builder:Reusing dataset glue (/root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:WARNING:datasets.builder:Reusing dataset glue (/root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:INFO:filelock:Lock 140162263163288 acquired on /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373.lock\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|file_utils.py:1532] 2022-05-02 13:45:55,052 >> https://huggingface.co/roberta-large/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp24j39iti\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:WARNING:datasets.builder:Reusing dataset glue (/root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:#015Downloading:   0%|          | 0.00/482 [00:00<?, ?B/s][1,6]<stderr>:#015Downloading: 100%|██████████| 482/482 [00:00<00:00, 333kB/s]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|file_utils.py:1536] 2022-05-02 13:45:55,090 >> storing https://huggingface.co/roberta-large/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|file_utils.py:1544] 2022-05-02 13:45:55,090 >> creating metadata file for /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:INFO:filelock:Lock 140162263163288 released on /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373.lock\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|configuration_utils.py:517] 2022-05-02 13:45:55,090 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|configuration_utils.py:553] 2022-05-02 13:45:55,091 >> Model config RobertaConfig {\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    \"RobertaForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  ],\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  \"finetuning_task\": \"mnli\",\u001b[0m\n",
      "\u001b[34m2022-05-02 14:11:46,735 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  \"id2label\": {\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    \"0\": \"LABEL_0\",\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    \"1\": \"LABEL_1\",\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    \"2\": \"LABEL_2\"\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  },\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  \"label2id\": {\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    \"LABEL_0\": 0,\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    \"LABEL_1\": 1,\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    \"LABEL_2\": 2\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  },\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  \"layer_norm_eps\": 1e-05,\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  \"max_position_embeddings\": 514,\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  \"model_type\": \"roberta\",\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  \"type_vocab_size\": 1,\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:INFO:filelock:Lock 140302113119256 acquired on /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373.lock\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:INFO:filelock:Lock 140302113119256 released on /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373.lock\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[INFO|configuration_utils.py:517] 2022-05-02 13:45:55,108 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[INFO|configuration_utils.py:553] 2022-05-02 13:45:55,108 >> Model config RobertaConfig {\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    \"RobertaForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  ],\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  \"finetuning_task\": \"mnli\",\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  \"id2label\": {\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    \"0\": \"LABEL_0\",\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    \"1\": \"LABEL_1\",\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    \"2\": \"LABEL_2\"\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  },\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  \"label2id\": {\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    \"LABEL_0\": 0,\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    \"LABEL_1\": 1,\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    \"LABEL_2\": 2\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  },\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  \"layer_norm_eps\": 1e-05,\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  \"max_position_embeddings\": 514,\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  \"model_type\": \"roberta\",\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  \"type_vocab_size\": 1,\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:WARNING:datasets.builder:Reusing dataset glue (/root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:filelock:Lock 140631610827104 acquired on /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373.lock\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:filelock:Lock 140631610827104 released on /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373.lock\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[INFO|configuration_utils.py:517] 2022-05-02 13:45:55,113 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[INFO|configuration_utils.py:517] 2022-05-02 13:45:55,113 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[INFO|configuration_utils.py:553] 2022-05-02 13:45:55,113 >> Model config RobertaConfig {\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    \"RobertaForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  ],\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  \"finetuning_task\": \"mnli\",\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  \"id2label\": {\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    \"0\": \"LABEL_0\",\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    \"1\": \"LABEL_1\",\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    \"2\": \"LABEL_2\"\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  },\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  \"label2id\": {\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    \"LABEL_0\": 0,\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    \"LABEL_1\": 1,\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    \"LABEL_2\": 2\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  },\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  \"layer_norm_eps\": 1e-05,\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  \"max_position_embeddings\": 514,\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  \"model_type\": \"roberta\",\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  \"type_vocab_size\": 1,\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[INFO|configuration_utils.py:553] 2022-05-02 13:45:55,114 >> Model config RobertaConfig {\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    \"RobertaForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  ],\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  \"finetuning_task\": \"mnli\",\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  \"id2label\": {\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    \"0\": \"LABEL_0\",\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    \"1\": \"LABEL_1\",\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    \"2\": \"LABEL_2\"\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  },\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  \"label2id\": {\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    \"LABEL_0\": 0,\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    \"LABEL_1\": 1,\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    \"LABEL_2\": 2\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  },\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  \"layer_norm_eps\": 1e-05,\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  \"max_position_embeddings\": 514,\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  \"model_type\": \"roberta\",\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  \"type_vocab_size\": 1,\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|configuration_utils.py:517] 2022-05-02 13:45:55,122 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|configuration_utils.py:553] 2022-05-02 13:45:55,123 >> Model config RobertaConfig {\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    \"RobertaForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  ],\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  \"layer_norm_eps\": 1e-05,\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  \"max_position_embeddings\": 514,\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  \"model_type\": \"roberta\",\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  \"type_vocab_size\": 1,\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[INFO|configuration_utils.py:517] 2022-05-02 13:45:55,140 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[INFO|configuration_utils.py:553] 2022-05-02 13:45:55,140 >> Model config RobertaConfig {\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    \"RobertaForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  ],\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  \"layer_norm_eps\": 1e-05,\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  \"max_position_embeddings\": 514,\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  \"model_type\": \"roberta\",\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  \"type_vocab_size\": 1,\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[INFO|configuration_utils.py:517] 2022-05-02 13:45:55,142 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[INFO|configuration_utils.py:553] 2022-05-02 13:45:55,142 >> Model config RobertaConfig {\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    \"RobertaForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  ],\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  \"layer_norm_eps\": 1e-05,\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  \"max_position_embeddings\": 514,\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  \"model_type\": \"roberta\",\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  \"type_vocab_size\": 1,\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[INFO|configuration_utils.py:517] 2022-05-02 13:45:55,144 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[INFO|configuration_utils.py:553] 2022-05-02 13:45:55,145 >> Model config RobertaConfig {\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    \"RobertaForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  ],\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  \"layer_norm_eps\": 1e-05,\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  \"max_position_embeddings\": 514,\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  \"model_type\": \"roberta\",\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  \"type_vocab_size\": 1,\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:INFO:filelock:Lock 140162244519192 acquired on /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab.lock\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|file_utils.py:1532] 2022-05-02 13:45:55,154 >> https://huggingface.co/roberta-large/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpdzneh0pw\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[INFO|configuration_utils.py:517] 2022-05-02 13:45:55,156 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[INFO|configuration_utils.py:553] 2022-05-02 13:45:55,157 >> Model config RobertaConfig {\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    \"RobertaForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  ],\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  \"finetuning_task\": \"mnli\",\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  \"id2label\": {\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    \"0\": \"LABEL_0\",\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    \"1\": \"LABEL_1\",\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    \"2\": \"LABEL_2\"\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  },\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  \"label2id\": {\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    \"LABEL_0\": 0,\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    \"LABEL_1\": 1,\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    \"LABEL_2\": 2\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  },\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  \"layer_norm_eps\": 1e-05,\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  \"max_position_embeddings\": 514,\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  \"model_type\": \"roberta\",\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  \"type_vocab_size\": 1,\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:WARNING:datasets.builder:Reusing dataset glue (/root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[INFO|configuration_utils.py:517] 2022-05-02 13:45:55,197 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[INFO|configuration_utils.py:553] 2022-05-02 13:45:55,198 >> Model config RobertaConfig {\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    \"RobertaForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  ],\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  \"layer_norm_eps\": 1e-05,\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  \"max_position_embeddings\": 514,\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  \"model_type\": \"roberta\",\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  \"type_vocab_size\": 1,\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:[INFO|configuration_utils.py:517] 2022-05-02 13:45:55,209 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:[INFO|configuration_utils.py:553] 2022-05-02 13:45:55,209 >> Model config RobertaConfig {\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    \"RobertaForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  ],\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  \"finetuning_task\": \"mnli\",\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  \"id2label\": {\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    \"0\": \"LABEL_0\",\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    \"1\": \"LABEL_1\",\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    \"2\": \"LABEL_2\"\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  },\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  \"label2id\": {\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    \"LABEL_0\": 0,\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    \"LABEL_1\": 1,\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    \"LABEL_2\": 2\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  },\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  \"layer_norm_eps\": 1e-05,\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  \"max_position_embeddings\": 514,\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  \"model_type\": \"roberta\",\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  \"type_vocab_size\": 1,\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:WARNING:datasets.builder:Reusing dataset glue (/root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:#015Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s][1,6]<stderr>:#015Downloading: 100%|██████████| 899k/899k [00:00<00:00, 43.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|file_utils.py:1536] 2022-05-02 13:45:55,235 >> storing https://huggingface.co/roberta-large/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|file_utils.py:1544] 2022-05-02 13:45:55,235 >> creating metadata file for /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:INFO:filelock:Lock 140162244519192 released on /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab.lock\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:[INFO|configuration_utils.py:517] 2022-05-02 13:45:55,240 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:[INFO|configuration_utils.py:553] 2022-05-02 13:45:55,241 >> Model config RobertaConfig {\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    \"RobertaForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  ],\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  \"layer_norm_eps\": 1e-05,\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  \"max_position_embeddings\": 514,\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  \"model_type\": \"roberta\",\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  \"type_vocab_size\": 1,\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:INFO:filelock:Lock 140162244520032 acquired on /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|file_utils.py:1532] 2022-05-02 13:45:55,270 >> https://huggingface.co/roberta-large/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp5z11dolu\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:[INFO|configuration_utils.py:517] 2022-05-02 13:45:55,271 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:[INFO|configuration_utils.py:553] 2022-05-02 13:45:55,272 >> Model config RobertaConfig {\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    \"RobertaForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  ],\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  \"finetuning_task\": \"mnli\",\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  \"id2label\": {\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    \"0\": \"LABEL_0\",\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    \"1\": \"LABEL_1\",\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    \"2\": \"LABEL_2\"\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  },\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  \"label2id\": {\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    \"LABEL_0\": 0,\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    \"LABEL_1\": 1,\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    \"LABEL_2\": 2\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  },\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  \"layer_norm_eps\": 1e-05,\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  \"max_position_embeddings\": 514,\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  \"model_type\": \"roberta\",\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  \"type_vocab_size\": 1,\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:INFO:filelock:Lock 140293162092360 acquired on /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab.lock\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:INFO:filelock:Lock 140293162092360 released on /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab.lock\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:filelock:Lock 139894013537248 acquired on /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab.lock\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:filelock:Lock 139894013537248 released on /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab.lock\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:filelock:Lock 140624755626952 acquired on /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab.lock\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:filelock:Lock 140624755626952 released on /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab.lock\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:[INFO|configuration_utils.py:517] 2022-05-02 13:45:55,310 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:[INFO|configuration_utils.py:553] 2022-05-02 13:45:55,310 >> Model config RobertaConfig {\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    \"RobertaForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  ],\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  \"layer_norm_eps\": 1e-05,\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  \"max_position_embeddings\": 514,\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  \"model_type\": \"roberta\",\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  \"type_vocab_size\": 1,\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:#015Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s][1,0]<stderr>:[INFO|configuration_utils.py:517] 2022-05-02 13:45:55,318 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|configuration_utils.py:553] 2022-05-02 13:45:55,319 >> Model config RobertaConfig {\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    \"RobertaForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  ],\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"finetuning_task\": \"mnli\",\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"id2label\": {\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    \"0\": \"LABEL_0\",\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    \"1\": \"LABEL_1\",\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    \"2\": \"LABEL_2\"\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  },\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"label2id\": {\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    \"LABEL_0\": 0,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    \"LABEL_1\": 1,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    \"LABEL_2\": 2\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  },\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"layer_norm_eps\": 1e-05,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"max_position_embeddings\": 514,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"model_type\": \"roberta\",\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"type_vocab_size\": 1,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:filelock:Lock 139653026712544 acquired on /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab.lock\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:filelock:Lock 139653026712544 released on /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab.lock\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:#015Downloading: 100%|██████████| 456k/456k [00:00<00:00, 34.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|file_utils.py:1536] 2022-05-02 13:45:55,332 >> storing https://huggingface.co/roberta-large/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|file_utils.py:1544] 2022-05-02 13:45:55,332 >> creating metadata file for /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:INFO:filelock:Lock 140162244520032 released on /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|configuration_utils.py:517] 2022-05-02 13:45:55,349 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:INFO:filelock:Lock 139713413129160 acquired on /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|configuration_utils.py:553] 2022-05-02 13:45:55,350 >> Model config RobertaConfig {\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    \"RobertaForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  ],\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"layer_norm_eps\": 1e-05,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"max_position_embeddings\": 514,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"model_type\": \"roberta\",\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"type_vocab_size\": 1,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:INFO:filelock:Lock 139713413129160 released on /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:INFO:filelock:Lock 140293162093648 acquired on /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:INFO:filelock:Lock 140293162093648 released on /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:filelock:Lock 139894013537080 acquired on /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:filelock:Lock 139894013537080 released on /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:INFO:filelock:Lock 140162244518688 acquired on /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730.lock\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|file_utils.py:1532] 2022-05-02 13:45:55,361 >> https://huggingface.co/roberta-large/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmppjinfayn\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:filelock:Lock 140624755628408 acquired on /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:filelock:Lock 140624755628408 released on /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:#015Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s][1,6]<stderr>:#015Downloading: 100%|██████████| 1.36M/1.36M [00:00<00:00, 37.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|file_utils.py:1536] 2022-05-02 13:45:55,473 >> storing https://huggingface.co/roberta-large/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|file_utils.py:1544] 2022-05-02 13:45:55,473 >> creating metadata file for /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:INFO:filelock:Lock 140162244518688 released on /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730.lock\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:INFO:filelock:Lock 139713413131064 acquired on /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730.lock\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:INFO:filelock:Lock 139713413131064 released on /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730.lock\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:filelock:Lock 139653026710472 acquired on /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730.lock\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:filelock:Lock 139653026710472 released on /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730.lock\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:filelock:Lock 139894013535176 acquired on /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730.lock\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:filelock:Lock 139894013535176 released on /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730.lock\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:filelock:Lock 139790397807360 acquired on /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730.lock\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:filelock:Lock 139790397807360 released on /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730.lock\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:INFO:filelock:Lock 140293162090568 acquired on /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730.lock\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:INFO:filelock:Lock 140293162090568 released on /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730.lock\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:INFO:filelock:Lock 140369533644696 acquired on /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730.lock\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:INFO:filelock:Lock 140369533644696 released on /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730.lock\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:filelock:Lock 140624755627176 acquired on /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730.lock\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:filelock:Lock 140624755627176 released on /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730.lock\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,575 >> loading file https://huggingface.co/roberta-large/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,575 >> loading file https://huggingface.co/roberta-large/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,575 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,575 >> loading file https://huggingface.co/roberta-large/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,575 >> loading file https://huggingface.co/roberta-large/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,575 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer_config.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,585 >> loading file https://huggingface.co/roberta-large/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,585 >> loading file https://huggingface.co/roberta-large/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,585 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,585 >> loading file https://huggingface.co/roberta-large/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,585 >> loading file https://huggingface.co/roberta-large/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,585 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer_config.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,586 >> loading file https://huggingface.co/roberta-large/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,586 >> loading file https://huggingface.co/roberta-large/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,586 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,586 >> loading file https://huggingface.co/roberta-large/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,586 >> loading file https://huggingface.co/roberta-large/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,586 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer_config.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,595 >> loading file https://huggingface.co/roberta-large/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,595 >> loading file https://huggingface.co/roberta-large/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,595 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,595 >> loading file https://huggingface.co/roberta-large/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,596 >> loading file https://huggingface.co/roberta-large/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,596 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer_config.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,600 >> loading file https://huggingface.co/roberta-large/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,601 >> loading file https://huggingface.co/roberta-large/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,601 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,601 >> loading file https://huggingface.co/roberta-large/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,601 >> loading file https://huggingface.co/roberta-large/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,601 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer_config.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,602 >> loading file https://huggingface.co/roberta-large/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,602 >> loading file https://huggingface.co/roberta-large/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,603 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,603 >> loading file https://huggingface.co/roberta-large/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,603 >> loading file https://huggingface.co/roberta-large/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,602 >> loading file https://huggingface.co/roberta-large/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,603 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer_config.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,603 >> loading file https://huggingface.co/roberta-large/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,603 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,603 >> loading file https://huggingface.co/roberta-large/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,603 >> loading file https://huggingface.co/roberta-large/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,603 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer_config.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,618 >> loading file https://huggingface.co/roberta-large/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,618 >> loading file https://huggingface.co/roberta-large/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,619 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,619 >> loading file https://huggingface.co/roberta-large/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,619 >> loading file https://huggingface.co/roberta-large/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[INFO|tokenization_utils_base.py:1717] 2022-05-02 13:45:55,619 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer_config.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:INFO:filelock:Lock 139713413070632 acquired on /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352.lock\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:[INFO|file_utils.py:1532] 2022-05-02 13:45:55,705 >> https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmppc0y1qta\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:#015Downloading:   0%|          | 0.00/1.43G [00:00<?, ?B/s][1,7]<stderr>:#015Downloading:   0%|          | 4.76M/1.43G [00:00<00:29, 47.6MB/s][1,7]<stderr>:#015Downloading:   1%|          | 9.69M/1.43G [00:00<00:29, 48.1MB/s][1,7]<stderr>:#015Downloading:   1%|          | 14.7M/1.43G [00:00<00:28, 48.8MB/s][1,7]<stderr>:#015Downloading:   1%|▏         | 19.8M/1.43G [00:00<00:28, 49.4MB/s][1,7]<stderr>:#015Downloading:   2%|▏         | 25.0M/1.43G [00:00<00:28, 50.0MB/s][1,7]<stderr>:#015Downloading:   2%|▏         | 30.2M/1.43G [00:00<00:27, 50.6MB/s][1,7]<stderr>:#015Downloading:   2%|▏         | 35.4M/1.43G [00:00<00:27, 51.0MB/s][1,7]<stderr>:#015Downloading:   3%|▎         | 40.7M/1.43G [00:00<00:26, 51.5MB/s][1,7]<stderr>:#015Downloading:   3%|▎         | 45.9M/1.43G [00:00<00:26, 51.7MB/s][1,7]<stderr>:#015Downloading:   4%|▎         | 51.1M/1.43G [00:01<00:26, 51.8MB/s][1,7]<stderr>:#015Downloading:   4%|▍         | 56.4M/1.43G [00:01<00:26, 52.2MB/s][1,7]<stderr>:#015Downloading:   4%|▍         | 61.7M/1.43G [00:01<00:26, 52.5MB/s][1,7]<stderr>:#015Downloading:   5%|▍         | 67.1M/1.43G [00:01<00:25, 52.8MB/s][1,7]<stderr>:#015Downloading:   5%|▌         | 72.5M/1.43G [00:01<00:25, 53.1MB/s][1,7]<stderr>:#015Downloading:   5%|▌         | 77.8M/1.43G [00:01<00:25, 53.3MB/s][1,7]<stderr>:#015Downloading:   6%|▌         | 83.2M/1.43G [00:01<00:25, 53.4MB/s][1,7]<stderr>:#015Downloading:   6%|▌         | 88.6M/1.43G [00:01<00:25, 53.4MB/s][1,7]<stderr>:#015Downloading:   7%|▋         | 94.0M/1.43G [00:01<00:24, 53.6MB/s][1,7]<stderr>:#015Downloading:   7%|▋         | 99.3M/1.43G [00:01<00:24, 53.6MB/s][1,7]<stderr>:#015Downloading:   7%|▋         | 105M/1.43G [00:02<00:24, 53.7MB/s] [1,7]<stderr>:#015Downloading:   8%|▊         | 110M/1.43G [00:02<00:24, 53.8MB/s][1,7]<stderr>:#015Downloading:   8%|▊         | 115M/1.43G [00:02<00:24, 53.8MB/s][1,7]<stderr>:#015Downloading:   8%|▊         | 121M/1.43G [00:02<00:24, 54.0MB/s][1,7]<stderr>:#015Downloading:   9%|▉         | 126M/1.43G [00:02<00:24, 54.1MB/s][1,7]<stderr>:#015Downloading:   9%|▉         | 132M/1.43G [00:02<00:23, 54.2MB/s][1,7]<stderr>:#015Downloading:  10%|▉         | 137M/1.43G [00:02<00:23, 54.4MB/s][1,7]<stderr>:#015Downloading:  10%|█         | 143M/1.43G [00:02<00:23, 54.6MB/s][1,7]<stderr>:#015Downloading:  10%|█         | 148M/1.43G [00:02<00:23, 54.7MB/s][1,7]<stderr>:#015Downloading:  11%|█         | 154M/1.43G [00:02<00:23, 54.7MB/s][1,7]<stderr>:#015Downloading:  11%|█         | 159M/1.43G [00:03<00:23, 54.9MB/s][1,7]<stderr>:#015Downloading:  12%|█▏        | 165M/1.43G [00:03<00:22, 54.9MB/s][1,7]<stderr>:#015Downloading:  12%|█▏        | 170M/1.43G [00:03<00:22, 54.9MB/s][1,7]<stderr>:#015Downloading:  12%|█▏        | 176M/1.43G [00:03<00:22, 54.8MB/s][1,7]<stderr>:#015Downloading:  13%|█▎        | 181M/1.43G [00:03<00:22, 54.9MB/s][1,7]<stderr>:#015Downloading:  13%|█▎        | 187M/1.43G [00:03<00:22, 54.7MB/s][1,7]<stderr>:#015Downloading:  13%|█▎        | 192M/1.43G [00:03<00:22, 54.7MB/s][1,7]<stderr>:#015Downloading:  14%|█▍        | 198M/1.43G [00:03<00:22, 54.7MB/s][1,7]<stderr>:#015Downloading:  14%|█▍        | 203M/1.43G [00:03<00:22, 54.6MB/s][1,7]<stderr>:#015Downloading:  15%|█▍        | 209M/1.43G [00:03<00:22, 54.8MB/s][1,7]<stderr>:#015Downloading:  15%|█▌        | 214M/1.43G [00:04<00:22, 54.7MB/s][1,7]<stderr>:#015Downloading:  15%|█▌        | 220M/1.43G [00:04<00:21, 54.9MB/s][1,7]<stderr>:#015Downloading:  16%|█▌        | 225M/1.43G [00:04<00:21, 54.9MB/s][1,7]<stderr>:#015Downloading:  16%|█▌        | 231M/1.43G [00:04<00:21, 55.1MB/s][1,7]<stderr>:#015Downloading:  17%|█▋        | 236M/1.43G [00:04<00:21, 55.1MB/s][1,7]<stderr>:#015Downloading:  17%|█▋        | 242M/1.43G [00:04<00:21, 55.2MB/s][1,7]<stderr>:#015Downloading:  17%|█▋        | 247M/1.43G [00:04<00:21, 55.3MB/s][1,7]<stderr>:#015Downloading:  18%|█▊        | 253M/1.43G [00:04<00:21, 55.2MB/s][1,7]<stderr>:#015Downloading:  18%|█▊        | 258M/1.43G [00:04<00:21, 55.1MB/s][1,7]<stderr>:#015Downloading:  19%|█▊        | 264M/1.43G [00:04<00:21, 52.9MB/s][1,7]<stderr>:#015Downloading:  19%|█▉        | 269M/1.43G [00:05<00:29, 38.6MB/s][1,7]<stderr>:#015Downloading:  19%|█▉        | 274M/1.43G [00:05<00:27, 41.7MB/s][1,7]<stderr>:#015Downloading:  20%|█▉        | 279M/1.43G [00:05<00:26, 43.8MB/s][1,7]<stderr>:#015Downloading:  20%|█▉        | 284M/1.43G [00:05<00:25, 44.6MB/s][1,7]<stderr>:#015Downloading:  20%|██        | 289M/1.43G [00:05<00:25, 45.5MB/s][1,7]<stderr>:#015Downloading:  21%|██        | 294M/1.43G [00:05<00:24, 46.1MB/s][1,7]<stderr>:#015Downloading:  21%|██        | 299M/1.43G [00:05<00:23, 47.8MB/s][1,7]<stderr>:#015Downloading:  21%|██▏       | 304M/1.43G [00:05<00:22, 48.9MB/s][1,7]<stderr>:#015Downloading:  22%|██▏       | 309M/1.43G [00:05<00:22, 49.3MB/s][1,7]<stderr>:#015Downloading:  22%|██▏       | 314M/1.43G [00:06<00:22, 48.5MB/s][1,7]<stderr>:#015Downloading:  22%|██▏       | 319M/1.43G [00:06<00:22, 48.8MB/s][1,7]<stderr>:#015Downloading:  23%|██▎       | 324M/1.43G [00:06<00:22, 49.8MB/s][1,7]<stderr>:#015Downloading:  23%|██▎       | 329M/1.43G [00:06<00:21, 49.9MB/s][1,7]<stderr>:#015Downloading:  23%|██▎       | 335M/1.43G [00:06<00:21, 50.9MB/s][1,7]<stderr>:#015Downloading:  24%|██▍       | 340M/1.43G [00:06<00:21, 51.3MB/s][1,7]<stderr>:#015Downloading:  24%|██▍       | 345M/1.43G [00:06<00:21, 51.0MB/s][1,7]<stderr>:#015Downloading:  25%|██▍       | 350M/1.43G [00:06<00:21, 51.1MB/s][1,7]<stderr>:#015Downloading:  25%|██▍       | 355M/1.43G [00:06<00:20, 51.7MB/s][1,7]<stderr>:#015Downloading:  25%|██▌       | 361M/1.43G [00:06<00:20, 51.6MB/s][1,7]<stderr>:#015Downloading:  26%|██▌       | 366M/1.43G [00:07<00:20, 52.4MB/s][1,7]<stderr>:#015Downloading:  26%|██▌       | 371M/1.43G [00:07<00:20, 51.9MB/s][1,7]<stderr>:#015Downloading:  26%|██▋       | 376M/1.43G [00:07<00:20, 51.7MB/s][1,7]<stderr>:#015Downloading:  27%|██▋       | 382M/1.43G [00:07<00:20, 51.4MB/s][1,7]<stderr>:#015Downloading:  27%|██▋       | 387M/1.43G [00:07<00:19, 52.0MB/s][1,7]<stderr>:#015Downloading:  28%|██▊       | 392M/1.43G [00:07<00:19, 51.8MB/s][1,7]<stderr>:#015Downloading:  28%|██▊       | 397M/1.43G [00:07<00:20, 51.0MB/s][1,7]<stderr>:#015Downloading:  28%|██▊       | 403M/1.43G [00:07<00:19, 51.4MB/s][1,7]<stderr>:#015Downloading:  29%|██▊       | 408M/1.43G [00:07<00:19, 51.5MB/s][1,7]<stderr>:#015Downloading:  29%|██▉       | 413M/1.43G [00:07<00:19, 52.4MB/s][1,7]<stderr>:#015Downloading:  29%|██▉       | 419M/1.43G [00:08<00:18, 53.3MB/s][1,7]<stderr>:#015Downloading:  30%|██▉       | 424M/1.43G [00:08<00:18, 54.0MB/s][1,7]<stderr>:#015Downloading:  30%|███       | 430M/1.43G [00:08<00:18, 54.6MB/s][1,7]<stderr>:#015Downloading:  31%|███       | 436M/1.43G [00:08<00:18, 54.9MB/s][1,7]<stderr>:#015Downloading:  31%|███       | 441M/1.43G [00:08<00:17, 55.0MB/s][1,7]<stderr>:#015Downloading:  31%|███▏      | 447M/1.43G [00:08<00:18, 54.0MB/s][1,7]<stderr>:#015Downloading:  32%|███▏      | 452M/1.43G [00:08<00:18, 52.8MB/s][1,7]<stderr>:#015Downloading:  32%|███▏      | 457M/1.43G [00:08<00:18, 52.9MB/s][1,7]<stderr>:#015Downloading:  32%|███▏      | 463M/1.43G [00:08<00:18, 52.1MB/s][1,7]<stderr>:#015Downloading:  33%|███▎      | 468M/1.43G [00:08<00:18, 53.1MB/s][1,7]<stderr>:#015Downloading:  33%|███▎      | 474M/1.43G [00:09<00:17, 53.6MB/s][1,7]<stderr>:#015Downloading:  34%|███▎      | 479M/1.43G [00:09<00:17, 54.2MB/s][1,7]<stderr>:#015Downloading:  34%|███▍      | 485M/1.43G [00:09<00:17, 54.8MB/s][1,7]<stderr>:#015Downloading:  34%|███▍      | 490M/1.43G [00:09<00:16, 55.1MB/s][1,7]<stderr>:#015Downloading:  35%|███▍      | 496M/1.43G [00:09<00:16, 55.4MB/s][1,7]<stderr>:#015Downloading:  35%|███▌      | 502M/1.43G [00:09<00:17, 53.0MB/s][1,7]<stderr>:#015Downloading:  36%|███▌      | 507M/1.43G [00:09<00:17, 53.4MB/s][1,7]<stderr>:#015Downloading:  36%|███▌      | 512M/1.43G [00:09<00:17, 52.6MB/s][1,7]<stderr>:#015Downloading:  36%|███▋      | 518M/1.43G [00:09<00:16, 54.8MB/s][1,7]<stderr>:#015Downloading:  37%|███▋      | 524M/1.43G [00:10<00:16, 55.3MB/s][1,7]<stderr>:#015Downloading:  37%|███▋      | 530M/1.43G [00:10<00:16, 55.8MB/s][1,7]<stderr>:#015Downloading:  38%|███▊      | 536M/1.43G [00:10<00:15, 57.0MB/s][1,7]<stderr>:#015Downloading:  38%|███▊      | 541M/1.43G [00:10<00:15, 56.4MB/s][1,7]<stderr>:#015Downloading:  38%|███▊      | 547M/1.43G [00:10<00:15, 57.6MB/s][1,7]<stderr>:#015Downloading:  39%|███▉      | 553M/1.43G [00:10<00:14, 58.2MB/s][1,7]<stderr>:#015Downloading:  39%|███▉      | 559M/1.43G [00:10<00:14, 58.7MB/s][1,7]<stderr>:#015Downloading:  40%|███▉      | 566M/1.43G [00:10<00:14, 59.2MB/s][1,7]<stderr>:#015Downloading:  40%|████      | 571M/1.43G [00:10<00:14, 59.1MB/s][1,7]<stderr>:#015Downloading:  40%|████      | 577M/1.43G [00:10<00:14, 57.4MB/s][1,7]<stderr>:#015Downloading:  41%|████      | 583M/1.43G [00:11<00:14, 57.6MB/s][1,7]<stderr>:#015Downloading:  41%|████▏     | 589M/1.43G [00:11<00:14, 58.1MB/s][1,7]<stderr>:#015Downloading:  42%|████▏     | 595M/1.43G [00:11<00:14, 58.4MB/s][1,7]<stderr>:#015Downloading:  42%|████▏     | 601M/1.43G [00:11<00:14, 58.7MB/s][1,7]<stderr>:#015Downloading:  43%|████▎     | 607M/1.43G [00:11<00:14, 57.9MB/s][1,7]<stderr>:#015Downloading:  43%|████▎     | 613M/1.43G [00:11<00:13, 58.6MB/s][1,7]<stderr>:#015Downloading:  43%|████▎     | 619M/1.43G [00:11<00:13, 58.9MB/s][1,7]<stderr>:#015Downloading:  44%|████▍     | 625M/1.43G [00:11<00:13, 59.3MB/s][1,7]<stderr>:#015Downloading:  44%|████▍     | 631M/1.43G [00:11<00:13, 58.4MB/s][1,7]<stderr>:#015Downloading:  45%|████▍     | 637M/1.43G [00:11<00:13, 58.2MB/s][1,7]<stderr>:#015Downloading:  45%|████▌     | 642M/1.43G [00:12<00:13, 57.0MB/s][1,7]<stderr>:#015Downloading:  45%|████▌     | 648M/1.43G [00:12<00:13, 57.8MB/s][1,7]<stderr>:#015Downloading:  46%|████▌     | 654M/1.43G [00:12<00:13, 58.5MB/s][1,7]<stderr>:#015Downloading:  46%|████▋     | 660M/1.43G [00:12<00:13, 56.8MB/s][1,7]<stderr>:#015Downloading:  47%|████▋     | 666M/1.43G [00:12<00:13, 58.0MB/s][1,7]<stderr>:#015Downloading:  47%|████▋     | 672M/1.43G [00:12<00:12, 58.6MB/s][1,7]<stderr>:#015Downloading:  48%|████▊     | 678M/1.43G [00:12<00:13, 57.4MB/s][1,7]<stderr>:#015Downloading:  48%|████▊     | 684M/1.43G [00:12<00:12, 57.8MB/s][1,7]<stderr>:#015Downloading:  48%|████▊     | 690M/1.43G [00:12<00:12, 58.0MB/s][1,7]<stderr>:#015Downloading:  49%|████▉     | 696M/1.43G [00:12<00:12, 58.0MB/s][1,7]<stderr>:#015Downloading:  49%|████▉     | 702M/1.43G [00:13<00:12, 56.7MB/s][1,7]<stderr>:#015Downloading:  50%|████▉     | 708M/1.43G [00:13<00:12, 57.4MB/s][1,7]<stderr>:#015Downloading:  50%|█████     | 714M/1.43G [00:13<00:12, 58.1MB/s][1,7]<stderr>:#015Downloading:  50%|█████     | 719M/1.43G [00:13<00:12, 57.4MB/s][1,7]<stderr>:#015Downloading:  51%|█████     | 725M/1.43G [00:13<00:12, 56.0MB/s][1,7]<stderr>:#015Downloading:  51%|█████     | 731M/1.43G [00:13<00:12, 54.9MB/s][1,7]<stderr>:#015Downloading:  52%|█████▏    | 736M/1.43G [00:13<00:12, 55.5MB/s][1,7]<stderr>:#015Downloading:  52%|█████▏    | 742M/1.43G [00:13<00:12, 54.9MB/s][1,7]<stderr>:#015Downloading:  52%|█████▏    | 748M/1.43G [00:13<00:12, 53.2MB/s][1,7]<stderr>:#015Downloading:  53%|█████▎    | 753M/1.43G [00:14<00:12, 54.7MB/s][1,7]<stderr>:#015Downloading:  53%|█████▎    | 759M/1.43G [00:14<00:12, 54.4MB/s][1,7]<stderr>:#015Downloading:  54%|█████▎    | 764M/1.43G [00:14<00:12, 53.7MB/s][1,7]<stderr>:#015Downloading:  54%|█████▍    | 770M/1.43G [00:14<00:12, 53.6MB/s][1,7]<stderr>:#015Downloading:  54%|█████▍    | 775M/1.43G [00:14<00:12, 53.2MB/s][1,7]<stderr>:#015Downloading:  55%|█████▍    | 780M/1.43G [00:14<00:12, 53.0MB/s][1,7]<stderr>:#015Downloading:  55%|█████▌    | 786M/1.43G [00:14<00:11, 53.5MB/s][1,7]<stderr>:#015Downloading:  56%|█████▌    | 791M/1.43G [00:14<00:11, 54.2MB/s][1,7]<stderr>:#015Downloading:  56%|█████▌    | 797M/1.43G [00:14<00:11, 55.0MB/s][1,7]<stderr>:#015Downloading:  56%|█████▋    | 803M/1.43G [00:14<00:11, 56.0MB/s][1,7]<stderr>:#015Downloading:  57%|█████▋    | 809M/1.43G [00:15<00:11, 54.8MB/s][1,7]<stderr>:#015Downloading:  57%|█████▋    | 815M/1.43G [00:15<00:10, 56.1MB/s][1,7]<stderr>:#015Downloading:  58%|█████▊    | 820M/1.43G [00:15<00:10, 56.8MB/s][1,7]<stderr>:#015Downloading:  58%|█████▊    | 826M/1.43G [00:15<00:10, 57.0MB/s][1,7]<stderr>:#015Downloading:  58%|█████▊    | 832M/1.43G [00:15<00:10, 55.8MB/s][1,7]<stderr>:#015Downloading:  59%|█████▊    | 837M/1.43G [00:15<00:10, 55.1MB/s][1,7]<stderr>:#015Downloading:  59%|█████▉    | 843M/1.43G [00:15<00:10, 55.3MB/s][1,7]<stderr>:#015Downloading:  60%|█████▉    | 849M/1.43G [00:15<00:10, 54.7MB/s][1,7]<stderr>:#015Downloading:  60%|█████▉    | 854M/1.43G [00:15<00:10, 54.5MB/s][1,7]<stderr>:#015Downloading:  60%|██████    | 860M/1.43G [00:15<00:10, 56.0MB/s][1,7]<stderr>:#015Downloading:  61%|██████    | 866M/1.43G [00:16<00:09, 56.8MB/s][1,7]<stderr>:#015Downloading:  61%|██████    | 872M/1.43G [00:16<00:09, 57.4MB/s][1,7]<stderr>:#015Downloading:  62%|██████▏   | 878M/1.43G [00:16<00:09, 56.5MB/s][1,7]<stderr>:#015Downloading:  62%|██████▏   | 883M/1.43G [00:16<00:09, 55.0MB/s][1,7]<stderr>:#015Downloading:  62%|██████▏   | 889M/1.43G [00:16<00:09, 54.8MB/s][1,7]<stderr>:#015Downloading:  63%|██████▎   | 894M/1.43G [00:16<00:09, 54.4MB/s][1,7]<stderr>:#015Downloading:  63%|██████▎   | 900M/1.43G [00:16<00:09, 55.9MB/s][1,7]<stderr>:#015Downloading:  64%|██████▎   | 906M/1.43G [00:16<00:09, 56.8MB/s][1,7]<stderr>:#015Downloading:  64%|██████▍   | 912M/1.43G [00:16<00:08, 57.5MB/s][1,7]<stderr>:#015Downloading:  64%|██████▍   | 918M/1.43G [00:16<00:08, 57.9MB/s][1,7]<stderr>:#015Downloading:  65%|██████▍   | 924M/1.43G [00:17<00:08, 58.1MB/s][1,7]<stderr>:#015Downloading:  65%|██████▌   | 930M/1.43G [00:17<00:08, 58.3MB/s][1,7]<stderr>:#015Downloading:  66%|██████▌   | 936M/1.43G [00:17<00:08, 58.4MB/s][1,7]<stderr>:#015Downloading:  66%|██████▌   | 941M/1.43G [00:17<00:08, 58.6MB/s][1,7]<stderr>:#015Downloading:  66%|██████▋   | 947M/1.43G [00:17<00:08, 58.9MB/s][1,7]<stderr>:#015Downloading:  67%|██████▋   | 953M/1.43G [00:17<00:08, 59.0MB/s][1,7]<stderr>:#015Downloading:  67%|██████▋   | 959M/1.43G [00:17<00:07, 59.0MB/s][1,7]<stderr>:#015Downloading:  68%|██████▊   | 965M/1.43G [00:17<00:07, 59.0MB/s][1,7]<stderr>:#015Downloading:  68%|██████▊   | 971M/1.43G [00:17<00:07, 58.8MB/s][1,7]<stderr>:#015Downloading:  69%|██████▊   | 977M/1.43G [00:17<00:07, 58.9MB/s][1,7]<stderr>:#015Downloading:  69%|██████▉   | 983M/1.43G [00:18<00:07, 58.9MB/s][1,7]<stderr>:#015Downloading:  69%|██████▉   | 989M/1.43G [00:18<00:07, 58.9MB/s][1,7]<stderr>:#015Downloading:  70%|██████▉   | 995M/1.43G [00:18<00:07, 58.9MB/s][1,7]<stderr>:#015Downloading:  70%|███████   | 1.00G/1.43G [00:18<00:07, 59.0MB/s][1,7]<stderr>:#015Downloading:  71%|███████   | 1.01G/1.43G [00:18<00:07, 59.0MB/s][1,7]<stderr>:#015Downloading:  71%|███████   | 1.01G/1.43G [00:18<00:06, 59.1MB/s][1,7]<stderr>:#015Downloading:  71%|███████▏  | 1.02G/1.43G [00:18<00:06, 59.3MB/s][1,7]<stderr>:#015Downloading:  72%|███████▏  | 1.02G/1.43G [00:18<00:06, 59.3MB/s][1,7]<stderr>:#015Downloading:  72%|███████▏  | 1.03G/1.43G [00:18<00:06, 59.1MB/s][1,7]<stderr>:#015Downloading:  73%|███████▎  | 1.04G/1.43G [00:18<00:06, 59.2MB/s][1,7]<stderr>:#015Downloading:  73%|███████▎  | 1.04G/1.43G [00:19<00:06, 59.1MB/s][1,7]<stderr>:#015Downloading\u001b[0m\n",
      "\u001b[34m:  73%|███████▎  | 1.05G/1.43G [00:19<00:06, 58.9MB/s][1,7]<stderr>:#015Downloading:  74%|███████▍  | 1.05G/1.43G [00:19<00:06, 59.0MB/s][1,7]<stderr>:#015Downloading:  74%|███████▍  | 1.06G/1.43G [00:19<00:06, 59.0MB/s][1,7]<stderr>:#015Downloading:  75%|███████▍  | 1.07G/1.43G [00:19<00:06, 59.0MB/s][1,7]<stderr>:#015Downloading:  75%|███████▌  | 1.07G/1.43G [00:19<00:06, 58.9MB/s][1,7]<stderr>:#015Downloading:  76%|███████▌  | 1.08G/1.43G [00:19<00:05, 59.0MB/s][1,7]<stderr>:#015Downloading:  76%|███████▌  | 1.08G/1.43G [00:19<00:05, 59.1MB/s][1,7]<stderr>:#015Downloading:  76%|███████▋  | 1.09G/1.43G [00:19<00:05, 59.0MB/s][1,7]<stderr>:#015Downloading:  77%|███████▋  | 1.10G/1.43G [00:19<00:05, 59.0MB/s][1,7]<stderr>:#015Downloading:  77%|███████▋  | 1.10G/1.43G [00:20<00:05, 58.9MB/s][1,7]<stderr>:#015Downloading:  78%|███████▊  | 1.11G/1.43G [00:20<00:05, 58.5MB/s][1,7]<stderr>:#015Downloading:  78%|███████▊  | 1.11G/1.43G [00:20<00:05, 58.7MB/s][1,7]<stderr>:#015Downloading:  78%|███████▊  | 1.12G/1.43G [00:20<00:05, 58.8MB/s][1,7]<stderr>:#015Downloading:  79%|███████▉  | 1.12G/1.43G [00:20<00:05, 59.0MB/s][1,7]<stderr>:#015Downloading:  79%|███████▉  | 1.13G/1.43G [00:20<00:04, 59.1MB/s][1,7]<stderr>:#015Downloading:  80%|███████▉  | 1.14G/1.43G [00:20<00:04, 59.1MB/s][1,7]<stderr>:#015Downloading:  80%|████████  | 1.14G/1.43G [00:20<00:04, 59.0MB/s][1,7]<stderr>:#015Downloading:  81%|████████  | 1.15G/1.43G [00:20<00:04, 59.1MB/s][1,7]<stderr>:#015Downloading:  81%|████████  | 1.15G/1.43G [00:20<00:04, 58.8MB/s][1,7]<stderr>:#015Downloading:  81%|████████▏ | 1.16G/1.43G [00:21<00:04, 58.7MB/s][1,7]<stderr>:#015Downloading:  82%|████████▏ | 1.17G/1.43G [00:21<00:04, 58.8MB/s][1,7]<stderr>:#015Downloading:  82%|████████▏ | 1.17G/1.43G [00:21<00:04, 59.0MB/s][1,7]<stderr>:#015Downloading:  83%|████████▎ | 1.18G/1.43G [00:21<00:04, 59.1MB/s][1,7]<stderr>:#015Downloading:  83%|████████▎ | 1.18G/1.43G [00:21<00:04, 59.0MB/s][1,7]<stderr>:#015Downloading:  83%|████████▎ | 1.19G/1.43G [00:21<00:04, 58.7MB/s][1,7]<stderr>:#015Downloading:  84%|████████▍ | 1.20G/1.43G [00:21<00:03, 59.0MB/s][1,7]<stderr>:#015Downloading:  84%|████████▍ | 1.20G/1.43G [00:21<00:03, 59.0MB/s][1,7]<stderr>:#015Downloading:  85%|████████▍ | 1.21G/1.43G [00:21<00:03, 59.3MB/s][1,7]<stderr>:#015Downloading:  85%|████████▌ | 1.21G/1.43G [00:21<00:03, 59.1MB/s][1,7]<stderr>:#015Downloading:  86%|████████▌ | 1.22G/1.43G [00:22<00:03, 59.1MB/s][1,7]<stderr>:#015Downloading:  86%|████████▌ | 1.23G/1.43G [00:22<00:03, 59.2MB/s][1,7]<stderr>:#015Downloading:  86%|████████▋ | 1.23G/1.43G [00:22<00:03, 59.2MB/s][1,7]<stderr>:#015Downloading:  87%|████████▋ | 1.24G/1.43G [00:22<00:03, 59.2MB/s][1,7]<stderr>:#015Downloading:  87%|████████▋ | 1.24G/1.43G [00:22<00:03, 58.9MB/s][1,7]<stderr>:#015Downloading:  88%|████████▊ | 1.25G/1.43G [00:22<00:03, 56.9MB/s][1,7]<stderr>:#015Downloading:  88%|████████▊ | 1.26G/1.43G [00:22<00:03, 56.4MB/s][1,7]<stderr>:#015Downloading:  88%|████████▊ | 1.26G/1.43G [00:22<00:02, 56.1MB/s][1,7]<stderr>:#015Downloading:  89%|████████▉ | 1.27G/1.43G [00:22<00:02, 57.2MB/s][1,7]<stderr>:#015Downloading:  89%|████████▉ | 1.27G/1.43G [00:23<00:02, 55.9MB/s][1,7]<stderr>:#015Downloading:  90%|████████▉ | 1.28G/1.43G [00:23<00:02, 55.1MB/s][1,7]<stderr>:#015Downloading:  90%|█████████ | 1.28G/1.43G [00:23<00:02, 54.8MB/s][1,7]<stderr>:#015Downloading:  90%|█████████ | 1.29G/1.43G [00:23<00:02, 54.4MB/s][1,7]<stderr>:#015Downloading:  91%|█████████ | 1.29G/1.43G [00:23<00:02, 53.9MB/s][1,7]<stderr>:#015Downloading:  91%|█████████ | 1.30G/1.43G [00:23<00:02, 53.7MB/s][1,7]<stderr>:#015Downloading:  92%|█████████▏| 1.31G/1.43G [00:23<00:02, 54.8MB/s][1,7]<stderr>:#015Downloading:  92%|█████████▏| 1.31G/1.43G [00:23<00:02, 54.8MB/s][1,7]<stderr>:#015Downloading:  92%|█████████▏| 1.32G/1.43G [00:23<00:01, 56.1MB/s][1,7]<stderr>:#015Downloading:  93%|█████████▎| 1.32G/1.43G [00:23<00:01, 56.9MB/s][1,7]<stderr>:#015Downloading:  93%|█████████▎| 1.33G/1.43G [00:24<00:01, 56.2MB/s][1,7]<stderr>:#015Downloading:  94%|█████████▎| 1.33G/1.43G [00:24<00:01, 55.1MB/s][1,7]<stderr>:#015Downloading:  94%|█████████▍| 1.34G/1.43G [00:24<00:01, 56.4MB/s][1,7]<stderr>:#015Downloading:  94%|█████████▍| 1.35G/1.43G [00:24<00:01, 54.8MB/s][1,7]<stderr>:#015Downloading:  95%|█████████▍| 1.35G/1.43G [00:24<00:01, 54.9MB/s][1,7]<stderr>:#015Downloading:  95%|█████████▌| 1.36G/1.43G [00:24<00:01, 54.7MB/s][1,7]<stderr>:#015Downloading:  96%|█████████▌| 1.36G/1.43G [00:24<00:01, 54.4MB/s][1,7]<stderr>:#015Downloading:  96%|█████████▌| 1.37G/1.43G [00:24<00:01, 54.2MB/s][1,7]<stderr>:#015Downloading:  96%|█████████▋| 1.37G/1.43G [00:24<00:00, 55.9MB/s][1,7]<stderr>:#015Downloading:  97%|█████████▋| 1.38G/1.43G [00:24<00:00, 54.9MB/s][1,7]<stderr>:#015Downloading:  97%|█████████▋| 1.39G/1.43G [00:25<00:00, 56.2MB/s][1,7]<stderr>:#015Downloading:  98%|█████████▊| 1.39G/1.43G [00:25<00:00, 57.0MB/s][1,7]<stderr>:#015Downloading:  98%|█████████▊| 1.40G/1.43G [00:25<00:00, 57.6MB/s][1,7]<stderr>:#015Downloading:  98%|█████████▊| 1.40G/1.43G [00:25<00:00, 55.3MB/s][1,7]<stderr>:#015Downloading:  99%|█████████▉| 1.41G/1.43G [00:25<00:00, 55.4MB/s][1,7]<stderr>:#015Downloading:  99%|█████████▉| 1.41G/1.43G [00:25<00:00, 55.8MB/s][1,7]<stderr>:#015Downloading: 100%|█████████▉| 1.42G/1.43G [00:25<00:00, 55.7MB/s][1,7]<stderr>:#015Downloading: 100%|█████████▉| 1.43G/1.43G [00:25<00:00, 56.5MB/s][1,7]<stderr>:#015Downloading: 100%|██████████| 1.43G/1.43G [00:25<00:00, 55.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:[INFO|file_utils.py:1536] 2022-05-02 13:46:21,507 >> storing https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:[INFO|file_utils.py:1544] 2022-05-02 13:46:21,507 >> creating metadata file for /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:INFO:filelock:Lock 139713413070632 released on /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352.lock\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:[INFO|modeling_utils.py:1155] 2022-05-02 13:46:21,508 >> loading weights file https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:filelock:Lock 139653026668328 acquired on /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352.lock\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:filelock:Lock 139653026668328 released on /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352.lock\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:filelock:Lock 140624755570672 acquired on /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352.lock\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:filelock:Lock 140624755570672 released on /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352.lock\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[INFO|modeling_utils.py:1155] 2022-05-02 13:46:21,535 >> loading weights file https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[INFO|modeling_utils.py:1155] 2022-05-02 13:46:21,535 >> loading weights file https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:filelock:Lock 139790397751024 acquired on /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352.lock\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:filelock:Lock 139790397751024 released on /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352.lock\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|modeling_utils.py:1155] 2022-05-02 13:46:21,585 >> loading weights file https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:INFO:filelock:Lock 140162263224616 acquired on /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352.lock\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:INFO:filelock:Lock 140162263224616 released on /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352.lock\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|modeling_utils.py:1155] 2022-05-02 13:46:21,636 >> loading weights file https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:filelock:Lock 139894013484840 acquired on /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352.lock\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:filelock:Lock 139894013484840 released on /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352.lock\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[INFO|modeling_utils.py:1155] 2022-05-02 13:46:21,686 >> loading weights file https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:INFO:filelock:Lock 140293162034960 acquired on /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352.lock\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:INFO:filelock:Lock 140293162034960 released on /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352.lock\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[INFO|modeling_utils.py:1155] 2022-05-02 13:46:21,736 >> loading weights file https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:INFO:filelock:Lock 140369538641592 acquired on /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352.lock\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:INFO:filelock:Lock 140369538641592 released on /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352.lock\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:[INFO|modeling_utils.py:1155] 2022-05-02 13:46:21,786 >> loading weights file https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[WARNING|modeling_utils.py:1331] 2022-05-02 13:46:26,623 >> Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[WARNING|modeling_utils.py:1342] 2022-05-02 13:46:26,623 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:#015  0%|          | 0/393 [00:00<?, ?ba/s][1,0]<stderr>:[WARNING|modeling_utils.py:1331] 2022-05-02 13:46:26,878 >> Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias']\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[WARNING|modeling_utils.py:1342] 2022-05-02 13:46:26,878 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:[WARNING|modeling_utils.py:1331] 2022-05-02 13:46:26,882 >> Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight']\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:[WARNING|modeling_utils.py:1342] 2022-05-02 13:46:26,882 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[WARNING|modeling_utils.py:1331] 2022-05-02 13:46:26,885 >> Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.weight']\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[WARNING|modeling_utils.py:1342] 2022-05-02 13:46:26,885 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:#015  0%|          | 1/393 [00:00<01:24,  4.61ba/s][1,6]<stderr>:[WARNING|modeling_utils.py:1331] 2022-05-02 13:46:26,933 >> Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[WARNING|modeling_utils.py:1342] 2022-05-02 13:46:26,933 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[WARNING|modeling_utils.py:1331] 2022-05-02 13:46:26,934 >> Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[WARNING|modeling_utils.py:1342] 2022-05-02 13:46:26,934 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  0%|          | 0/393 [00:00<?, ?ba/s][1,2]<stderr>:#015  0%|          | 0/393 [00:00<?, ?ba/s][1,7]<stderr>:#015  0%|          | 0/393 [00:00<?, ?ba/s][1,6]<stderr>:#015  0%|          | 0/393 [00:00<?, ?ba/s][1,1]<stderr>:#015  0%|          | 0/393 [00:00<?, ?ba/s][1,3]<stderr>:#015  1%|          | 3/393 [00:00<01:08,  5.73ba/s][1,6]<stderr>:#015  0%|          | 1/393 [00:00<01:13,  5.31ba/s][1,3]<stderr>:#015  1%|▏         | 5/393 [00:00<00:56,  6.87ba/s][1,1]<stderr>:#015  0%|          | 1/393 [00:00<01:23,  4.68ba/s][1,4]<stderr>:[WARNING|modeling_utils.py:1331] 2022-05-02 13:46:27,224 >> Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[1,4]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:[WARNING|modeling_utils.py:1342] 2022-05-02 13:46:27,224 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:#015  0%|          | 0/393 [00:00<?, ?ba/s][1,0]<stderr>:#015  0%|          | 1/393 [00:00<02:10,  3.00ba/s][1,5]<stderr>:[WARNING|modeling_utils.py:1331] 2022-05-02 13:46:27,281 >> Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias']\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[WARNING|modeling_utils.py:1342] 2022-05-02 13:46:27,281 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:#015  0%|          | 1/393 [00:00<02:20,  2.79ba/s][1,7]<stderr>:#015  0%|          | 1/393 [00:00<02:18,  2.83ba/s][1,5]<stderr>:#015  0%|          | 0/393 [00:00<?, ?ba/s][1,3]<stderr>:#015  2%|▏         | 7/393 [00:00<00:50,  7.69ba/s][1,1]<stderr>:#015  1%|          | 3/393 [00:00<01:09,  5.62ba/s][1,6]<stderr>:#015  1%|          | 2/393 [00:00<01:17,  5.04ba/s][1,7]<stderr>:#015  1%|          | 2/393 [00:00<01:50,  3.55ba/s][1,0]<stderr>:#015  1%|          | 2/393 [00:00<01:50,  3.55ba/s][1,2]<stderr>:#015  1%|          | 2/393 [00:00<01:54,  3.41ba/s][1,6]<stderr>:#015  1%|          | 3/393 [00:00<01:07,  5.79ba/s][1,1]<stderr>:#015  1%|          | 4/393 [00:00<01:02,  6.21ba/s][1,7]<stderr>:#015  1%|          | 3/393 [00:00<01:31,  4.26ba/s][1,0]<stderr>:#015  1%|          | 3/393 [00:00<01:29,  4.34ba/s][1,2]<stderr>:#015  1%|          | 3/393 [00:00<01:32,  4.23ba/s][1,4]<stderr>:#015  0%|          | 1/393 [00:00<02:01,  3.22ba/s][1,3]<stderr>:#015  2%|▏         | 9/393 [00:00<00:46,  8.23ba/s][1,6]<stderr>:#015  1%|          | 4/393 [00:00<01:01,  6.37ba/s][1,7]<stderr>:#015  1%|          | 4/393 [00:00<01:15,  5.12ba/s][1,1]<stderr>:#015  1%|▏         | 5/393 [00:00<00:58,  6.58ba/s][1,0]<stderr>:#015  1%|          | 4/393 [00:00<01:16,  5.10ba/s][1,2]<stderr>:#015  1%|          | 4/393 [00:00<01:20,  4.86ba/s][1,4]<stderr>:#015  1%|          | 2/393 [00:00<01:40,  3.90ba/s][1,5]<stderr>:#015  0%|          | 1/393 [00:00<02:43,  2.40ba/s][1,6]<stderr>:#015  1%|▏         | 5/393 [00:00<00:57,  6.75ba/s][1,0]<stderr>:#015  1%|▏         | 5/393 [00:00<01:05,  5.96ba/s][1,1]<stderr>:#015  2%|▏         | 6/393 [00:00<00:54,  7.07ba/s][1,3]<stderr>:#015  3%|▎         | 11/393 [00:01<00:42,  8.99ba/s][1,7]<stderr>:#015  1%|▏         | 5/393 [00:00<01:10,  5.51ba/s][1,2]<stderr>:#015  1%|▏         | 5/393 [00:00<01:09,  5.56ba/s][1,4]<stderr>:#015  1%|          | 3/393 [00:00<01:21,  4.76ba/s][1,3]<stderr>:#015  3%|▎         | 12/393 [00:01<00:41,  9.13ba/s][1,5]<stderr>:#015  1%|          | 2/393 [00:00<02:10,  2.99ba/s][1,0]<stderr>:#015  2%|▏         | 6/393 [00:00<01:00,  6.40ba/s][1,2]<stderr>:#015  2%|▏         | 6/393 [00:00<01:00,  6.41ba/s][1,6]<stderr>:#015  2%|▏         | 6/393 [00:00<00:56,  6.86ba/s][1,1]<stderr>:#015  2%|▏         | 7/393 [00:00<00:53,  7.16ba/s][1,7]<stderr>:#015  2%|▏         | 6/393 [00:00<01:01,  6.25ba/s][1,4]<stderr>:#015  1%|          | 4/393 [00:00<01:10,  5.51ba/s][1,7]<stderr>:#015  2%|▏         | 7/393 [00:01<00:55,  6.93ba/s][1,6]<stderr>:#015  2%|▏         | 7/393 [00:01<00:52,  7.38ba/s][1,1]<stderr>:#015  2%|▏         | 8/393 [00:01<00:50,  7.63ba/s][1,2]<stderr>:#015  2%|▏         | 7/393 [00:01<00:55,  6.97ba/s][1,0]<stderr>:#015  2%|▏         | 7/393 [00:01<00:56,  6.84ba/s][1,5]<stderr>:#015  1%|          | 3/393 [00:00<01:47,  3.64ba/s][1,4]<stderr>:#015  1%|▏         | 5/393 [00:00<01:03,  6.08ba/s][1,3]<stderr>:#015  3%|▎         | 13/393 [00:01<00:51,  7.32ba/s][1,6]<stderr>:#015  2%|▏         | 8/393 [00:01<00:49,  7.84ba/s][1,0]<stderr>:#015  2%|▏         | 8/393 [00:01<00:53,  7.25ba/s][1,7]<stderr>:#015  2%|▏         | 8/393 [00:01<00:54,  7.12ba/s][1,1]<stderr>:#015  2%|▏         | 9/393 [00:01<00:50,  7.64ba/s][1,2]<stderr>:#015  2%|▏         | 8/393 [00:01<00:53,  7.16ba/s][1,4]<stderr>:#015  2%|▏         | 6/393 [00:00<00:58,  6.56ba/s][1,5]<stderr>:#015  1%|          | 4/393 [00:00<01:33,  4.18ba/s][1,3]<stderr>:#015  4%|▎         | 14/393 [00:01<00:49,  7.58ba/s][1,6]<stderr>:#015  2%|▏         | 9/393 [00:01<00:47,  8.02ba/s][1,2]<stderr>:#015  2%|▏         | 9/393 [00:01<00:50,  7.61ba/s][1,7]<stderr>:#015  2%|▏         | 9/393 [00:01<00:51,  7.44ba/s][1,0]<stderr>:#015  2%|▏         | 9/393 [00:01<00:52,  7.36ba/s][1,1]<stderr>:#015  3%|▎         | 10/393 [00:01<00:49,  7.74ba/s][1,3]<stderr>:#015  4%|▍         | 15/393 [00:01<00:46,  8.11ba/s][1,4]<stderr>:#015  2%|▏         | 7/393 [00:01<00:55,  6.92ba/s][1,6]<stderr>:#015  3%|▎         | 10/393 [00:01<00:45,  8.47ba/s][1,0]<stderr>:#015  3%|▎         | 10/393 [00:01<00:49,  7.80ba/s][1,5]<stderr>:#015  2%|▏         | 6/393 [00:01<01:16,  5.08ba/s][1,2]<stderr>:#015  3%|▎         | 10/393 [00:01<00:49,  7.77ba/s][1,1]<stderr>:#015  3%|▎         | 11/393 [00:01<00:47,  8.08ba/s][1,7]<stderr>:#015  3%|▎         | 10/393 [00:01<00:50,  7.52ba/s][1,3]<stderr>:#015  4%|▍         | 16/393 [00:01<00:45,  8.31ba/s][1,5]<stderr>:#015  2%|▏         | 7/393 [00:01<01:08,  5.67ba/s][1,1]<stderr>:#015  3%|▎         | 12/393 [00:01<00:47,  7.99ba/s][1,4]<stderr>:#015  2%|▏         | 9/393 [00:01<00:50,  7.58ba/s][1,0]<stderr>:#015  3%|▎         | 11/393 [00:01<00:50,  7.59ba/s][1,6]<stderr>:#015  3%|▎         | 12/393 [00:01<00:41,  9.19ba/s][1,7]<stderr>:#015  3%|▎         | 11/393 [00:01<00:50,  7.55ba/s][1,2]<stderr>:#015  3%|▎         | 11/393 [00:01<00:53,  7.11ba/s][1,5]<stderr>:#015  2%|▏         | 8/393 [00:01<01:01,  6.22ba/s][1,7]<stderr>:#015  3%|▎         | 12/393 [00:01<00:47,  7.97ba/s][1,3]<stderr>:#015  5%|▍         | 18/393 [00:01<00:43,  8.55ba/s][1,4]<stderr>:#015  3%|▎         | 10/393 [00:01<00:50,  7.51ba/s][1,2]<stderr>:#015  3%|▎         | 12/393 [00:01<00:48,  7.78ba/s][1,0]<stderr>:#015  3%|▎         | 12/393 [00:01<00:50,  7.52ba/s][1,1]<stderr>:#015  3%|▎         | 13/393 [00:01<00:58,  6.45ba/s][1,3]<stderr>:#015  5%|▍         | 19/393 [00:02<00:42,  8.73ba/s][1,6]<stderr>:#015  3%|▎         | 13/393 [00:01<00:54,  6.93ba/s][1,4]<stderr>:#015  3%|▎         | 11/393 [00:01<00:48,  7.84ba/s][1,5]<stderr>:#015  2%|▏         | 9/393 [00:01<00:58,  6.58ba/s][1,7]<stderr>:#015  3%|▎         | 13/393 [00:01<00:55,  6.80ba/s][1,0]<stderr>:#015  3%|▎         | 13/393 [00:01<00:56,  6.75ba/s][1,1]<stderr>:#015  4%|▎         | 14/393 [00:01<00:53,  7.09ba/s][1,3]<stderr>:#015  5%|▌         | 20/393 [00:02<00:41,  8.93ba/s][1,6]<stderr>:#015  4%|▎         | 14/393 [00:01<00:51,  7.40ba/s][1,2]<stderr>:#015  3%|▎         | 13/393 [00:01<01:04,  5.86ba/s][1,7]<stderr>:#015  4%|▎         | 14/393 [00:02<00:53,  7.07ba/s][1,5]<stderr>:#015  3%|▎         | 11/393 [00:01<00:52,  7.34ba/s][1,6]<stderr>:#015  4%|▍         | 15/393 [00:01<00:48,  7.86ba/s][1,1]<stderr>:#015  4%|▍         | 15/393 [00:01<00:52,  7.24ba/s][1,0]<stderr>:#015  4%|▎         | 14/393 [00:02<00:55,  6.87ba/s][1,3]<stderr>:#015  5%|▌         | 21/393 [00:02<00:45,  8.26ba/s][1,7]<stderr>:#015  4%|▍         | 15/393 [00:02<00:51,  7.28ba/s][1,0]<stderr>:#015  4%|▍         | 15/393 [00:02<00:51,  7.34ba/s][1,1]<stderr>:#015  4%|▍         | 16/393 [00:02<00:49,  7.58ba/s][1,3]<stderr>:#015  6%|▌         | 22/393 [00:02<00:42,  8.70ba/s][1,2]<stderr>:#015  4%|▍         | 15/393 [00:02<00:55,  6.82ba/s][1,5]<stderr>:#015  3%|▎         | 12/393 [00:01<00:53,  7.09ba/s][1,6]<stderr>:#015  4%|▍         | 16/393 [00:02<00:52,  7.25ba/s][1,4]<stderr>:#015  3%|▎         | 13/393 [00:01<00:56,  6.77ba/s][1,2]<stderr>:#015  4%|▍         | 16/393 [00:02<00:51,  7.31ba/s][1,1]<stderr>:#015  4%|▍         | 17/393 [00:02<00:49,  7.66ba/s][1,0]<stderr>:#015  4%|▍         | 16/393 [00:02<00:51,  7.33ba/s][1,6]<stderr>:#015  4%|▍         | 17/393 [00:02<00:48,  7.81ba/s][1,7]<stderr>:#015  4%|▍         | 16/393 [00:02<00:53,  7.07ba/s][1,3]<stderr>:#015  6%|▌         | 23/393 [00:02<00:46,  8.02ba/s][1,4]<stderr>:#015  4%|▎         | 14/393 [00:02<00:53,  7.07ba/s][1,0]<stderr>:#015  4%|▍         | 17/393 [00:02<00:48,  7.75ba/s][1,3]<stderr>:#015  6%|▌         | 24/393 [00:02<00:43,  8.50ba/s][1,1]<stderr>:#015  5%|▍         | 18/393 [00:02<00:48,  7.77ba/s][1,7]<stderr>:#015  4%|▍         | 17/393 [00:02<00:49,  7.53ba/s][1,2]<stderr>:#015  4%|▍         | 17/393 [00:02<00:51,  7.27ba/s][1,5]<stderr>:#015  3%|▎         | 13/393 [00:02<01:09,  5.50ba/s][1,1]<stderr>:#015  5%|▍         | 19/393 [00:02<00:47,  7.92ba/s][1,7]<stderr>:#015  5%|▍         | 18/393 [00:02<00:48,  7.72ba/s][1,2]<stderr>:#015  5%|▍         | 18/393 [00:02<00:49,  7.56ba/s][1,0]<stderr>:#015  5%|▍         | 18/393 [00:02<00:49,  7.53ba/s][1,3]<stderr>:#015  6%|▋         | 25/393 [00:02<00:46,  8.00ba/s][1,6]<stderr>:#015  5%|▍         | 19/393 [00:02<00:47,  7.89ba/s][1,4]<stderr>:#015  4%|▍         | 16/393 [00:02<00:49,  7.57ba/s][1,3]<stderr>:#015  7%|▋         | 26/393 [00:02<00:44,  8.29ba/s][1,0]<stderr>:#015  5%|▍         | 19/393 [00:02<00:48,  7.78ba/s][1,6]<stderr>:#015  5%|▌         | 20/393 [00:02<00:45,  8.12ba/s][1,4]<stderr>:#015  4%|▍         | 17/393 [00:02<00:46,  8.14ba/s][1,5]<stderr>:#015  4%|▍         | 15/393 [00:02<00:59,  6.31ba/s][1,2]<stderr>:#015  5%|▍         | 19/393 [00:02<00:49,  7.60ba/s][1,1]<stderr>:#015  5%|▌         | 20/393 [00:02<00:48,  7.63ba/s][1,7]<stderr>:#015  5%|▍         | 19/393 [00:02<00:48,  7.64ba/s][1,0]<stderr>:#015  5%|▌         | 20/393 [00:02<00:47,  7.85ba/s][1,4]<stderr>:#015  5%|▍         | 18/393 [00:02<00:46,  8.03ba/s][1,3]<stderr>:#015  7%|▋         | 27/393 [00:03<00:45,  8.00ba/s][1,7]<stderr>:#015  5%|▌         | 20/393 [00:02<00:48,  7.77ba/s][1,5]<stderr>:#015  4%|▍         | 16/393 [00:02<00:56,  6.64ba/s][1,6]<stderr>:#015  5%|▌         | 21/393 [00:02<00:49,  7.51ba/s][1,1]<stderr>:#015  5%|▌         | 21/393 [00:02<00:51,  7.27ba/s][1,2]<stderr>:#015  5%|▌         | 20/393 [00:02<00:53,  6.96ba/s][1,3]<stderr>:#015  7%|▋         | 28/393 [00:03<00:45,  8.10ba/s][1,4]<stderr>:#015  5%|▍         | 19/393 [00:02<00:46,  8.05ba/s][1,7]<stderr>:#015  5%|▌         | 21/393 [00:02<00:47,  7.82ba/s][1,6]<stderr>:#015  6%|▌         | 22/393 [00:02<00:46,  8.03ba/s][1,0]<stderr>:#015  5%|▌         | 21/393 [00:02<00:48,  7.61ba/s][1,5]<stderr>:#015  4%|▍         | 17/393 [00:02<00:54,  6.92ba/s][1,2]<stderr>:#015  5%|▌         | 21/393 [00:02<00:49,  7.51ba/s][1,6]<stderr>:#015  6%|▌         | 23/393 [00:02<00:45,  8.14ba/s][1,0]<stderr>:#015  6%|▌         | 22/393 [00:03<00:47,  7.79ba/s][1,1]<stderr>:#015  6%|▌         | 23/393 [00:02<00:48,  7.68ba/s][1,4]<stderr>:#015  5%|▌         | 20/393 [00:02<00:47,  7.85ba/s][1,2]<stderr>:#015  6%|▌         | 22/393 [00:03<00:46,  8.05ba/s][1,7]<stderr>:#015  6%|▌         | 22/393 [00:03<00:48,  7.72ba/s][1,5]<stderr>:#015  5%|▍         | 18/393 [00:02<00:54,  6.82ba/s][1,3]<stderr>:#015  7%|▋         | 29/393 [00:03<00:56,  6.39ba/s][1,7]<stderr>:#015  6%|▌         | 23/393 [00:03<00:45,  8.14ba/s][1,0]<stderr>:#015  6%|▌         | 23/393 [00:03<00:45,  8.08ba/s][1,1]<stderr>:#015  6%|▌         | 24/393 [00:03<00:46,  7.97ba/s][1,6]<stderr>:#015  6%|▌         | 24/393 [00:03<00:45,  8.12ba/s][1,4]<stderr>:#015  5%|▌         | 21/393 [00:02<00:46,  8.06ba/s][1,6]<stderr>:#015  6%|▋         | 25/393 [00:03<00:44,  8.26ba/s][1,3]<stderr>:#015  8%|▊         | 30/393 [00:03<00:54,  6.65ba/s][1,2]<stderr>:#015  6%|▌         | 24/393 [00:03<00:44,  8.21ba/s][1,0]<stderr>:#015  6%|▌         | 24/393 [00:03<00:45,  8.08ba/s][1,5]<stderr>:#015  5%|▌         | 20/393 [00:02<00:49,  7.46ba/s][1,7]<stderr>:#015  6%|▌         | 24/393 [00:03<00:45,  8.03ba/s][1,4]<stderr>:#015  6%|▌         | 22/393 [00:02<00:46,  7.94ba/s][1,1]<stderr>:#015  6%|▋         | 25/393 [00:03<00:47,  7.80ba/s][1,6]<stderr>:#015  7%|▋         | 26/393 [00:03<00:44,  8.31ba/s][1,2]<stderr>:#015  6%|▋         | 25/393 [00:03<00:44,  8.22ba/s][1,3]<stderr>:#015  8%|▊         | 31/393 [00:03<00:51,  7.03ba/s][1,0]<stderr>:#015  6%|▋         | 25/393 [00:03<00:45,  8.12ba/s][1,1]<stderr>:#015  7%|▋         | 26/393 [00:03<00:45,  8.15ba/s][1,7]<stderr>:#015  6%|▋         | 25/393 [00:03<00:45,  8.05ba/s][1,4]<stderr>:#015  6%|▌         | 23/393 [00:03<00:49,  7.45ba/s][1,5]<stderr>:#015  5%|▌         | 21/393 [00:03<00:54,  6.88ba/s][1,2]<stderr>:#015  7%|▋         | 26/393 [00:03<00:42,  8.63ba/s][1,3]<stderr>:#015  8%|▊         | 32/393 [00:03<00:47,  7.62ba/s][1,6]<stderr>:#015  7%|▋         | 27/393 [00:03<00:43,  8.43ba/s][1,1]<stderr>:#015  7%|▋         | 27/393 [00:03<00:43,  8.45ba/s][1,0]<stderr>:#015  7%|▋         | 26/393 [00:03<00:46,  7.88ba/s][1,7]<stderr>:#015  7%|▋         | 26/393 [00:03<00:49,  7.47ba/s][1,4]<stderr>:#015  6%|▌         | 24/393 [00:03<00:48,  7.67ba/s][1,5]<stderr>:#015  6%|▌         | 22/393 [00:03<00:49,  7.42ba/s][1,2]<stderr>:#015  7%|▋         | 27/393 [00:03<00:42,  8.71ba/s][1,1]<stderr>:#015  7%|▋         | 28/393 [00:03<00:43,  8.47ba/s][1,3]<stderr>:#015  8%|▊         | 33/393 [00:03<00:47,  7.60ba/s][1,6]<stderr>:#015  7%|▋         | 28/393 [00:03<00:44,  8.15ba/s][1,0]<stderr>:#015  7%|▋         | 27/393 [00:03<00:45,  8.05ba/s][1,5]<stderr>:#015  6%|▌         | 23/393 [00:03<00:49,  7.52ba/s][1,7]<stderr>:#015  7%|▋         | 27/393 [00:03<00:52,  7.00ba/s][1,2]<stderr>:#015  7%|▋         | 28/393 [00:03<00:42,  8.52ba/s][1,0]<stderr>:#015  7%|▋         | 28/393 [00:03<00:43,  8.42ba/s][1,4]<stderr>:#015  7%|▋         | 26/393 [00:03<00:44,  8.26ba/s][1,3]<stderr>:#015  9%|▊         | 34/393 [00:04<00:46,  7.67ba/s][1,6]<stderr>:#015  8%|▊         | 30/393 [00:03<00:42,  8.56ba/s][1,1]<stderr>:#015  7%|▋         | 29/393 [00:03<00:55,  6.53ba/s][1,7]<stderr>:#015  7%|▋         | 28/393 [00:03<00:51,  7.06ba/s][1,3]<stderr>:#015  9%|▉         | 35/393 [00:04<00:44,  8.01ba/s][1,4]<stderr>:#015  7%|▋         | 27/393 [00:03<00:45,  8.03ba/s][1,5]<stderr>:#015  6%|▋         | 25/393 [00:03<00:46,  7.92ba/s][1,1]<stderr>:#015  8%|▊         | 30/393 [00:03<00:50,  7.14ba/s][1,0]<stderr>:#015  7%|▋         | 29/393 [00:04<00:57,  6.37ba/s][1,6]<stderr>:#015  8%|▊         | 31/393 [00:03<00:47,  7.66ba/s][1,2]<stderr>:#015  7%|▋         | 29/393 [00:04<01:02,  5.83ba/s][1,3]<stderr>:#015  9%|▉         | 37/393 [00:04<00:42,  8.33ba/s][1,1]<stderr>:#015  8%|▊         | 31/393 [00:04<00:48,  7.41ba/s][1,7]<stderr>:#015  7%|▋         | 29/393 [00:04<01:01,  5.94ba/s][1,0]<stderr>:#015  8%|▊         | 30/393 [00:04<00:54,  6.72ba/s][1,5]<stderr>:#015  7%|▋         | 27/393 [00:03<00:44,  8.20ba/s][1,4]<stderr>:#015  7%|▋         | 29/393 [00:03<00:47,  7.71ba/s][1,6]<stderr>:#015  8%|▊         | 33/393 [00:04<00:43,  8.37ba/s][1,3]<stderr>:#015 10%|▉         | 38/393 [00:04<00:42,  8.31ba/s][1,1]<stderr>:#015  8%|▊         | 32/393 [00:04<00:48,  7.43ba/s][1,7]<stderr>:#015  8%|▊         | 30/393 [00:04<00:57,  6.29ba/s][1,2]<stderr>:#015  8%|▊         | 31/393 [00:04<00:54,  6.59ba/s][1,0]<stderr>:#015  8%|▊         | 31/393 [00:04<00:50,  7.20ba/s][1,4]<stderr>:#015  8%|▊         | 30/393 [00:03<00:48,  7.54ba/s][1,2]<stderr>:#015  8%|▊         | 32/393 [00:04<00:50,  7.12ba/s][1,3]<stderr>:#015 10%|▉         | 39/393 [00:04<00:45,  7.77ba/s][1,7]<stderr>:#015  8%|▊         | 31/393 [00:04<00:54,  6.62ba/s][1,1]<stderr>:#015  8%|▊         | 33/393 [00:04<00:48,  7.36ba/s][1,6]<stderr>:#015  9%|▉         | 35/393 [00:04<00:40,  8.82ba/s][1,0]<stderr>:#015  8%|▊         | 32/393 [00:04<00:51,  7.02ba/s][1,5]<stderr>:#015  7%|▋         | 29/393 [00:04<00:49,  7.36ba/s][1,1]<stderr>:#015  9%|▊         | 34/393 [00:04<00:46,  7.65ba/s][1,3]<stderr>:#015 10%|█         | 40/393 [00:04<00:45,  7.82ba/s][1,7]<stderr>:#015  8%|▊         | 32/393 [00:04<00:51,  6.96ba/s][1,2]<stderr>:#015  8%|▊         | 33/393 [00:04<00:50,  7.16ba/s][1,6]<stderr>:#015  9%|▉         | 36/393 [00:04<00:39,  9.05ba/s][1,4]<stderr>:#015  8%|▊         | 32/393 [00:04<00:43,  8.33ba/s][1,0]<stderr>:#015  9%|▊         | 34/393 [00:04<00:46,  7.69ba/s][1,7]<stderr>:#015  8%|▊         | 33/393 [00:04<00:48,  7.40ba/s][1,5]<stderr>:#015  8%|▊         | 30/393 [00:04<00:49,  7.40ba/s][1,1]<stderr>:#015  9%|▉         | 35/393 [00:04<00:46,  7.66ba/s][1,6]<stderr>:#015  9%|▉         | 37/393 [00:04<00:41,  8.65ba/s][1,4]<stderr>:#015  8%|▊         | 33/393 [00:04<00:43,  8.19ba/s][1,3]<stderr>:#015 10%|█         | 41/393 [00:04<00:48,  7.31ba/s][1,2]<stderr>:#015  9%|▊         | 34/393 [00:04<00:52,  6.84ba/s][1,7]<stderr>:#015  9%|▊         | 34/393 [00:04<00:45,  7.87ba/s][1,0]<stderr>:#015  9%|▉         | 35/393 [00:04<00:45,  7.85ba/s][1,6]<stderr>:#015 10%|▉         | 38/393 [00:04<00:39,  8.98ba/s][1,1]<stderr>:#015  9%|▉         | 36/393 [00:04<00:44,  7.96ba/s][1,5]<stderr>:#015  8%|▊         | 31/393 [00:04<00:48,  7.39ba/s][1,4]<stderr>:#015  9%|▊         | 34/393 [00:04<00:43,  8.22ba/s][1,3]<stderr>:#015 11%|█         | 42/393 [00:05<00:44,  7.83ba/s][1,7]<stderr>:#015  9%|▉         | 35/393 [00:04<00:44,  8.12ba/s][1,3]<stderr>:#015 11%|█         | 43/393 [00:05<00:42,  8.31ba/s][1,0]<stderr>:#015  9%|▉         | 36/393 [00:04<00:46,  7.76ba/s][1,2]<stderr>:#015  9%|▉         | 36/393 [00:04<00:47,  7.47ba/s][1,1]<stderr>:#015  9%|▉         | 37/393 [00:04<00:46,  7.70ba/s][1,5]<stderr>:#015  8%|▊         | 32/393 [00:04<00:48,  7\u001b[0m\n",
      "\u001b[34m.45ba/s][1,4]<stderr>:#015  9%|▉         | 35/393 [00:04<00:44,  8.08ba/s][1,6]<stderr>:#015 10%|▉         | 39/393 [00:04<00:43,  8.07ba/s][1,3]<stderr>:#015 11%|█         | 44/393 [00:05<00:42,  8.23ba/s][1,2]<stderr>:#015  9%|▉         | 37/393 [00:04<00:45,  7.80ba/s][1,1]<stderr>:#015 10%|▉         | 38/393 [00:04<00:45,  7.72ba/s][1,0]<stderr>:#015  9%|▉         | 37/393 [00:05<00:47,  7.48ba/s][1,6]<stderr>:#015 10%|█         | 40/393 [00:04<00:43,  8.03ba/s][1,5]<stderr>:#015  8%|▊         | 33/393 [00:04<00:48,  7.44ba/s][1,7]<stderr>:#015  9%|▉         | 37/393 [00:05<00:40,  8.76ba/s][1,4]<stderr>:#015  9%|▉         | 36/393 [00:04<00:46,  7.74ba/s][1,3]<stderr>:#015 11%|█▏        | 45/393 [00:05<00:42,  8.28ba/s][1,1]<stderr>:#015 10%|▉         | 39/393 [00:05<00:44,  8.03ba/s][1,2]<stderr>:#015 10%|▉         | 38/393 [00:05<00:45,  7.73ba/s][1,4]<stderr>:#015  9%|▉         | 37/393 [00:04<00:43,  8.23ba/s][1,0]<stderr>:#015 10%|▉         | 38/393 [00:05<00:45,  7.73ba/s][1,5]<stderr>:#015  9%|▊         | 34/393 [00:04<00:45,  7.81ba/s][1,7]<stderr>:#015 10%|▉         | 38/393 [00:05<00:40,  8.80ba/s][1,6]<stderr>:#015 10%|█         | 41/393 [00:05<00:50,  6.99ba/s][1,1]<stderr>:#015 10%|█         | 40/393 [00:05<00:43,  8.13ba/s][1,5]<stderr>:#015  9%|▉         | 35/393 [00:04<00:43,  8.18ba/s][1,0]<stderr>:#015 10%|▉         | 39/393 [00:05<00:43,  8.08ba/s][1,2]<stderr>:#015 10%|▉         | 39/393 [00:05<00:45,  7.79ba/s][1,7]<stderr>:#015 10%|▉         | 39/393 [00:05<00:40,  8.73ba/s][1,4]<stderr>:#015 10%|▉         | 38/393 [00:04<00:43,  8.13ba/s][1,3]<stderr>:#015 12%|█▏        | 46/393 [00:05<00:49,  6.96ba/s][1,2]<stderr>:#015 10%|█         | 40/393 [00:05<00:45,  7.84ba/s][1,7]<stderr>:#015 10%|█         | 40/393 [00:05<00:41,  8.52ba/s][1,4]<stderr>:#015 10%|▉         | 39/393 [00:05<00:43,  8.10ba/s][1,5]<stderr>:#015  9%|▉         | 36/393 [00:04<00:45,  7.77ba/s][1,0]<stderr>:#015 10%|█         | 40/393 [00:05<00:45,  7.69ba/s][1,6]<stderr>:#015 11%|█         | 43/393 [00:05<00:44,  7.81ba/s][1,1]<stderr>:#015 10%|█         | 41/393 [00:05<00:48,  7.23ba/s][1,3]<stderr>:#015 12%|█▏        | 47/393 [00:05<00:46,  7.40ba/s][1,4]<stderr>:#015 10%|█         | 40/393 [00:05<00:44,  7.98ba/s][1,1]<stderr>:#015 11%|█         | 42/393 [00:05<00:45,  7.73ba/s][1,3]<stderr>:#015 12%|█▏        | 48/393 [00:05<00:43,  7.99ba/s][1,5]<stderr>:#015  9%|▉         | 37/393 [00:05<00:47,  7.56ba/s][1,6]<stderr>:#015 11%|█         | 44/393 [00:05<00:45,  7.61ba/s][1,2]<stderr>:#015 10%|█         | 41/393 [00:05<00:50,  6.95ba/s][1,7]<stderr>:#015 10%|█         | 41/393 [00:05<00:49,  7.14ba/s][1,0]<stderr>:#015 10%|█         | 41/393 [00:05<00:54,  6.47ba/s][1,1]<stderr>:#015 11%|█         | 43/393 [00:05<00:43,  8.13ba/s][1,6]<stderr>:#015 11%|█▏        | 45/393 [00:05<00:42,  8.11ba/s][1,5]<stderr>:#015 10%|▉         | 38/393 [00:05<00:44,  7.97ba/s][1,2]<stderr>:#015 11%|█         | 42/393 [00:05<00:46,  7.58ba/s][1,4]<stderr>:#015 10%|█         | 41/393 [00:05<00:47,  7.34ba/s][1,3]<stderr>:#015 12%|█▏        | 49/393 [00:05<00:46,  7.44ba/s][1,7]<stderr>:#015 11%|█         | 42/393 [00:05<00:46,  7.52ba/s][1,0]<stderr>:#015 11%|█         | 42/393 [00:05<00:48,  7.21ba/s][1,5]<stderr>:#015 10%|▉         | 39/393 [00:05<00:42,  8.34ba/s][1,1]<stderr>:#015 11%|█         | 44/393 [00:05<00:41,  8.37ba/s][1,6]<stderr>:#015 12%|█▏        | 46/393 [00:05<00:43,  7.98ba/s][1,4]<stderr>:#015 11%|█         | 42/393 [00:05<00:45,  7.72ba/s][1,3]<stderr>:#015 13%|█▎        | 50/393 [00:06<00:43,  7.82ba/s][1,7]<stderr>:#015 11%|█         | 43/393 [00:05<00:43,  8.04ba/s][1,2]<stderr>:#015 11%|█         | 44/393 [00:05<00:42,  8.13ba/s][1,1]<stderr>:#015 11%|█▏        | 45/393 [00:05<00:43,  8.09ba/s][1,5]<stderr>:#015 10%|█         | 40/393 [00:05<00:44,  7.89ba/s][1,4]<stderr>:#015 11%|█         | 43/393 [00:05<00:42,  8.18ba/s][1,7]<stderr>:#015 11%|█         | 44/393 [00:05<00:41,  8.44ba/s][1,3]<stderr>:#015 13%|█▎        | 51/393 [00:06<00:42,  7.96ba/s][1,6]<stderr>:#015 12%|█▏        | 47/393 [00:05<00:47,  7.29ba/s][1,0]<stderr>:#015 11%|█         | 44/393 [00:05<00:46,  7.43ba/s][1,2]<stderr>:#015 11%|█▏        | 45/393 [00:05<00:41,  8.34ba/s][1,7]<stderr>:#015 11%|█▏        | 45/393 [00:06<00:42,  8.27ba/s][1,4]<stderr>:#015 11%|█         | 44/393 [00:05<00:43,  8.01ba/s][1,3]<stderr>:#015 13%|█▎        | 52/393 [00:06<00:41,  8.18ba/s][1,5]<stderr>:#015 10%|█         | 41/393 [00:05<00:50,  6.96ba/s][1,1]<stderr>:#015 12%|█▏        | 46/393 [00:06<00:53,  6.55ba/s][1,3]<stderr>:#015 13%|█▎        | 53/393 [00:06<00:39,  8.58ba/s][1,4]<stderr>:#015 11%|█▏        | 45/393 [00:05<00:43,  8.04ba/s][1,6]<stderr>:#015 12%|█▏        | 48/393 [00:06<00:57,  6.01ba/s][1,2]<stderr>:#015 12%|█▏        | 46/393 [00:06<00:49,  7.04ba/s][1,0]<stderr>:#015 12%|█▏        | 46/393 [00:06<00:48,  7.22ba/s][1,7]<stderr>:#015 12%|█▏        | 46/393 [00:06<00:53,  6.52ba/s][1,3]<stderr>:#015 14%|█▍        | 55/393 [00:06<00:35,  9.55ba/s][1,6]<stderr>:#015 12%|█▏        | 49/393 [00:06<00:52,  6.52ba/s][1,2]<stderr>:#015 12%|█▏        | 47/393 [00:06<00:46,  7.44ba/s][1,5]<stderr>:#015 11%|█         | 43/393 [00:05<00:47,  7.44ba/s][1,1]<stderr>:#015 12%|█▏        | 48/393 [00:06<00:48,  7.12ba/s][1,7]<stderr>:#015 12%|█▏        | 47/393 [00:06<00:48,  7.19ba/s][1,0]<stderr>:#015 12%|█▏        | 47/393 [00:06<00:45,  7.62ba/s][1,6]<stderr>:#015 13%|█▎        | 50/393 [00:06<00:48,  7.07ba/s][1,2]<stderr>:#015 12%|█▏        | 48/393 [00:06<00:44,  7.82ba/s][1,4]<stderr>:#015 12%|█▏        | 46/393 [00:06<01:03,  5.50ba/s][1,0]<stderr>:#015 12%|█▏        | 48/393 [00:06<00:43,  7.88ba/s][1,5]<stderr>:#015 11%|█▏        | 45/393 [00:06<00:43,  8.00ba/s][1,6]<stderr>:#015 13%|█▎        | 51/393 [00:06<00:45,  7.55ba/s][1,1]<stderr>:#015 13%|█▎        | 50/393 [00:06<00:44,  7.79ba/s][1,3]<stderr>:#015 15%|█▍        | 57/393 [00:06<00:36,  9.18ba/s][1,2]<stderr>:#015 12%|█▏        | 49/393 [00:06<00:43,  7.82ba/s][1,4]<stderr>:#015 12%|█▏        | 47/393 [00:06<00:54,  6.29ba/s][1,7]<stderr>:#015 12%|█▏        | 49/393 [00:06<00:45,  7.63ba/s][1,6]<stderr>:#015 13%|█▎        | 52/393 [00:06<00:44,  7.68ba/s][1,0]<stderr>:#015 12%|█▏        | 49/393 [00:06<00:47,  7.32ba/s][1,3]<stderr>:#015 15%|█▍        | 58/393 [00:06<00:37,  8.84ba/s][1,1]<stderr>:#015 13%|█▎        | 51/393 [00:06<00:44,  7.71ba/s][1,4]<stderr>:#015 12%|█▏        | 48/393 [00:06<00:52,  6.54ba/s][1,5]<stderr>:#015 12%|█▏        | 46/393 [00:06<00:53,  6.50ba/s][1,6]<stderr>:#015 13%|█▎        | 53/393 [00:06<00:42,  7.92ba/s][1,7]<stderr>:#015 13%|█▎        | 51/393 [00:06<00:40,  8.53ba/s][1,2]<stderr>:#015 13%|█▎        | 51/393 [00:06<00:42,  8.13ba/s][1,3]<stderr>:#015 15%|█▌        | 59/393 [00:07<00:39,  8.48ba/s][1,0]<stderr>:#015 13%|█▎        | 50/393 [00:06<00:47,  7.17ba/s][1,6]<stderr>:#015 14%|█▎        | 54/393 [00:06<00:43,  7.88ba/s][1,7]<stderr>:#015 13%|█▎        | 52/393 [00:06<00:41,  8.20ba/s][1,5]<stderr>:#015 12%|█▏        | 48/393 [00:06<00:45,  7.55ba/s][1,2]<stderr>:#015 13%|█▎        | 52/393 [00:06<00:42,  7.94ba/s][1,3]<stderr>:#015 15%|█▌        | 60/393 [00:07<00:38,  8.55ba/s][1,1]<stderr>:#015 13%|█▎        | 53/393 [00:06<00:42,  7.92ba/s][1,0]<stderr>:#015 13%|█▎        | 51/393 [00:06<00:44,  7.71ba/s][1,4]<stderr>:#015 13%|█▎        | 50/393 [00:06<00:46,  7.41ba/s][1,0]<stderr>:#015 13%|█▎        | 52/393 [00:07<00:43,  7.76ba/s][1,7]<stderr>:#015 13%|█▎        | 53/393 [00:07<00:42,  8.00ba/s][1,3]<stderr>:#015 16%|█▌        | 61/393 [00:07<00:40,  8.29ba/s][1,5]<stderr>:#015 12%|█▏        | 49/393 [00:06<00:45,  7.55ba/s][1,1]<stderr>:#015 14%|█▎        | 54/393 [00:06<00:43,  7.86ba/s][1,2]<stderr>:#015 13%|█▎        | 53/393 [00:07<00:43,  7.80ba/s][1,4]<stderr>:#015 13%|█▎        | 51/393 [00:06<00:46,  7.34ba/s][1,6]<stderr>:#015 14%|█▍        | 55/393 [00:07<00:48,  6.93ba/s][1,0]<stderr>:#015 13%|█▎        | 53/393 [00:07<00:41,  8.10ba/s][1,5]<stderr>:#015 13%|█▎        | 50/393 [00:06<00:44,  7.78ba/s][1,1]<stderr>:#015 14%|█▍        | 55/393 [00:07<00:42,  7.96ba/s][1,4]<stderr>:#015 13%|█▎        | 52/393 [00:06<00:45,  7.48ba/s][1,7]<stderr>:#015 14%|█▎        | 54/393 [00:07<00:46,  7.32ba/s][1,2]<stderr>:#015 14%|█▎        | 54/393 [00:07<00:47,  7.13ba/s][1,6]<stderr>:#015 14%|█▍        | 56/393 [00:07<00:47,  7.06ba/s][1,0]<stderr>:#015 14%|█▎        | 54/393 [00:07<00:42,  8.06ba/s][1,1]<stderr>:#015 14%|█▍        | 56/393 [00:07<00:41,  8.11ba/s][1,5]<stderr>:#015 13%|█▎        | 51/393 [00:06<00:44,  7.74ba/s][1,4]<stderr>:#015 13%|█▎        | 53/393 [00:06<00:43,  7.82ba/s][1,7]<stderr>:#015 14%|█▍        | 55/393 [00:07<00:43,  7.72ba/s][1,2]<stderr>:#015 14%|█▍        | 55/393 [00:07<00:44,  7.67ba/s][1,3]<stderr>:#015 16%|█▌        | 62/393 [00:07<00:57,  5.73ba/s][1,0]<stderr>:#015 14%|█▍        | 55/393 [00:07<00:43,  7.77ba/s][1,5]<stderr>:#015 13%|█▎        | 52/393 [00:06<00:44,  7.70ba/s][1,6]<stderr>:#015 15%|█▍        | 58/393 [00:07<00:43,  7.73ba/s][1,7]<stderr>:#015 14%|█▍        | 56/393 [00:07<00:41,  8.08ba/s][1,2]<stderr>:#015 14%|█▍        | 56/393 [00:07<00:41,  8.06ba/s][1,1]<stderr>:#015 15%|█▍        | 57/393 [00:07<00:44,  7.58ba/s][1,4]<stderr>:#015 14%|█▎        | 54/393 [00:07<00:44,  7.60ba/s][1,3]<stderr>:#015 16%|█▌        | 63/393 [00:07<00:53,  6.23ba/s][1,0]<stderr>:#015 14%|█▍        | 56/393 [00:07<00:43,  7.82ba/s][1,5]<stderr>:#015 13%|█▎        | 53/393 [00:07<00:43,  7.84ba/s][1,6]<stderr>:#015 15%|█▌        | 59/393 [00:07<00:43,  7.62ba/s][1,7]<stderr>:#015 15%|█▍        | 57/393 [00:07<00:42,  7.90ba/s][1,1]<stderr>:#015 15%|█▍        | 58/393 [00:07<00:44,  7.60ba/s][1,2]<stderr>:#015 15%|█▍        | 57/393 [00:07<00:43,  7.75ba/s][1,4]<stderr>:#015 14%|█▍        | 55/393 [00:07<00:44,  7.59ba/s][1,3]<stderr>:#015 16%|█▋        | 64/393 [00:07<00:47,  6.90ba/s][1,0]<stderr>:#015 15%|█▍        | 57/393 [00:07<00:44,  7.61ba/s][1,5]<stderr>:#015 14%|█▎        | 54/393 [00:07<00:44,  7.64ba/s][1,1]<stderr>:#015 15%|█▌        | 59/393 [00:07<00:42,  7.82ba/s][1,6]<stderr>:#015 15%|█▌        | 60/393 [00:07<00:42,  7.76ba/s][1,2]<stderr>:#015 15%|█▍        | 58/393 [00:07<00:42,  7.93ba/s][1,4]<stderr>:#015 14%|█▍        | 56/393 [00:07<00:42,  7.84ba/s][1,7]<stderr>:#015 15%|█▍        | 58/393 [00:07<00:43,  7.75ba/s][1,3]<stderr>:#015 17%|█▋        | 65/393 [00:07<00:47,  6.91ba/s][1,6]<stderr>:#015 16%|█▌        | 61/393 [00:07<00:41,  7.93ba/s][1,5]<stderr>:#015 14%|█▍        | 55/393 [00:07<00:43,  7.77ba/s][1,1]<stderr>:#015 15%|█▌        | 60/393 [00:07<00:43,  7.67ba/s][1,2]<stderr>:#015 15%|█▌        | 59/393 [00:07<00:42,  7.81ba/s][1,4]<stderr>:#015 15%|█▍        | 57/393 [00:07<00:43,  7.72ba/s][1,0]<stderr>:#015 15%|█▍        | 58/393 [00:07<00:45,  7.33ba/s][1,7]<stderr>:#015 15%|█▌        | 59/393 [00:07<00:47,  6.98ba/s][1,2]<stderr>:#015 15%|█▌        | 60/393 [00:07<00:42,  7.83ba/s][1,5]<stderr>:#015 14%|█▍        | 56/393 [00:07<00:44,  7.56ba/s][1,3]<stderr>:#015 17%|█▋        | 67/393 [00:08<00:44,  7.39ba/s][1,0]<stderr>:#015 15%|█▌        | 59/393 [00:07<00:44,  7.46ba/s][1,4]<stderr>:#015 15%|█▍        | 58/393 [00:07<00:44,  7.48ba/s][1,6]<stderr>:#015 16%|█▌        | 62/393 [00:07<00:46,  7.13ba/s][1,1]<stderr>:#015 16%|█▌        | 61/393 [00:07<00:46,  7.15ba/s][1,7]<stderr>:#015 16%|█▌        | 61/393 [00:08<00:43,  7.55ba/s][1,6]<stderr>:#015 16%|█▌        | 63/393 [00:08<00:43,  7.62ba/s][1,2]<stderr>:#015 16%|█▌        | 61/393 [00:08<00:43,  7.61ba/s][1,3]<stderr>:#015 17%|█▋        | 68/393 [00:08<00:44,  7.31ba/s][1,5]<stderr>:#015 15%|█▍        | 57/393 [00:07<00:45,  7.36ba/s][1,4]<stderr>:#015 15%|█▌        | 59/393 [00:07<00:44,  7.57ba/s][1,0]<stderr>:#015 15%|█▌        | 60/393 [00:08<00:44,  7.40ba/s][1,0]<stderr>:#015 16%|█▌        | 61/393 [00:08<00:42,  7.88ba/s][1,6]<stderr>:#015 16%|█▋        | 64/393 [00:08<00:42,  7.80ba/s][1,3]<stderr>:#015 18%|█▊        | 69/393 [00:08<00:42,  7.57ba/s][1,4]<stderr>:#015 15%|█▌        | 60/393 [00:07<00:42,  7.79ba/s][1,5]<stderr>:#015 15%|█▍        | 58/393 [00:07<00:44,  7.56ba/s][1,1]<stderr>:#015 16%|█▌        | 62/393 [00:08<01:00,  5.47ba/s][1,2]<stderr>:#015 16%|█▌        | 62/393 [00:08<00:49,  6.71ba/s][1,7]<stderr>:#015 16%|█▌        | 62/393 [00:08<00:50,  6.62ba/s][1,4]<stderr>:#015 16%|█▌        | 61/393 [00:07<00:39,  8.30ba/s][1,5]<stderr>:#015 15%|█▌        | 59/393 [00:07<00:42,  7.93ba/s][1,3]<stderr>:#015 18%|█▊        | 70/393 [00:08<00:43,  7.49ba/s][1,1]<stderr>:#015 16%|█▌        | 63/393 [00:08<00:52,  6.28ba/s][1,6]<stderr>:#015 17%|█▋        | 65/393 [00:08<00:51,  6.38ba/s][1,5]<stderr>:#015 15%|█▌        | 60/393 [00:08<00:41,  7.98ba/s][1,3]<stderr>:#015 18%|█▊        | 71/393 [00:08<00:40,  8.00ba/s][1,7]<stderr>:#015 16%|█▋        | 64/393 [00:08<00:43,  7.56ba/s][1,2]<stderr>:#015 16%|█▋        | 64/393 [00:08<00:43,  7.62ba/s][1,1]<stderr>:#015 16%|█▋        | 64/393 [00:08<00:46,  7.03ba/s][1,0]<stderr>:#015 16%|█▌        | 62/393 [00:08<00:58,  5.69ba/s][1,7]<stderr>:#015 17%|█▋        | 65/393 [00:08<00:40,  8.07ba/s][1,3]<stderr>:#015 18%|█▊        | 72/393 [00:08<00:39,  8.08ba/s][1,5]<stderr>:#015 16%|█▌        | 61/393 [00:08<00:41,  8.04ba/s][1,1]<stderr>:#015 17%|█▋        | 65/393 [00:08<00:44,  7.44ba/s][1,4]<stderr>:#015 16%|█▌        | 62/393 [00:08<00:55,  6.00ba/s][1,2]<stderr>:#015 17%|█▋        | 65/393 [00:08<00:43,  7.46ba/s][1,0]<stderr>:#015 16%|█▌        | 63/393 [00:08<00:52,  6.29ba/s][1,6]<stderr>:#015 17%|█▋        | 67/393 [00:08<00:45,  7.18ba/s][1,7]<stderr>:#015 17%|█▋        | 66/393 [00:08<00:42,  7.74ba/s][1,1]<stderr>:#015 17%|█▋        | 66/393 [00:08<00:43,  7.55ba/s][1,3]<stderr>:#015 19%|█▊        | 73/393 [00:08<00:41,  7.72ba/s][1,2]<stderr>:#015 17%|█▋        | 66/393 [00:08<00:42,  7.71ba/s][1,0]<stderr>:#015 16%|█▋        | 64/393 [00:08<00:47,  6.97ba/s][1,4]<stderr>:#015 16%|█▌        | 63/393 [00:08<00:51,  6.41ba/s][1,7]<stderr>:#015 17%|█▋        | 67/393 [00:08<00:41,  7.81ba/s][1,1]<stderr>:#015 17%|█▋        | 67/393 [00:08<00:41,  7.84ba/s][1,2]<stderr>:#015 17%|█▋        | 67/393 [00:08<00:40,  7.99ba/s][1,0]<stderr>:#015 17%|█▋        | 65/393 [00:08<00:44,  7.40ba/s][1,3]<stderr>:#015 19%|█▉        | 74/393 [00:09<00:40,  7.85ba/s][1,6]<stderr>:#015 18%|█▊        | 69/393 [00:08<00:41,  7.75ba/s][1,4]<stderr>:#015 16%|█▋        | 64/393 [00:08<00:48,  6.82ba/s][1,5]<stderr>:#015 16%|█▌        | 62/393 [00:08<00:56,  5.84ba/s][1,2]<stderr>:#015 17%|█▋        | 68/393 [00:08<00:40,  8.09ba/s][1,6]<stderr>:#015 18%|█▊        | 70/393 [00:08<00:41,  7.79ba/s][1,5]<stderr>:#015 16%|█▌        | 63/393 [00:08<00:50,  6.49ba/s][1,1]<stderr>:#015 17%|█▋        | 68/393 [00:08<00:42,  7.66ba/s][1,7]<stderr>:#015 17%|█▋        | 68/393 [00:08<00:42,  7.59ba/s][1,4]<stderr>:#015 17%|█▋        | 65/393 [00:08<00:45,  7.16ba/s][1,0]<stderr>:#015 17%|█▋        | 66/393 [00:08<00:44,  7.39ba/s][1,3]<stderr>:#015 19%|█▉        | 75/393 [00:09<00:41,  7.65ba/s][1,2]<stderr>:#015 18%|█▊        | 69/393 [00:09<00:40,  8.10ba/s][1,3]<stderr>:#015 19%|█▉        | 76/393 [00:09<00:38,  8.18ba/s][1,1]<stderr>:#015 18%|█▊        | 69/393 [00:09<00:40,  7.90ba/s][1,4]<stderr>:#015 17%|█▋        | 66/393 [00:08<00:43,  7.49ba/s][1,7]<stderr>:#015 18%|█▊        | 69/393 [00:09<00:41,  7.79ba/s][1,6]<stderr>:#015 18%|█▊        | 71/393 [00:09<00:40,  7.86ba/s][1,0]<stderr>:#015 17%|█▋        | 67/393 [00:09<00:42,  7.64ba/s][1,5]<stderr>:#015 16%|█▋        | 64/393 [00:08<00:48,  6.81ba/s][1,2]<stderr>:#015 18%|█▊        | 70/393 [00:09<00:40,  7.94ba/s][1,1]<stderr>:#015 18%|█▊        | 70/393 [00:09<00:40,  7.95ba/s][1,4]<stderr>:#015 17%|█▋        | 67/393 [00:08<00:42,  7.67ba/s][1,7]<stderr>:#015 18%|█▊        | 70/393 [00:09<00:41,  7.87ba/s][1,5]<stderr>:#015 17%|█▋        | 65/393 [00:08<00:45,  7.25ba/s][1,3]<stderr>:#015 20%|█▉        | 77/393 [00:09<00:40,  7.88ba/s][1,6]<stderr>:#015 18%|█▊        | 72/393 [00:09<00:40,  7.83ba/s][1,0]<stderr>:#015 17%|█▋        | 68/393 [00:09<00:42,  7.58ba/s][1,1]<stderr>:#015 18%|█▊        | 71/393 [00:09<00:39,  8.21ba/s][1,2]<stderr>:#015 18%|█▊        | 71/393 [00:09<00:40,  8.00ba/s][1,3\u001b[0m\n",
      "\u001b[34m]<stderr>:#015 20%|█▉        | 78/393 [00:09<00:39,  8.03ba/s][1,4]<stderr>:#015 17%|█▋        | 68/393 [00:08<00:42,  7.63ba/s][1,7]<stderr>:#015 18%|█▊        | 71/393 [00:09<00:41,  7.71ba/s][1,5]<stderr>:#015 17%|█▋        | 66/393 [00:08<00:44,  7.30ba/s][1,6]<stderr>:#015 19%|█▊        | 73/393 [00:09<00:41,  7.67ba/s][1,0]<stderr>:#015 18%|█▊        | 69/393 [00:09<00:44,  7.28ba/s][1,1]<stderr>:#015 18%|█▊        | 72/393 [00:09<00:40,  7.87ba/s][1,7]<stderr>:#015 18%|█▊        | 72/393 [00:09<00:40,  8.02ba/s][1,2]<stderr>:#015 18%|█▊        | 72/393 [00:09<00:41,  7.78ba/s][1,5]<stderr>:#015 17%|█▋        | 67/393 [00:09<00:42,  7.63ba/s][1,4]<stderr>:#015 18%|█▊        | 69/393 [00:09<00:41,  7.78ba/s][1,0]<stderr>:#015 18%|█▊        | 70/393 [00:09<00:41,  7.82ba/s][1,6]<stderr>:#015 19%|█▉        | 74/393 [00:09<00:42,  7.50ba/s][1,7]<stderr>:#015 19%|█▊        | 73/393 [00:09<00:39,  8.17ba/s][1,5]<stderr>:#015 17%|█▋        | 68/393 [00:09<00:41,  7.91ba/s][1,2]<stderr>:#015 19%|█▊        | 73/393 [00:09<00:40,  7.94ba/s][1,4]<stderr>:#015 18%|█▊        | 70/393 [00:09<00:41,  7.71ba/s][1,1]<stderr>:#015 19%|█▊        | 73/393 [00:09<00:42,  7.54ba/s][1,0]<stderr>:#015 18%|█▊        | 71/393 [00:09<00:43,  7.33ba/s][1,3]<stderr>:#015 20%|██        | 79/393 [00:09<00:59,  5.24ba/s][1,7]<stderr>:#015 19%|█▉        | 74/393 [00:09<00:37,  8.47ba/s][1,4]<stderr>:#015 18%|█▊        | 71/393 [00:09<00:40,  7.89ba/s][1,2]<stderr>:#015 19%|█▉        | 74/393 [00:09<00:41,  7.77ba/s][1,6]<stderr>:#015 19%|█▉        | 76/393 [00:09<00:40,  7.83ba/s][1,1]<stderr>:#015 19%|█▉        | 74/393 [00:09<00:40,  7.79ba/s][1,5]<stderr>:#015 18%|█▊        | 69/393 [00:09<00:45,  7.08ba/s][1,0]<stderr>:#015 18%|█▊        | 72/393 [00:09<00:44,  7.18ba/s][1,7]<stderr>:#015 19%|█▉        | 75/393 [00:09<00:36,  8.63ba/s][1,3]<stderr>:#015 20%|██        | 80/393 [00:10<00:53,  5.82ba/s][1,1]<stderr>:#015 19%|█▉        | 75/393 [00:09<00:40,  7.92ba/s][1,2]<stderr>:#015 19%|█▉        | 75/393 [00:09<00:40,  7.81ba/s][1,4]<stderr>:#015 18%|█▊        | 72/393 [00:09<00:40,  7.85ba/s][1,6]<stderr>:#015 20%|█▉        | 77/393 [00:09<00:42,  7.50ba/s][1,0]<stderr>:#015 19%|█▊        | 73/393 [00:09<00:41,  7.78ba/s][1,5]<stderr>:#015 18%|█▊        | 70/393 [00:09<00:47,  6.87ba/s][1,4]<stderr>:#015 19%|█▊        | 73/393 [00:09<00:38,  8.28ba/s][1,1]<stderr>:#015 19%|█▉        | 76/393 [00:09<00:39,  7.95ba/s][1,2]<stderr>:#015 19%|█▉        | 76/393 [00:09<00:42,  7.39ba/s][1,0]<stderr>:#015 19%|█▉        | 74/393 [00:09<00:39,  8.06ba/s][1,7]<stderr>:#015 20%|█▉        | 77/393 [00:09<00:35,  8.95ba/s][1,6]<stderr>:#015 20%|█▉        | 78/393 [00:09<00:43,  7.22ba/s][1,3]<stderr>:#015 21%|██        | 82/393 [00:10<00:47,  6.53ba/s][1,4]<stderr>:#015 19%|█▉        | 74/393 [00:09<00:37,  8.47ba/s][1,7]<stderr>:#015 20%|█▉        | 78/393 [00:10<00:34,  9.19ba/s][1,1]<stderr>:#015 20%|█▉        | 77/393 [00:10<00:40,  7.74ba/s][1,0]<stderr>:#015 19%|█▉        | 75/393 [00:10<00:38,  8.25ba/s][1,5]<stderr>:#015 18%|█▊        | 72/393 [00:09<00:43,  7.39ba/s][1,3]<stderr>:#015 21%|██        | 83/393 [00:10<00:44,  6.98ba/s][1,2]<stderr>:#015 20%|█▉        | 77/393 [00:10<00:43,  7.19ba/s][1,6]<stderr>:#015 20%|██        | 79/393 [00:10<00:42,  7.35ba/s][1,4]<stderr>:#015 19%|█▉        | 75/393 [00:09<00:36,  8.77ba/s][1,0]<stderr>:#015 19%|█▉        | 76/393 [00:10<00:39,  8.07ba/s][1,5]<stderr>:#015 19%|█▊        | 73/393 [00:09<00:40,  7.81ba/s][1,2]<stderr>:#015 20%|█▉        | 78/393 [00:10<00:41,  7.68ba/s][1,6]<stderr>:#015 20%|██        | 80/393 [00:10<00:40,  7.82ba/s][1,1]<stderr>:#015 20%|█▉        | 78/393 [00:10<00:44,  7.12ba/s][1,4]<stderr>:#015 19%|█▉        | 76/393 [00:09<00:35,  8.97ba/s][1,3]<stderr>:#015 21%|██▏       | 84/393 [00:10<00:43,  7.14ba/s][1,7]<stderr>:#015 20%|██        | 79/393 [00:10<00:44,  7.02ba/s][1,0]<stderr>:#015 20%|█▉        | 77/393 [00:10<00:39,  8.03ba/s][1,5]<stderr>:#015 19%|█▉        | 74/393 [00:09<00:43,  7.31ba/s][1,4]<stderr>:#015 20%|█▉        | 77/393 [00:10<00:37,  8.44ba/s][1,3]<stderr>:#015 22%|██▏       | 86/393 [00:10<00:38,  7.99ba/s][1,6]<stderr>:#015 21%|██        | 81/393 [00:10<00:46,  6.65ba/s][1,2]<stderr>:#015 20%|██        | 79/393 [00:10<00:52,  5.96ba/s][1,7]<stderr>:#015 21%|██        | 81/393 [00:10<00:39,  7.82ba/s][1,4]<stderr>:#015 20%|█▉        | 78/393 [00:10<00:36,  8.68ba/s][1,5]<stderr>:#015 19%|█▉        | 75/393 [00:10<00:41,  7.64ba/s][1,1]<stderr>:#015 20%|██        | 79/393 [00:10<00:54,  5.72ba/s][1,6]<stderr>:#015 21%|██        | 82/393 [00:10<00:44,  6.92ba/s][1,5]<stderr>:#015 19%|█▉        | 76/393 [00:10<00:39,  8.07ba/s][1,3]<stderr>:#015 22%|██▏       | 88/393 [00:10<00:34,  8.80ba/s][1,1]<stderr>:#015 20%|██        | 80/393 [00:10<00:48,  6.50ba/s][1,7]<stderr>:#015 21%|██        | 82/393 [00:10<00:39,  7.91ba/s][1,2]<stderr>:#015 20%|██        | 80/393 [00:10<00:48,  6.47ba/s][1,0]<stderr>:#015 20%|██        | 79/393 [00:10<00:43,  7.18ba/s][1,4]<stderr>:#015 20%|██        | 79/393 [00:10<00:44,  7.04ba/s][1,3]<stderr>:#015 23%|██▎       | 89/393 [00:10<00:33,  9.04ba/s][1,5]<stderr>:#015 20%|█▉        | 77/393 [00:10<00:38,  8.30ba/s][1,7]<stderr>:#015 21%|██        | 83/393 [00:10<00:37,  8.17ba/s][1,1]<stderr>:#015 21%|██        | 81/393 [00:10<00:45,  6.88ba/s][1,2]<stderr>:#015 21%|██        | 81/393 [00:10<00:46,  6.65ba/s][1,6]<stderr>:#015 21%|██▏       | 84/393 [00:10<00:41,  7.43ba/s][1,4]<stderr>:#015 20%|██        | 80/393 [00:10<00:41,  7.53ba/s][1,0]<stderr>:#015 20%|██        | 80/393 [00:10<00:43,  7.26ba/s][1,1]<stderr>:#015 21%|██        | 82/393 [00:10<00:41,  7.54ba/s][1,5]<stderr>:#015 20%|█▉        | 78/393 [00:10<00:38,  8.26ba/s][1,7]<stderr>:#015 21%|██▏       | 84/393 [00:10<00:37,  8.17ba/s][1,3]<stderr>:#015 23%|██▎       | 90/393 [00:11<00:34,  8.74ba/s][1,1]<stderr>:#015 21%|██        | 83/393 [00:10<00:40,  7.63ba/s][1,4]<stderr>:#015 21%|██        | 82/393 [00:10<00:36,  8.57ba/s][1,0]<stderr>:#015 21%|██        | 81/393 [00:10<00:43,  7.19ba/s][1,6]<stderr>:#015 22%|██▏       | 86/393 [00:10<00:37,  8.26ba/s][1,7]<stderr>:#015 22%|██▏       | 85/393 [00:10<00:38,  8.03ba/s][1,2]<stderr>:#015 21%|██        | 83/393 [00:10<00:43,  7.20ba/s][1,3]<stderr>:#015 23%|██▎       | 91/393 [00:11<00:37,  8.02ba/s][1,4]<stderr>:#015 21%|██        | 83/393 [00:10<00:35,  8.84ba/s][1,1]<stderr>:#015 21%|██▏       | 84/393 [00:11<00:38,  8.08ba/s][1,6]<stderr>:#015 22%|██▏       | 87/393 [00:11<00:36,  8.28ba/s][1,2]<stderr>:#015 21%|██▏       | 84/393 [00:11<00:41,  7.49ba/s][1,0]<stderr>:#015 21%|██        | 82/393 [00:11<00:42,  7.39ba/s][1,7]<stderr>:#015 22%|██▏       | 86/393 [00:11<00:38,  7.99ba/s][1,3]<stderr>:#015 23%|██▎       | 92/393 [00:11<00:36,  8.16ba/s][1,4]<stderr>:#015 21%|██▏       | 84/393 [00:10<00:36,  8.49ba/s][1,5]<stderr>:#015 20%|██        | 79/393 [00:10<01:00,  5.18ba/s][1,6]<stderr>:#015 22%|██▏       | 88/393 [00:11<00:36,  8.43ba/s][1,1]<stderr>:#015 22%|██▏       | 85/393 [00:11<00:38,  7.91ba/s][1,0]<stderr>:#015 21%|██        | 83/393 [00:11<00:39,  7.75ba/s][1,2]<stderr>:#015 22%|██▏       | 85/393 [00:11<00:39,  7.82ba/s][1,3]<stderr>:#015 24%|██▎       | 93/393 [00:11<00:35,  8.43ba/s][1,7]<stderr>:#015 22%|██▏       | 87/393 [00:11<00:37,  8.10ba/s][1,1]<stderr>:#015 22%|██▏       | 86/393 [00:11<00:38,  8.02ba/s][1,0]<stderr>:#015 21%|██▏       | 84/393 [00:11<00:38,  7.99ba/s][1,3]<stderr>:#015 24%|██▍       | 94/393 [00:11<00:35,  8.45ba/s][1,2]<stderr>:#015 22%|██▏       | 86/393 [00:11<00:38,  7.88ba/s][1,4]<stderr>:#015 22%|██▏       | 85/393 [00:11<00:38,  8.09ba/s][1,6]<stderr>:#015 23%|██▎       | 89/393 [00:11<00:37,  8.09ba/s][1,5]<stderr>:#015 20%|██        | 80/393 [00:10<00:55,  5.65ba/s][1,2]<stderr>:#015 22%|██▏       | 87/393 [00:11<00:36,  8.29ba/s][1,1]<stderr>:#015 22%|██▏       | 87/393 [00:11<00:38,  7.99ba/s][1,7]<stderr>:#015 23%|██▎       | 89/393 [00:11<00:36,  8.24ba/s][1,5]<stderr>:#015 21%|██        | 81/393 [00:11<00:49,  6.31ba/s][1,0]<stderr>:#015 22%|██▏       | 85/393 [00:11<00:38,  7.91ba/s][1,6]<stderr>:#015 23%|██▎       | 90/393 [00:11<00:37,  8.13ba/s][1,4]<stderr>:#015 22%|██▏       | 86/393 [00:11<00:40,  7.52ba/s][1,7]<stderr>:#015 23%|██▎       | 90/393 [00:11<00:35,  8.44ba/s][1,0]<stderr>:#015 22%|██▏       | 86/393 [00:11<00:37,  8.18ba/s][1,6]<stderr>:#015 23%|██▎       | 91/393 [00:11<00:35,  8.44ba/s][1,2]<stderr>:#015 22%|██▏       | 88/393 [00:11<00:37,  8.15ba/s][1,1]<stderr>:#015 22%|██▏       | 88/393 [00:11<00:37,  8.13ba/s][1,5]<stderr>:#015 21%|██        | 82/393 [00:11<00:45,  6.86ba/s][1,3]<stderr>:#015 24%|██▍       | 95/393 [00:11<00:50,  5.92ba/s][1,7]<stderr>:#015 23%|██▎       | 91/393 [00:11<00:34,  8.72ba/s][1,1]<stderr>:#015 23%|██▎       | 89/393 [00:11<00:35,  8.46ba/s][1,5]<stderr>:#015 21%|██        | 83/393 [00:11<00:42,  7.24ba/s][1,2]<stderr>:#015 23%|██▎       | 89/393 [00:11<00:37,  8.04ba/s][1,6]<stderr>:#015 23%|██▎       | 92/393 [00:11<00:37,  7.94ba/s][1,0]<stderr>:#015 22%|██▏       | 87/393 [00:11<00:40,  7.48ba/s][1,4]<stderr>:#015 22%|██▏       | 88/393 [00:11<00:39,  7.67ba/s][1,3]<stderr>:#015 24%|██▍       | 96/393 [00:12<00:47,  6.29ba/s][1,1]<stderr>:#015 23%|██▎       | 90/393 [00:11<00:34,  8.67ba/s][1,7]<stderr>:#015 23%|██▎       | 92/393 [00:11<00:34,  8.69ba/s][1,2]<stderr>:#015 23%|██▎       | 90/393 [00:11<00:38,  7.84ba/s][1,6]<stderr>:#015 24%|██▎       | 93/393 [00:11<00:38,  7.78ba/s][1,5]<stderr>:#015 22%|██▏       | 85/393 [00:11<00:38,  8.05ba/s][1,0]<stderr>:#015 22%|██▏       | 88/393 [00:11<00:43,  7.02ba/s][1,7]<stderr>:#015 24%|██▎       | 93/393 [00:11<00:34,  8.80ba/s][1,2]<stderr>:#015 23%|██▎       | 91/393 [00:11<00:38,  7.76ba/s][1,6]<stderr>:#015 24%|██▍       | 94/393 [00:11<00:39,  7.63ba/s][1,4]<stderr>:#015 23%|██▎       | 90/393 [00:11<00:38,  7.83ba/s][1,3]<stderr>:#015 25%|██▍       | 98/393 [00:12<00:43,  6.82ba/s][1,5]<stderr>:#015 22%|██▏       | 86/393 [00:11<00:40,  7.62ba/s][1,1]<stderr>:#015 23%|██▎       | 92/393 [00:11<00:35,  8.58ba/s][1,6]<stderr>:#015 24%|██▍       | 95/393 [00:12<00:39,  7.61ba/s][1,3]<stderr>:#015 25%|██▌       | 99/393 [00:12<00:41,  7.04ba/s][1,4]<stderr>:#015 23%|██▎       | 91/393 [00:11<00:40,  7.44ba/s][1,5]<stderr>:#015 22%|██▏       | 87/393 [00:11<00:39,  7.75ba/s][1,2]<stderr>:#015 24%|██▎       | 93/393 [00:12<00:35,  8.51ba/s][1,0]<stderr>:#015 23%|██▎       | 90/393 [00:12<00:43,  7.01ba/s][1,7]<stderr>:#015 24%|██▍       | 95/393 [00:12<00:38,  7.82ba/s][1,6]<stderr>:#015 24%|██▍       | 96/393 [00:12<00:38,  7.79ba/s][1,1]<stderr>:#015 24%|██▍       | 94/393 [00:12<00:33,  8.80ba/s][1,5]<stderr>:#015 22%|██▏       | 88/393 [00:11<00:37,  8.17ba/s][1,4]<stderr>:#015 23%|██▎       | 92/393 [00:11<00:39,  7.53ba/s][1,3]<stderr>:#015 25%|██▌       | 100/393 [00:12<00:43,  6.70ba/s][1,2]<stderr>:#015 24%|██▍       | 94/393 [00:12<00:39,  7.64ba/s][1,7]<stderr>:#015 24%|██▍       | 96/393 [00:12<00:36,  8.19ba/s][1,5]<stderr>:#015 23%|██▎       | 89/393 [00:11<00:37,  8.04ba/s][1,4]<stderr>:#015 24%|██▎       | 93/393 [00:12<00:38,  7.75ba/s][1,6]<stderr>:#015 25%|██▍       | 97/393 [00:12<00:41,  7.16ba/s][1,3]<stderr>:#015 26%|██▌       | 101/393 [00:12<00:42,  6.93ba/s][1,0]<stderr>:#015 23%|██▎       | 92/393 [00:12<00:41,  7.31ba/s][1,7]<stderr>:#015 25%|██▍       | 98/393 [00:12<00:32,  8.95ba/s][1,5]<stderr>:#015 23%|██▎       | 90/393 [00:12<00:38,  7.87ba/s][1,3]<stderr>:#015 26%|██▌       | 102/393 [00:12<00:38,  7.52ba/s][1,1]<stderr>:#015 24%|██▍       | 95/393 [00:12<00:50,  5.95ba/s][1,2]<stderr>:#015 24%|██▍       | 95/393 [00:12<00:51,  5.78ba/s][1,3]<stderr>:#015 26%|██▌       | 103/393 [00:12<00:35,  8.12ba/s][1,5]<stderr>:#015 23%|██▎       | 91/393 [00:12<00:36,  8.20ba/s][1,7]<stderr>:#015 25%|██▌       | 99/393 [00:12<00:34,  8.56ba/s][1,6]<stderr>:#015 25%|██▍       | 98/393 [00:12<00:49,  5.96ba/s][1,0]<stderr>:#015 24%|██▍       | 94/393 [00:12<00:38,  7.82ba/s][1,4]<stderr>:#015 24%|██▍       | 95/393 [00:12<00:39,  7.51ba/s][1,6]<stderr>:#015 25%|██▌       | 99/393 [00:12<00:45,  6.53ba/s][1,1]<stderr>:#015 25%|██▍       | 97/393 [00:12<00:44,  6.59ba/s][1,2]<stderr>:#015 25%|██▍       | 97/393 [00:12<00:44,  6.72ba/s][1,7]<stderr>:#015 25%|██▌       | 100/393 [00:12<00:35,  8.20ba/s][1,3]<stderr>:#015 26%|██▋       | 104/393 [00:13<00:37,  7.74ba/s][1,5]<stderr>:#015 23%|██▎       | 92/393 [00:12<00:41,  7.26ba/s][1,4]<stderr>:#015 24%|██▍       | 96/393 [00:12<00:39,  7.43ba/s][1,6]<stderr>:#015 25%|██▌       | 100/393 [00:12<00:42,  6.88ba/s][1,1]<stderr>:#015 25%|██▍       | 98/393 [00:12<00:42,  6.92ba/s][1,2]<stderr>:#015 25%|██▍       | 98/393 [00:12<00:41,  7.04ba/s][1,7]<stderr>:#015 26%|██▌       | 101/393 [00:12<00:36,  8.09ba/s][1,3]<stderr>:#015 27%|██▋       | 105/393 [00:13<00:37,  7.77ba/s][1,5]<stderr>:#015 24%|██▎       | 93/393 [00:12<00:38,  7.89ba/s][1,0]<stderr>:#015 24%|██▍       | 95/393 [00:12<00:50,  5.93ba/s][1,4]<stderr>:#015 25%|██▍       | 97/393 [00:12<00:39,  7.54ba/s][1,0]<stderr>:#015 24%|██▍       | 96/393 [00:13<00:44,  6.70ba/s][1,2]<stderr>:#015 25%|██▌       | 99/393 [00:13<00:40,  7.32ba/s][1,6]<stderr>:#015 26%|██▌       | 101/393 [00:12<00:41,  7.02ba/s][1,7]<stderr>:#015 26%|██▌       | 102/393 [00:13<00:36,  8.07ba/s][1,1]<stderr>:#015 25%|██▌       | 99/393 [00:12<00:41,  7.11ba/s][1,3]<stderr>:#015 27%|██▋       | 106/393 [00:13<00:36,  7.86ba/s][1,5]<stderr>:#015 24%|██▍       | 94/393 [00:12<00:37,  8.02ba/s][1,4]<stderr>:#015 25%|██▍       | 98/393 [00:12<00:39,  7.43ba/s][1,0]<stderr>:#015 25%|██▍       | 97/393 [00:13<00:41,  7.20ba/s][1,7]<stderr>:#015 26%|██▌       | 103/393 [00:13<00:35,  8.08ba/s][1,2]<stderr>:#015 25%|██▌       | 100/393 [00:13<00:39,  7.43ba/s][1,6]<stderr>:#015 26%|██▌       | 102/393 [00:13<00:40,  7.22ba/s][1,3]<stderr>:#015 27%|██▋       | 107/393 [00:13<00:36,  7.80ba/s][1,1]<stderr>:#015 25%|██▌       | 100/393 [00:13<00:41,  7.07ba/s][1,0]<stderr>:#015 25%|██▍       | 98/393 [00:13<00:38,  7.67ba/s][1,5]<stderr>:#015 24%|██▍       | 95/393 [00:12<00:45,  6.60ba/s][1,2]<stderr>:#015 26%|██▌       | 101/393 [00:13<00:38,  7.65ba/s][1,6]<stderr>:#015 26%|██▌       | 103/393 [00:13<00:38,  7.55ba/s][1,7]<stderr>:#015 26%|██▋       | 104/393 [00:13<00:36,  7.99ba/s][1,3]<stderr>:#015 27%|██▋       | 108/393 [00:13<00:36,  7.89ba/s][1,1]<stderr>:#015 26%|██▌       | 101/393 [00:13<00:38,  7.55ba/s][1,4]<stderr>:#015 25%|██▌       | 100/393 [00:12<00:36,  7.97ba/s][1,5]<stderr>:#015 24%|██▍       | 96/393 [00:12<00:40,  7.31ba/s][1,0]<stderr>:#015 25%|██▌       | 99/393 [00:13<00:36,  8.06ba/s][1,2]<stderr>:#015 26%|██▌       | 102/393 [00:13<00:36,  7.93ba/s][1,6]<stderr>:#015 26%|██▋       | 104/393 [00:13<00:37,  7.75ba/s][1,4]<stderr>:#015 26%|██▌       | 101/393 [00:13<00:35,  8.33ba/s][1,3]<stderr>:#015 28%|██▊       | 109/393 [00:13<00:35,  8.02ba/s][1,7]<stderr>:#015 27%|██▋       | 105/393 [00:13<00:36,  7.91ba/s][1,1]<stderr>:#015 26%|██▌       | 102/393 [00:13<00:38,  7.60ba/s][1,2]<stderr>:#015 26%|██▌       | 103/393 [00:13<00:37,  7.83ba/s][1,7]<stderr>:#015 27%|██▋       | 106/393 [00:13<00:35,  8.01ba/s][1,0]<stderr>:#015 26%|██▌       | 101/393 [00:13<00:33,  8.77ba/s][1,5]<stderr>:#015 25%|██▍       | 98/393 [00:13<00:36,  8.08ba/s][1,1]<stderr>:#015 26%|██▌       | 103/393 [00:13<00:37,  7.78ba/s][1,4]<stderr>:#015 26%|██▌       | 102/393 [00:13<00:36,  7.99ba/s][1,6]<stderr>:#015 27%|██▋       | 105/393 [00:13<00:38,  7.51ba/s][1,3]<stderr>:#015 28%|██▊       | 110/393 [00:13<00:36,  7.70ba/s][1,1]<stderr>:#015 26%|██▋       | 104/393 [00:13<00:35,  8.10ba/s][1,4]<stderr>:#015 26%|██▌       | 103/393 [00:13<00:35,  8.22ba/s][1,0]<stderr>:#015 26%|██▌       | 102/393 [00:13<00:33,  8.62ba/s][1,2]<stderr>:#015 26%|██▋       | 104/393 \u001b[0m\n",
      "\u001b[34m[00:13<00:37,  7.76ba/s][1,5]<stderr>:#015 25%|██▌       | 99/393 [00:13<00:36,  8.04ba/s][1,6]<stderr>:#015 27%|██▋       | 106/393 [00:13<00:38,  7.54ba/s][1,7]<stderr>:#015 27%|██▋       | 107/393 [00:13<00:37,  7.56ba/s][1,3]<stderr>:#015 28%|██▊       | 111/393 [00:13<00:37,  7.44ba/s][1,4]<stderr>:#015 26%|██▋       | 104/393 [00:13<00:35,  8.11ba/s][1,6]<stderr>:#015 27%|██▋       | 107/393 [00:13<00:35,  7.96ba/s][1,7]<stderr>:#015 27%|██▋       | 108/393 [00:13<00:34,  8.15ba/s][1,0]<stderr>:#015 26%|██▌       | 103/393 [00:13<00:35,  8.23ba/s][1,1]<stderr>:#015 27%|██▋       | 105/393 [00:13<00:37,  7.77ba/s][1,5]<stderr>:#015 25%|██▌       | 100/393 [00:13<00:36,  7.93ba/s][1,2]<stderr>:#015 27%|██▋       | 105/393 [00:13<00:38,  7.40ba/s][1,7]<stderr>:#015 28%|██▊       | 109/393 [00:13<00:34,  8.27ba/s][1,1]<stderr>:#015 27%|██▋       | 106/393 [00:13<00:35,  8.11ba/s][1,0]<stderr>:#015 26%|██▋       | 104/393 [00:13<00:34,  8.37ba/s][1,4]<stderr>:#015 27%|██▋       | 105/393 [00:13<00:35,  8.07ba/s][1,6]<stderr>:#015 27%|██▋       | 108/393 [00:13<00:35,  8.02ba/s][1,5]<stderr>:#015 26%|██▌       | 101/393 [00:13<00:35,  8.17ba/s][1,2]<stderr>:#015 27%|██▋       | 106/393 [00:13<00:37,  7.65ba/s][1,3]<stderr>:#015 28%|██▊       | 112/393 [00:14<00:49,  5.66ba/s][1,5]<stderr>:#015 26%|██▌       | 102/393 [00:13<00:35,  8.14ba/s][1,4]<stderr>:#015 27%|██▋       | 106/393 [00:13<00:36,  7.95ba/s][1,0]<stderr>:#015 27%|██▋       | 105/393 [00:14<00:35,  8.12ba/s][1,7]<stderr>:#015 28%|██▊       | 110/393 [00:14<00:35,  7.93ba/s][1,1]<stderr>:#015 27%|██▋       | 107/393 [00:13<00:36,  7.81ba/s][1,2]<stderr>:#015 27%|██▋       | 107/393 [00:14<00:36,  7.93ba/s][1,6]<stderr>:#015 28%|██▊       | 109/393 [00:13<00:36,  7.79ba/s][1,4]<stderr>:#015 27%|██▋       | 107/393 [00:13<00:36,  7.94ba/s][1,6]<stderr>:#015 28%|██▊       | 110/393 [00:14<00:35,  7.89ba/s][1,2]<stderr>:#015 27%|██▋       | 108/393 [00:14<00:35,  7.98ba/s][1,5]<stderr>:#015 26%|██▌       | 103/393 [00:13<00:37,  7.84ba/s][1,7]<stderr>:#015 28%|██▊       | 111/393 [00:14<00:36,  7.82ba/s][1,3]<stderr>:#015 29%|██▉       | 114/393 [00:14<00:43,  6.47ba/s][1,1]<stderr>:#015 27%|██▋       | 108/393 [00:14<00:37,  7.69ba/s][1,0]<stderr>:#015 27%|██▋       | 106/393 [00:14<00:36,  7.81ba/s][1,3]<stderr>:#015 29%|██▉       | 115/393 [00:14<00:39,  6.97ba/s][1,5]<stderr>:#015 26%|██▋       | 104/393 [00:13<00:36,  7.89ba/s][1,6]<stderr>:#015 28%|██▊       | 111/393 [00:14<00:36,  7.81ba/s][1,1]<stderr>:#015 28%|██▊       | 109/393 [00:14<00:36,  7.79ba/s][1,2]<stderr>:#015 28%|██▊       | 109/393 [00:14<00:36,  7.71ba/s][1,4]<stderr>:#015 27%|██▋       | 108/393 [00:13<00:37,  7.54ba/s][1,0]<stderr>:#015 27%|██▋       | 107/393 [00:14<00:37,  7.64ba/s][1,7]<stderr>:#015 28%|██▊       | 112/393 [00:14<00:43,  6.47ba/s][1,1]<stderr>:#015 28%|██▊       | 110/393 [00:14<00:34,  8.25ba/s][1,3]<stderr>:#015 30%|██▉       | 116/393 [00:14<00:38,  7.22ba/s][1,0]<stderr>:#015 27%|██▋       | 108/393 [00:14<00:35,  7.98ba/s][1,6]<stderr>:#015 28%|██▊       | 112/393 [00:14<00:37,  7.55ba/s][1,5]<stderr>:#015 27%|██▋       | 106/393 [00:14<00:34,  8.27ba/s][1,3]<stderr>:#015 30%|██▉       | 117/393 [00:14<00:35,  7.75ba/s][1,1]<stderr>:#015 28%|██▊       | 111/393 [00:14<00:34,  8.18ba/s][1,7]<stderr>:#015 29%|██▉       | 113/393 [00:14<00:42,  6.63ba/s][1,4]<stderr>:#015 28%|██▊       | 110/393 [00:14<00:35,  7.94ba/s][1,0]<stderr>:#015 28%|██▊       | 109/393 [00:14<00:34,  8.27ba/s][1,2]<stderr>:#015 28%|██▊       | 111/393 [00:14<00:36,  7.64ba/s][1,4]<stderr>:#015 28%|██▊       | 111/393 [00:14<00:35,  8.05ba/s][1,3]<stderr>:#015 30%|███       | 118/393 [00:14<00:35,  7.78ba/s][1,7]<stderr>:#015 29%|██▉       | 114/393 [00:14<00:39,  7.02ba/s][1,0]<stderr>:#015 28%|██▊       | 110/393 [00:14<00:34,  8.14ba/s][1,5]<stderr>:#015 27%|██▋       | 108/393 [00:14<00:31,  9.11ba/s][1,1]<stderr>:#015 28%|██▊       | 112/393 [00:14<00:41,  6.78ba/s][1,6]<stderr>:#015 29%|██▉       | 114/393 [00:14<00:38,  7.27ba/s][1,3]<stderr>:#015 30%|███       | 119/393 [00:15<00:34,  8.05ba/s][1,2]<stderr>:#015 28%|██▊       | 112/393 [00:14<00:44,  6.27ba/s][1,6]<stderr>:#015 29%|██▉       | 115/393 [00:14<00:36,  7.70ba/s][1,7]<stderr>:#015 30%|██▉       | 116/393 [00:14<00:36,  7.67ba/s][1,1]<stderr>:#015 29%|██▉       | 113/393 [00:14<00:40,  6.97ba/s][1,3]<stderr>:#015 31%|███       | 120/393 [00:15<00:32,  8.39ba/s][1,5]<stderr>:#015 28%|██▊       | 110/393 [00:14<00:30,  9.35ba/s][1,4]<stderr>:#015 28%|██▊       | 112/393 [00:14<00:44,  6.30ba/s][1,7]<stderr>:#015 30%|██▉       | 117/393 [00:14<00:35,  7.86ba/s][1,3]<stderr>:#015 31%|███       | 121/393 [00:15<00:31,  8.74ba/s][1,6]<stderr>:#015 30%|██▉       | 116/393 [00:14<00:35,  7.73ba/s][1,5]<stderr>:#015 28%|██▊       | 111/393 [00:14<00:29,  9.50ba/s][1,1]<stderr>:#015 29%|██▉       | 114/393 [00:14<00:38,  7.26ba/s][1,0]<stderr>:#015 28%|██▊       | 112/393 [00:14<00:38,  7.31ba/s][1,2]<stderr>:#015 29%|██▉       | 114/393 [00:14<00:39,  7.03ba/s][1,7]<stderr>:#015 30%|███       | 118/393 [00:15<00:34,  8.00ba/s][1,1]<stderr>:#015 29%|██▉       | 115/393 [00:15<00:35,  7.76ba/s][1,6]<stderr>:#015 30%|██▉       | 117/393 [00:15<00:35,  7.74ba/s][1,3]<stderr>:#015 31%|███       | 122/393 [00:15<00:32,  8.38ba/s][1,0]<stderr>:#015 29%|██▉       | 113/393 [00:15<00:37,  7.50ba/s][1,4]<stderr>:#015 29%|██▉       | 114/393 [00:14<00:41,  6.69ba/s][1,5]<stderr>:#015 28%|██▊       | 112/393 [00:14<00:39,  7.08ba/s][1,1]<stderr>:#015 30%|██▉       | 116/393 [00:15<00:34,  8.05ba/s][1,6]<stderr>:#015 30%|███       | 118/393 [00:15<00:34,  8.07ba/s][1,2]<stderr>:#015 30%|██▉       | 116/393 [00:15<00:36,  7.56ba/s][1,7]<stderr>:#015 30%|███       | 119/393 [00:15<00:34,  7.97ba/s][1,3]<stderr>:#015 31%|███▏      | 123/393 [00:15<00:36,  7.49ba/s][1,6]<stderr>:#015 30%|███       | 119/393 [00:15<00:33,  8.23ba/s][1,2]<stderr>:#015 30%|██▉       | 117/393 [00:15<00:35,  7.87ba/s][1,0]<stderr>:#015 29%|██▉       | 115/393 [00:15<00:34,  7.99ba/s][1,1]<stderr>:#015 30%|██▉       | 117/393 [00:15<00:34,  8.00ba/s][1,7]<stderr>:#015 31%|███       | 120/393 [00:15<00:34,  8.02ba/s][1,4]<stderr>:#015 30%|██▉       | 116/393 [00:15<00:37,  7.36ba/s][1,5]<stderr>:#015 29%|██▉       | 114/393 [00:14<00:34,  8.07ba/s][1,1]<stderr>:#015 30%|███       | 118/393 [00:15<00:32,  8.35ba/s][1,7]<stderr>:#015 31%|███       | 121/393 [00:15<00:32,  8.30ba/s][1,4]<stderr>:#015 30%|██▉       | 117/393 [00:15<00:34,  7.92ba/s][1,2]<stderr>:#015 30%|███       | 118/393 [00:15<00:34,  7.94ba/s][1,0]<stderr>:#015 30%|██▉       | 116/393 [00:15<00:34,  7.92ba/s][1,3]<stderr>:#015 32%|███▏      | 125/393 [00:15<00:33,  8.10ba/s][1,6]<stderr>:#015 31%|███       | 120/393 [00:15<00:34,  7.86ba/s][1,7]<stderr>:#015 31%|███       | 122/393 [00:15<00:31,  8.61ba/s][1,4]<stderr>:#015 30%|███       | 118/393 [00:15<00:32,  8.35ba/s][1,2]<stderr>:#015 30%|███       | 119/393 [00:15<00:32,  8.40ba/s][1,1]<stderr>:#015 30%|███       | 119/393 [00:15<00:33,  8.14ba/s][1,3]<stderr>:#015 32%|███▏      | 126/393 [00:15<00:31,  8.38ba/s][1,6]<stderr>:#015 31%|███       | 121/393 [00:15<00:33,  8.08ba/s][1,5]<stderr>:#015 30%|██▉       | 116/393 [00:15<00:33,  8.16ba/s][1,0]<stderr>:#015 30%|██▉       | 117/393 [00:15<00:36,  7.48ba/s][1,7]<stderr>:#015 31%|███▏      | 123/393 [00:15<00:31,  8.61ba/s][1,2]<stderr>:#015 31%|███       | 120/393 [00:15<00:32,  8.44ba/s][1,4]<stderr>:#015 30%|███       | 119/393 [00:15<00:32,  8.39ba/s][1,6]<stderr>:#015 31%|███       | 122/393 [00:15<00:33,  8.10ba/s][1,1]<stderr>:#015 31%|███       | 120/393 [00:15<00:34,  7.91ba/s][1,5]<stderr>:#015 30%|██▉       | 117/393 [00:15<00:35,  7.69ba/s][1,4]<stderr>:#015 31%|███       | 120/393 [00:15<00:32,  8.49ba/s][1,7]<stderr>:#015 32%|███▏      | 124/393 [00:15<00:31,  8.41ba/s][1,2]<stderr>:#015 31%|███       | 121/393 [00:15<00:33,  8.01ba/s][1,0]<stderr>:#015 30%|███       | 119/393 [00:15<00:33,  8.09ba/s][1,6]<stderr>:#015 31%|███▏      | 123/393 [00:15<00:32,  8.38ba/s][1,3]<stderr>:#015 33%|███▎      | 128/393 [00:16<00:34,  7.67ba/s][1,4]<stderr>:#015 31%|███       | 121/393 [00:15<00:32,  8.39ba/s][1,1]<stderr>:#015 31%|███       | 122/393 [00:15<00:32,  8.29ba/s][1,6]<stderr>:#015 32%|███▏      | 124/393 [00:15<00:31,  8.42ba/s][1,7]<stderr>:#015 32%|███▏      | 125/393 [00:15<00:33,  8.03ba/s][1,2]<stderr>:#015 31%|███       | 122/393 [00:15<00:33,  8.03ba/s][1,0]<stderr>:#015 31%|███       | 120/393 [00:15<00:33,  8.08ba/s][1,5]<stderr>:#015 30%|███       | 119/393 [00:15<00:34,  7.98ba/s][1,3]<stderr>:#015 33%|███▎      | 129/393 [00:16<00:33,  7.78ba/s][1,4]<stderr>:#015 31%|███       | 122/393 [00:15<00:32,  8.26ba/s][1,0]<stderr>:#015 31%|███       | 121/393 [00:16<00:32,  8.49ba/s][1,2]<stderr>:#015 31%|███▏      | 123/393 [00:16<00:32,  8.39ba/s][1,1]<stderr>:#015 31%|███▏      | 123/393 [00:15<00:34,  7.91ba/s][1,7]<stderr>:#015 32%|███▏      | 126/393 [00:16<00:34,  7.80ba/s][1,6]<stderr>:#015 32%|███▏      | 126/393 [00:16<00:30,  8.82ba/s][1,4]<stderr>:#015 31%|███▏      | 123/393 [00:15<00:31,  8.65ba/s][1,3]<stderr>:#015 33%|███▎      | 130/393 [00:16<00:33,  7.74ba/s][1,2]<stderr>:#015 32%|███▏      | 124/393 [00:16<00:30,  8.76ba/s][1,1]<stderr>:#015 32%|███▏      | 124/393 [00:16<00:32,  8.38ba/s][1,5]<stderr>:#015 31%|███       | 121/393 [00:15<00:31,  8.61ba/s][1,0]<stderr>:#015 31%|███       | 122/393 [00:16<00:32,  8.25ba/s][1,4]<stderr>:#015 32%|███▏      | 124/393 [00:15<00:31,  8.65ba/s][1,2]<stderr>:#015 32%|███▏      | 125/393 [00:16<00:30,  8.81ba/s][1,6]<stderr>:#015 32%|███▏      | 127/393 [00:16<00:31,  8.36ba/s][1,0]<stderr>:#015 31%|███▏      | 123/393 [00:16<00:32,  8.44ba/s][1,1]<stderr>:#015 32%|███▏      | 125/393 [00:16<00:32,  8.34ba/s][1,3]<stderr>:#015 33%|███▎      | 131/393 [00:16<00:35,  7.43ba/s][1,5]<stderr>:#015 31%|███       | 122/393 [00:15<00:32,  8.45ba/s][1,4]<stderr>:#015 32%|███▏      | 125/393 [00:16<00:30,  8.78ba/s][1,2]<stderr>:#015 32%|███▏      | 126/393 [00:16<00:29,  8.93ba/s][1,1]<stderr>:#015 32%|███▏      | 126/393 [00:16<00:31,  8.59ba/s][1,3]<stderr>:#015 34%|███▎      | 132/393 [00:16<00:32,  7.95ba/s][1,0]<stderr>:#015 32%|███▏      | 124/393 [00:16<00:31,  8.51ba/s][1,7]<stderr>:#015 33%|███▎      | 128/393 [00:16<00:37,  7.16ba/s][1,5]<stderr>:#015 31%|███▏      | 123/393 [00:16<00:32,  8.36ba/s][1,6]<stderr>:#015 33%|███▎      | 129/393 [00:16<00:29,  8.99ba/s][1,4]<stderr>:#015 32%|███▏      | 126/393 [00:16<00:30,  8.78ba/s][1,3]<stderr>:#015 34%|███▍      | 133/393 [00:16<00:31,  8.25ba/s][1,7]<stderr>:#015 33%|███▎      | 129/393 [00:16<00:35,  7.53ba/s][1,1]<stderr>:#015 32%|███▏      | 127/393 [00:16<00:32,  8.23ba/s][1,5]<stderr>:#015 32%|███▏      | 124/393 [00:16<00:31,  8.43ba/s][1,0]<stderr>:#015 32%|███▏      | 125/393 [00:16<00:35,  7.62ba/s][1,6]<stderr>:#015 33%|███▎      | 130/393 [00:16<00:29,  8.90ba/s][1,2]<stderr>:#015 33%|███▎      | 128/393 [00:16<00:32,  8.21ba/s][1,7]<stderr>:#015 33%|███▎      | 130/393 [00:16<00:35,  7.36ba/s][1,0]<stderr>:#015 32%|███▏      | 126/393 [00:16<00:33,  7.90ba/s][1,5]<stderr>:#015 32%|███▏      | 126/393 [00:16<00:28,  9.40ba/s][1,3]<stderr>:#015 34%|███▍      | 135/393 [00:16<00:29,  8.86ba/s][1,1]<stderr>:#015 33%|███▎      | 128/393 [00:16<00:40,  6.61ba/s][1,0]<stderr>:#015 32%|███▏      | 127/393 [00:16<00:31,  8.42ba/s][1,7]<stderr>:#015 33%|███▎      | 131/393 [00:16<00:33,  7.74ba/s][1,4]<stderr>:#015 33%|███▎      | 128/393 [00:16<00:33,  7.81ba/s][1,6]<stderr>:#015 33%|███▎      | 131/393 [00:16<00:38,  6.73ba/s][1,2]<stderr>:#015 33%|███▎      | 130/393 [00:16<00:29,  8.92ba/s][1,7]<stderr>:#015 34%|███▎      | 132/393 [00:16<00:33,  7.73ba/s][1,3]<stderr>:#015 35%|███▍      | 137/393 [00:17<00:28,  8.89ba/s][1,4]<stderr>:#015 33%|███▎      | 129/393 [00:16<00:32,  8.01ba/s][1,1]<stderr>:#015 33%|███▎      | 130/393 [00:16<00:34,  7.60ba/s][1,6]<stderr>:#015 34%|███▎      | 132/393 [00:16<00:36,  7.09ba/s][1,5]<stderr>:#015 33%|███▎      | 128/393 [00:16<00:31,  8.41ba/s][1,2]<stderr>:#015 34%|███▎      | 132/393 [00:17<00:27,  9.45ba/s][1,4]<stderr>:#015 33%|███▎      | 130/393 [00:16<00:31,  8.37ba/s][1,0]<stderr>:#015 33%|███▎      | 128/393 [00:17<00:42,  6.26ba/s][1,7]<stderr>:#015 34%|███▍      | 133/393 [00:17<00:33,  7.75ba/s][1,6]<stderr>:#015 34%|███▍      | 133/393 [00:16<00:34,  7.54ba/s][1,1]<stderr>:#015 33%|███▎      | 131/393 [00:16<00:33,  7.83ba/s][1,3]<stderr>:#015 35%|███▌      | 139/393 [00:17<00:27,  9.27ba/s][1,2]<stderr>:#015 34%|███▍      | 133/393 [00:17<00:27,  9.54ba/s][1,4]<stderr>:#015 33%|███▎      | 131/393 [00:16<00:32,  7.95ba/s][1,6]<stderr>:#015 34%|███▍      | 134/393 [00:17<00:34,  7.59ba/s][1,1]<stderr>:#015 34%|███▎      | 132/393 [00:17<00:33,  7.79ba/s][1,7]<stderr>:#015 34%|███▍      | 134/393 [00:17<00:33,  7.70ba/s][1,5]<stderr>:#015 33%|███▎      | 130/393 [00:16<00:29,  8.93ba/s][1,0]<stderr>:#015 33%|███▎      | 129/393 [00:17<00:41,  6.43ba/s][1,2]<stderr>:#015 34%|███▍      | 134/393 [00:17<00:28,  9.05ba/s][1,0]<stderr>:#015 33%|███▎      | 130/393 [00:17<00:36,  7.12ba/s][1,1]<stderr>:#015 34%|███▍      | 133/393 [00:17<00:32,  7.98ba/s][1,3]<stderr>:#015 36%|███▌      | 141/393 [00:17<00:25,  9.79ba/s][1,6]<stderr>:#015 34%|███▍      | 135/393 [00:17<00:33,  7.74ba/s][1,4]<stderr>:#015 34%|███▎      | 132/393 [00:16<00:32,  7.94ba/s][1,7]<stderr>:#015 34%|███▍      | 135/393 [00:17<00:34,  7.39ba/s][1,5]<stderr>:#015 33%|███▎      | 131/393 [00:16<00:32,  7.97ba/s][1,2]<stderr>:#015 34%|███▍      | 135/393 [00:17<00:27,  9.26ba/s][1,1]<stderr>:#015 34%|███▍      | 134/393 [00:17<00:32,  7.93ba/s][1,4]<stderr>:#015 34%|███▍      | 133/393 [00:17<00:32,  8.01ba/s][1,0]<stderr>:#015 33%|███▎      | 131/393 [00:17<00:36,  7.25ba/s][1,6]<stderr>:#015 35%|███▍      | 136/393 [00:17<00:32,  7.80ba/s][1,7]<stderr>:#015 35%|███▍      | 136/393 [00:17<00:32,  7.84ba/s][1,3]<stderr>:#015 36%|███▋      | 143/393 [00:17<00:25,  9.63ba/s][1,5]<stderr>:#015 34%|███▍      | 133/393 [00:17<00:30,  8.64ba/s][1,4]<stderr>:#015 34%|███▍      | 134/393 [00:17<00:31,  8.33ba/s][1,6]<stderr>:#015 35%|███▍      | 137/393 [00:17<00:31,  8.12ba/s][1,7]<stderr>:#015 35%|███▍      | 137/393 [00:17<00:30,  8.26ba/s][1,1]<stderr>:#015 34%|███▍      | 135/393 [00:17<00:32,  8.04ba/s][1,2]<stderr>:#015 35%|███▍      | 137/393 [00:17<00:27,  9.44ba/s][1,0]<stderr>:#015 34%|███▎      | 132/393 [00:17<00:38,  6.85ba/s][1,3]<stderr>:#015 37%|███▋      | 144/393 [00:17<00:26,  9.31ba/s][1,4]<stderr>:#015 34%|███▍      | 135/393 [00:17<00:29,  8.73ba/s][1,6]<stderr>:#015 35%|███▌      | 138/393 [00:17<00:29,  8.57ba/s][1,5]<stderr>:#015 34%|███▍      | 134/393 [00:17<00:29,  8.67ba/s][1,7]<stderr>:#015 35%|███▌      | 138/393 [00:17<00:30,  8.43ba/s][1,1]<stderr>:#015 35%|███▍      | 136/393 [00:17<00:31,  8.05ba/s][1,0]<stderr>:#015 34%|███▍      | 133/393 [00:17<00:34,  7.47ba/s][1,2]<stderr>:#015 35%|███▌      | 138/393 [00:17<00:30,  8.42ba/s][1,4]<stderr>:#015 35%|███▍      | 136/393 [00:17<00:28,  8.98ba/s][1,5]<stderr>:#015 34%|███▍      | 135/393 [00:17<00:30,  8.39ba/s][1,7]<stderr>:#015 35%|███▌      | 139/393 [00:17<00:29,  8.48ba/s][1,1]<stderr>:#015 35%|███▍      | 137/393 [00:17<00:30,  8.46ba/s][1,6]<stderr>:#015 36%|███▌      | 140/393 [00:17<00:27,  9.29ba/s][1,2]<stderr>:#015 35%|███▌      | 139/393 [00:17<00:30,  8.35ba/s][1,0]<stderr>:#015 34%|███▍      | 135/393 [\u001b[0m\n",
      "\u001b[34m00:17<00:31,  8.30ba/s][1,7]<stderr>:#015 36%|███▌      | 140/393 [00:17<00:28,  8.80ba/s][1,1]<stderr>:#015 35%|███▌      | 138/393 [00:17<00:28,  8.79ba/s][1,3]<stderr>:#015 37%|███▋      | 145/393 [00:18<00:37,  6.67ba/s][1,4]<stderr>:#015 35%|███▌      | 138/393 [00:17<00:27,  9.39ba/s][1,2]<stderr>:#015 36%|███▌      | 140/393 [00:17<00:29,  8.45ba/s][1,5]<stderr>:#015 35%|███▍      | 137/393 [00:17<00:28,  8.93ba/s][1,3]<stderr>:#015 37%|███▋      | 146/393 [00:18<00:34,  7.17ba/s][1,1]<stderr>:#015 35%|███▌      | 139/393 [00:17<00:29,  8.59ba/s][1,7]<stderr>:#015 36%|███▌      | 141/393 [00:17<00:29,  8.43ba/s][1,6]<stderr>:#015 36%|███▌      | 142/393 [00:17<00:27,  9.04ba/s][1,4]<stderr>:#015 35%|███▌      | 139/393 [00:17<00:29,  8.58ba/s][1,2]<stderr>:#015 36%|███▌      | 141/393 [00:18<00:30,  8.20ba/s][1,5]<stderr>:#015 35%|███▌      | 138/393 [00:17<00:29,  8.72ba/s][1,0]<stderr>:#015 35%|███▍      | 137/393 [00:18<00:29,  8.59ba/s][1,1]<stderr>:#015 36%|███▌      | 140/393 [00:18<00:32,  7.68ba/s][1,6]<stderr>:#015 36%|███▋      | 143/393 [00:18<00:29,  8.57ba/s][1,3]<stderr>:#015 38%|███▊      | 148/393 [00:18<00:31,  7.75ba/s][1,0]<stderr>:#015 35%|███▌      | 138/393 [00:18<00:29,  8.52ba/s][1,4]<stderr>:#015 36%|███▌      | 140/393 [00:17<00:31,  8.09ba/s][1,2]<stderr>:#015 36%|███▌      | 142/393 [00:18<00:31,  7.91ba/s][1,7]<stderr>:#015 36%|███▋      | 143/393 [00:18<00:28,  8.74ba/s][1,5]<stderr>:#015 35%|███▌      | 139/393 [00:17<00:33,  7.52ba/s][1,0]<stderr>:#015 35%|███▌      | 139/393 [00:18<00:30,  8.46ba/s][1,2]<stderr>:#015 36%|███▋      | 143/393 [00:18<00:31,  8.05ba/s][1,3]<stderr>:#015 38%|███▊      | 149/393 [00:18<00:32,  7.52ba/s][1,4]<stderr>:#015 36%|███▌      | 141/393 [00:17<00:32,  7.82ba/s][1,1]<stderr>:#015 36%|███▌      | 142/393 [00:18<00:30,  8.30ba/s][1,7]<stderr>:#015 37%|███▋      | 144/393 [00:18<00:30,  8.15ba/s][1,6]<stderr>:#015 37%|███▋      | 145/393 [00:18<00:28,  8.76ba/s][1,0]<stderr>:#015 36%|███▌      | 140/393 [00:18<00:29,  8.66ba/s][1,2]<stderr>:#015 37%|███▋      | 144/393 [00:18<00:30,  8.22ba/s][1,3]<stderr>:#015 38%|███▊      | 150/393 [00:18<00:30,  7.93ba/s][1,4]<stderr>:#015 36%|███▌      | 142/393 [00:18<00:31,  7.91ba/s][1,5]<stderr>:#015 36%|███▌      | 141/393 [00:18<00:31,  7.92ba/s][1,1]<stderr>:#015 36%|███▋      | 143/393 [00:18<00:31,  8.06ba/s][1,6]<stderr>:#015 37%|███▋      | 146/393 [00:18<00:27,  8.90ba/s][1,0]<stderr>:#015 36%|███▌      | 141/393 [00:18<00:29,  8.68ba/s][1,7]<stderr>:#015 37%|███▋      | 145/393 [00:18<00:36,  6.74ba/s][1,3]<stderr>:#015 38%|███▊      | 151/393 [00:18<00:30,  7.97ba/s][1,4]<stderr>:#015 36%|███▋      | 143/393 [00:18<00:31,  8.04ba/s][1,1]<stderr>:#015 37%|███▋      | 144/393 [00:18<00:29,  8.46ba/s][1,5]<stderr>:#015 36%|███▋      | 143/393 [00:18<00:28,  8.64ba/s][1,7]<stderr>:#015 37%|███▋      | 146/393 [00:18<00:33,  7.43ba/s][1,2]<stderr>:#015 37%|███▋      | 145/393 [00:18<00:38,  6.52ba/s][1,4]<stderr>:#015 37%|███▋      | 144/393 [00:18<00:29,  8.41ba/s][1,3]<stderr>:#015 39%|███▊      | 152/393 [00:18<00:29,  8.04ba/s][1,6]<stderr>:#015 37%|███▋      | 147/393 [00:18<00:34,  7.10ba/s][1,0]<stderr>:#015 36%|███▋      | 143/393 [00:18<00:26,  9.56ba/s][1,5]<stderr>:#015 37%|███▋      | 144/393 [00:18<00:28,  8.86ba/s][1,7]<stderr>:#015 37%|███▋      | 147/393 [00:18<00:31,  7.91ba/s][1,2]<stderr>:#015 37%|███▋      | 146/393 [00:18<00:34,  7.11ba/s][1,1]<stderr>:#015 37%|███▋      | 145/393 [00:18<00:36,  6.84ba/s][1,0]<stderr>:#015 37%|███▋      | 144/393 [00:18<00:25,  9.62ba/s][1,3]<stderr>:#015 39%|███▉      | 153/393 [00:19<00:29,  8.21ba/s][1,6]<stderr>:#015 38%|███▊      | 148/393 [00:18<00:32,  7.57ba/s][1,2]<stderr>:#015 37%|███▋      | 147/393 [00:18<00:32,  7.52ba/s][1,1]<stderr>:#015 37%|███▋      | 146/393 [00:18<00:33,  7.42ba/s][1,4]<stderr>:#015 37%|███▋      | 145/393 [00:18<00:37,  6.65ba/s][1,3]<stderr>:#015 39%|███▉      | 154/393 [00:19<00:28,  8.25ba/s][1,7]<stderr>:#015 38%|███▊      | 149/393 [00:18<00:27,  8.74ba/s][1,6]<stderr>:#015 38%|███▊      | 149/393 [00:18<00:32,  7.52ba/s][1,2]<stderr>:#015 38%|███▊      | 148/393 [00:18<00:30,  8.12ba/s][1,0]<stderr>:#015 37%|███▋      | 145/393 [00:19<00:33,  7.38ba/s][1,4]<stderr>:#015 37%|███▋      | 146/393 [00:18<00:34,  7.12ba/s][1,5]<stderr>:#015 37%|███▋      | 145/393 [00:18<00:39,  6.24ba/s][1,6]<stderr>:#015 38%|███▊      | 150/393 [00:18<00:30,  8.03ba/s][1,1]<stderr>:#015 38%|███▊      | 148/393 [00:19<00:30,  7.95ba/s][1,2]<stderr>:#015 38%|███▊      | 149/393 [00:19<00:31,  7.80ba/s][1,3]<stderr>:#015 40%|███▉      | 156/393 [00:19<00:27,  8.48ba/s][1,0]<stderr>:#015 37%|███▋      | 146/393 [00:19<00:33,  7.35ba/s][1,5]<stderr>:#015 37%|███▋      | 146/393 [00:18<00:37,  6.66ba/s][1,7]<stderr>:#015 38%|███▊      | 151/393 [00:19<00:27,  8.87ba/s][1,4]<stderr>:#015 37%|███▋      | 147/393 [00:18<00:33,  7.27ba/s][1,1]<stderr>:#015 38%|███▊      | 149/393 [00:19<00:30,  7.91ba/s][1,3]<stderr>:#015 40%|███▉      | 157/393 [00:19<00:28,  8.32ba/s][1,0]<stderr>:#015 37%|███▋      | 147/393 [00:19<00:32,  7.49ba/s][1,6]<stderr>:#015 39%|███▊      | 152/393 [00:19<00:29,  8.25ba/s][1,5]<stderr>:#015 37%|███▋      | 147/393 [00:18<00:35,  7.01ba/s][1,4]<stderr>:#015 38%|███▊      | 148/393 [00:18<00:33,  7.39ba/s][1,7]<stderr>:#015 39%|███▊      | 152/393 [00:19<00:30,  7.82ba/s][1,2]<stderr>:#015 38%|███▊      | 151/393 [00:19<00:28,  8.43ba/s][1,0]<stderr>:#015 38%|███▊      | 148/393 [00:19<00:32,  7.59ba/s][1,5]<stderr>:#015 38%|███▊      | 148/393 [00:19<00:34,  7.09ba/s][1,6]<stderr>:#015 39%|███▉      | 153/393 [00:19<00:30,  7.82ba/s][1,1]<stderr>:#015 38%|███▊      | 151/393 [00:19<00:28,  8.61ba/s][1,2]<stderr>:#015 39%|███▊      | 152/393 [00:19<00:27,  8.78ba/s][1,4]<stderr>:#015 38%|███▊      | 149/393 [00:19<00:33,  7.34ba/s][1,7]<stderr>:#015 39%|███▉      | 153/393 [00:19<00:29,  8.17ba/s][1,3]<stderr>:#015 40%|████      | 158/393 [00:19<00:30,  7.65ba/s][1,0]<stderr>:#015 38%|███▊      | 149/393 [00:19<00:32,  7.54ba/s][1,6]<stderr>:#015 39%|███▉      | 154/393 [00:19<00:30,  7.91ba/s][1,1]<stderr>:#015 39%|███▊      | 152/393 [00:19<00:28,  8.49ba/s][1,5]<stderr>:#015 38%|███▊      | 149/393 [00:19<00:33,  7.20ba/s][1,2]<stderr>:#015 39%|███▉      | 153/393 [00:19<00:28,  8.49ba/s][1,3]<stderr>:#015 40%|████      | 159/393 [00:19<00:30,  7.77ba/s][1,7]<stderr>:#015 39%|███▉      | 154/393 [00:19<00:29,  8.08ba/s][1,4]<stderr>:#015 38%|███▊      | 150/393 [00:19<00:32,  7.38ba/s][1,6]<stderr>:#015 39%|███▉      | 155/393 [00:19<00:30,  7.85ba/s][1,5]<stderr>:#015 38%|███▊      | 150/393 [00:19<00:32,  7.44ba/s][1,0]<stderr>:#015 38%|███▊      | 150/393 [00:19<00:32,  7.48ba/s][1,3]<stderr>:#015 41%|████      | 160/393 [00:19<00:29,  7.88ba/s][1,4]<stderr>:#015 38%|███▊      | 151/393 [00:19<00:31,  7.69ba/s][1,7]<stderr>:#015 39%|███▉      | 155/393 [00:19<00:29,  8.08ba/s][1,2]<stderr>:#015 39%|███▉      | 154/393 [00:19<00:29,  8.20ba/s][1,1]<stderr>:#015 39%|███▉      | 153/393 [00:19<00:29,  8.06ba/s][1,0]<stderr>:#015 38%|███▊      | 151/393 [00:19<00:31,  7.79ba/s][1,5]<stderr>:#015 38%|███▊      | 151/393 [00:19<00:31,  7.73ba/s][1,6]<stderr>:#015 40%|███▉      | 156/393 [00:19<00:29,  7.97ba/s][1,2]<stderr>:#015 39%|███▉      | 155/393 [00:19<00:28,  8.25ba/s][1,1]<stderr>:#015 39%|███▉      | 154/393 [00:19<00:29,  8.12ba/s][1,7]<stderr>:#015 40%|███▉      | 156/393 [00:19<00:29,  8.01ba/s][1,4]<stderr>:#015 39%|███▊      | 152/393 [00:19<00:31,  7.65ba/s][1,1]<stderr>:#015 39%|███▉      | 155/393 [00:19<00:27,  8.52ba/s][1,0]<stderr>:#015 39%|███▊      | 152/393 [00:19<00:30,  7.95ba/s][1,5]<stderr>:#015 39%|███▊      | 152/393 [00:19<00:30,  7.81ba/s][1,4]<stderr>:#015 39%|███▉      | 153/393 [00:19<00:30,  7.94ba/s][1,2]<stderr>:#015 40%|███▉      | 156/393 [00:19<00:28,  8.23ba/s][1,7]<stderr>:#015 40%|███▉      | 157/393 [00:19<00:29,  8.07ba/s][1,6]<stderr>:#015 40%|███▉      | 157/393 [00:19<00:30,  7.76ba/s][1,0]<stderr>:#015 39%|███▉      | 153/393 [00:20<00:29,  8.06ba/s][1,7]<stderr>:#015 40%|████      | 158/393 [00:20<00:27,  8.46ba/s][1,1]<stderr>:#015 40%|███▉      | 156/393 [00:19<00:28,  8.25ba/s][1,4]<stderr>:#015 39%|███▉      | 154/393 [00:19<00:29,  8.10ba/s][1,5]<stderr>:#015 39%|███▉      | 153/393 [00:19<00:30,  7.81ba/s][1,2]<stderr>:#015 40%|███▉      | 157/393 [00:20<00:28,  8.22ba/s][1,3]<stderr>:#015 41%|████      | 161/393 [00:20<00:46,  4.98ba/s][1,4]<stderr>:#015 39%|███▉      | 155/393 [00:19<00:28,  8.25ba/s][1,5]<stderr>:#015 39%|███▉      | 154/393 [00:19<00:29,  8.10ba/s][1,7]<stderr>:#015 40%|████      | 159/393 [00:20<00:28,  8.29ba/s][1,3]<stderr>:#015 41%|████      | 162/393 [00:20<00:40,  5.76ba/s][1,6]<stderr>:#015 40%|████      | 159/393 [00:20<00:29,  8.03ba/s][1,1]<stderr>:#015 40%|███▉      | 157/393 [00:20<00:28,  8.24ba/s][1,0]<stderr>:#015 39%|███▉      | 154/393 [00:20<00:30,  7.86ba/s][1,2]<stderr>:#015 40%|████      | 158/393 [00:20<00:30,  7.82ba/s][1,7]<stderr>:#015 41%|████      | 160/393 [00:20<00:28,  8.26ba/s][1,5]<stderr>:#015 39%|███▉      | 155/393 [00:19<00:29,  8.07ba/s][1,3]<stderr>:#015 41%|████▏     | 163/393 [00:20<00:37,  6.19ba/s][1,2]<stderr>:#015 40%|████      | 159/393 [00:20<00:28,  8.31ba/s][1,4]<stderr>:#015 40%|███▉      | 156/393 [00:19<00:29,  7.94ba/s][1,1]<stderr>:#015 40%|████      | 158/393 [00:20<00:29,  7.98ba/s][1,0]<stderr>:#015 39%|███▉      | 155/393 [00:20<00:30,  7.81ba/s][1,6]<stderr>:#015 41%|████      | 160/393 [00:20<00:29,  7.83ba/s][1,3]<stderr>:#015 42%|████▏     | 164/393 [00:20<00:33,  6.82ba/s][1,5]<stderr>:#015 40%|███▉      | 156/393 [00:20<00:29,  8.10ba/s][1,4]<stderr>:#015 40%|███▉      | 157/393 [00:20<00:28,  8.16ba/s][1,0]<stderr>:#015 40%|███▉      | 156/393 [00:20<00:29,  8.05ba/s][1,1]<stderr>:#015 40%|████      | 159/393 [00:20<00:28,  8.14ba/s][1,2]<stderr>:#015 41%|████      | 160/393 [00:20<00:28,  8.25ba/s][1,4]<stderr>:#015 40%|████      | 158/393 [00:20<00:27,  8.56ba/s][1,0]<stderr>:#015 40%|███▉      | 157/393 [00:20<00:27,  8.48ba/s][1,3]<stderr>:#015 42%|████▏     | 165/393 [00:20<00:30,  7.36ba/s][1,5]<stderr>:#015 40%|███▉      | 157/393 [00:20<00:28,  8.39ba/s][1,1]<stderr>:#015 41%|████      | 160/393 [00:20<00:27,  8.54ba/s][1,6]<stderr>:#015 41%|████      | 161/393 [00:20<00:39,  5.86ba/s][1,0]<stderr>:#015 40%|████      | 158/393 [00:20<00:26,  8.88ba/s][1,4]<stderr>:#015 40%|████      | 159/393 [00:20<00:26,  8.86ba/s][1,5]<stderr>:#015 40%|████      | 158/393 [00:20<00:27,  8.69ba/s][1,7]<stderr>:#015 41%|████      | 161/393 [00:20<00:48,  4.77ba/s][1,3]<stderr>:#015 42%|████▏     | 167/393 [00:20<00:28,  8.06ba/s][1,4]<stderr>:#015 41%|████      | 160/393 [00:20<00:25,  9.15ba/s][1,6]<stderr>:#015 41%|████▏     | 163/393 [00:20<00:33,  6.78ba/s][1,2]<stderr>:#015 41%|████      | 161/393 [00:20<00:45,  5.09ba/s][1,0]<stderr>:#015 41%|████      | 160/393 [00:20<00:25,  9.13ba/s][1,3]<stderr>:#015 43%|████▎     | 168/393 [00:21<00:27,  8.29ba/s][1,5]<stderr>:#015 41%|████      | 160/393 [00:20<00:25,  8.98ba/s][1,7]<stderr>:#015 41%|████      | 162/393 [00:20<00:43,  5.33ba/s][1,1]<stderr>:#015 41%|████      | 161/393 [00:20<00:44,  5.16ba/s][1,3]<stderr>:#015 43%|████▎     | 169/393 [00:21<00:25,  8.65ba/s][1,6]<stderr>:#015 42%|████▏     | 164/393 [00:20<00:35,  6.39ba/s][1,7]<stderr>:#015 41%|████▏     | 163/393 [00:20<00:38,  5.98ba/s][1,2]<stderr>:#015 41%|████▏     | 163/393 [00:20<00:37,  6.10ba/s][1,4]<stderr>:#015 41%|████      | 161/393 [00:20<00:39,  5.88ba/s][1,6]<stderr>:#015 42%|████▏     | 165/393 [00:20<00:32,  7.07ba/s][1,3]<stderr>:#015 43%|████▎     | 170/393 [00:21<00:25,  8.66ba/s][1,1]<stderr>:#015 41%|████▏     | 163/393 [00:20<00:37,  6.20ba/s][1,7]<stderr>:#015 42%|████▏     | 164/393 [00:21<00:35,  6.46ba/s][1,0]<stderr>:#015 41%|████      | 161/393 [00:21<00:39,  5.86ba/s][1,4]<stderr>:#015 41%|████      | 162/393 [00:20<00:34,  6.65ba/s][1,6]<stderr>:#015 42%|████▏     | 166/393 [00:21<00:29,  7.70ba/s][1,5]<stderr>:#015 41%|████      | 161/393 [00:20<00:41,  5.58ba/s][1,2]<stderr>:#015 42%|████▏     | 165/393 [00:21<00:33,  6.75ba/s][1,7]<stderr>:#015 42%|████▏     | 165/393 [00:21<00:32,  7.00ba/s][1,1]<stderr>:#015 42%|████▏     | 164/393 [00:21<00:35,  6.48ba/s][1,4]<stderr>:#015 41%|████▏     | 163/393 [00:20<00:31,  7.29ba/s][1,6]<stderr>:#015 42%|████▏     | 167/393 [00:21<00:27,  8.13ba/s][1,0]<stderr>:#015 41%|████      | 162/393 [00:21<00:37,  6.20ba/s][1,5]<stderr>:#015 41%|████      | 162/393 [00:20<00:36,  6.32ba/s][1,3]<stderr>:#015 44%|████▍     | 172/393 [00:21<00:26,  8.47ba/s][1,2]<stderr>:#015 42%|████▏     | 166/393 [00:21<00:30,  7.43ba/s][1,7]<stderr>:#015 42%|████▏     | 166/393 [00:21<00:30,  7.56ba/s][1,1]<stderr>:#015 42%|████▏     | 165/393 [00:21<00:32,  6.94ba/s][1,6]<stderr>:#015 43%|████▎     | 168/393 [00:21<00:28,  7.87ba/s][1,5]<stderr>:#015 41%|████▏     | 163/393 [00:21<00:35,  6.53ba/s][1,3]<stderr>:#015 44%|████▍     | 173/393 [00:21<00:26,  8.27ba/s][1,0]<stderr>:#015 41%|████▏     | 163/393 [00:21<00:36,  6.29ba/s][1,7]<stderr>:#015 42%|████▏     | 167/393 [00:21<00:29,  7.69ba/s][1,1]<stderr>:#015 42%|████▏     | 166/393 [00:21<00:30,  7.49ba/s][1,4]<stderr>:#015 42%|████▏     | 165/393 [00:21<00:28,  8.03ba/s][1,2]<stderr>:#015 42%|████▏     | 167/393 [00:21<00:30,  7.29ba/s][1,6]<stderr>:#015 43%|████▎     | 169/393 [00:21<00:28,  7.83ba/s][1,5]<stderr>:#015 42%|████▏     | 164/393 [00:21<00:32,  6.95ba/s][1,4]<stderr>:#015 42%|████▏     | 166/393 [00:21<00:27,  8.38ba/s][1,7]<stderr>:#015 43%|████▎     | 168/393 [00:21<00:28,  7.91ba/s][1,2]<stderr>:#015 43%|████▎     | 168/393 [00:21<00:29,  7.71ba/s][1,0]<stderr>:#015 42%|████▏     | 164/393 [00:21<00:33,  6.77ba/s][1,3]<stderr>:#015 44%|████▍     | 174/393 [00:21<00:27,  8.01ba/s][1,1]<stderr>:#015 42%|████▏     | 167/393 [00:21<00:29,  7.65ba/s][1,1]<stderr>:#015 43%|████▎     | 168/393 [00:21<00:27,  8.17ba/s][1,4]<stderr>:#015 42%|████▏     | 167/393 [00:21<00:28,  8.03ba/s][1,2]<stderr>:#015 43%|████▎     | 169/393 [00:21<00:29,  7.60ba/s][1,0]<stderr>:#015 42%|████▏     | 165/393 [00:21<00:33,  6.89ba/s][1,3]<stderr>:#015 45%|████▍     | 175/393 [00:21<00:28,  7.75ba/s][1,5]<stderr>:#015 42%|████▏     | 165/393 [00:21<00:33,  6.73ba/s][1,6]<stderr>:#015 44%|████▎     | 171/393 [00:21<00:25,  8.57ba/s][1,7]<stderr>:#015 43%|████▎     | 169/393 [00:21<00:30,  7.39ba/s][1,1]<stderr>:#015 43%|████▎     | 169/393 [00:21<00:27,  8.28ba/s][1,2]<stderr>:#015 43%|████▎     | 170/393 [00:21<00:28,  7.91ba/s][1,0]<stderr>:#015 42%|████▏     | 166/393 [00:21<00:31,  7.14ba/s][1,4]<stderr>:#015 43%|████▎     | 168/393 [00:21<00:29,  7.71ba/s][1,5]<stderr>:#015 42%|████▏     | 166/393 [00:21<00:31,  7.13ba/s][1,6]<stderr>:#015 44%|████▍     | 172/393 [00:21<00:26,  8.43ba/s][1,7]<stderr>:#015 43%|████▎     | 170/393 [00:21<00:29,  7.60ba/s][1,3]<stderr>:#015 45%|████▍     | 176/393 [00:22<00:29,  7.35ba/s][1,1]<stderr>:#015 43%|████▎     | 170/393 [00:21<00:25,  8.60ba/s][1,2]<stderr>:#015 44%|████▎     | 171/393 [00:21<00:28,  7.76ba/s][1,5]<stderr>:#015 42%|████▏     | 167\u001b[0m\n",
      "\u001b[34m/393 [00:21<00:30,  7.51ba/s][1,6]<stderr>:#015 44%|████▍     | 173/393 [00:21<00:25,  8.52ba/s][1,4]<stderr>:#015 43%|████▎     | 169/393 [00:21<00:29,  7.67ba/s][1,3]<stderr>:#015 45%|████▌     | 177/393 [00:22<00:27,  7.76ba/s][1,7]<stderr>:#015 44%|████▎     | 171/393 [00:21<00:29,  7.49ba/s][1,0]<stderr>:#015 42%|████▏     | 167/393 [00:21<00:32,  6.89ba/s][1,1]<stderr>:#015 44%|████▎     | 171/393 [00:21<00:26,  8.43ba/s][1,2]<stderr>:#015 44%|████▍     | 172/393 [00:22<00:27,  7.94ba/s][1,5]<stderr>:#015 43%|████▎     | 168/393 [00:21<00:29,  7.70ba/s][1,6]<stderr>:#015 44%|████▍     | 174/393 [00:22<00:27,  7.99ba/s][1,0]<stderr>:#015 43%|████▎     | 168/393 [00:22<00:30,  7.49ba/s][1,7]<stderr>:#015 44%|████▍     | 172/393 [00:22<00:28,  7.72ba/s][1,4]<stderr>:#015 43%|████▎     | 170/393 [00:21<00:29,  7.58ba/s][1,6]<stderr>:#015 45%|████▍     | 175/393 [00:22<00:26,  8.18ba/s][1,0]<stderr>:#015 43%|████▎     | 169/393 [00:22<00:29,  7.55ba/s][1,4]<stderr>:#015 44%|████▎     | 171/393 [00:21<00:29,  7.60ba/s][1,3]<stderr>:#015 45%|████▌     | 178/393 [00:22<00:36,  5.92ba/s][1,7]<stderr>:#015 44%|████▍     | 173/393 [00:22<00:28,  7.61ba/s][1,5]<stderr>:#015 43%|████▎     | 170/393 [00:21<00:25,  8.68ba/s][1,2]<stderr>:#015 44%|████▍     | 174/393 [00:22<00:25,  8.75ba/s][1,1]<stderr>:#015 44%|████▍     | 173/393 [00:22<00:25,  8.60ba/s][1,6]<stderr>:#015 45%|████▍     | 176/393 [00:22<00:27,  7.98ba/s][1,5]<stderr>:#015 44%|████▎     | 171/393 [00:21<00:25,  8.56ba/s][1,4]<stderr>:#015 44%|████▍     | 172/393 [00:22<00:28,  7.62ba/s][1,3]<stderr>:#015 46%|████▌     | 179/393 [00:22<00:33,  6.37ba/s][1,2]<stderr>:#015 45%|████▍     | 175/393 [00:22<00:25,  8.56ba/s][1,7]<stderr>:#015 44%|████▍     | 174/393 [00:22<00:28,  7.65ba/s][1,0]<stderr>:#015 43%|████▎     | 170/393 [00:22<00:30,  7.41ba/s][1,1]<stderr>:#015 44%|████▍     | 174/393 [00:22<00:26,  8.41ba/s][1,6]<stderr>:#015 45%|████▌     | 177/393 [00:22<00:27,  7.90ba/s][1,0]<stderr>:#015 44%|████▎     | 171/393 [00:22<00:28,  7.90ba/s][1,3]<stderr>:#015 46%|████▌     | 180/393 [00:22<00:30,  6.93ba/s][1,2]<stderr>:#015 45%|████▍     | 176/393 [00:22<00:26,  8.34ba/s][1,7]<stderr>:#015 45%|████▍     | 175/393 [00:22<00:28,  7.72ba/s][1,4]<stderr>:#015 44%|████▍     | 173/393 [00:22<00:28,  7.65ba/s][1,5]<stderr>:#015 44%|████▍     | 172/393 [00:22<00:27,  8.04ba/s][1,1]<stderr>:#015 45%|████▍     | 175/393 [00:22<00:27,  7.96ba/s][1,2]<stderr>:#015 45%|████▌     | 177/393 [00:22<00:25,  8.51ba/s][1,7]<stderr>:#015 45%|████▍     | 176/393 [00:22<00:27,  7.96ba/s][1,6]<stderr>:#015 45%|████▌     | 178/393 [00:22<00:28,  7.64ba/s][1,3]<stderr>:#015 46%|████▌     | 181/393 [00:22<00:30,  7.05ba/s][1,5]<stderr>:#015 44%|████▍     | 173/393 [00:22<00:26,  8.19ba/s][1,1]<stderr>:#015 45%|████▍     | 176/393 [00:22<00:26,  8.32ba/s][1,4]<stderr>:#015 44%|████▍     | 174/393 [00:22<00:28,  7.76ba/s][1,0]<stderr>:#015 44%|████▍     | 172/393 [00:22<00:29,  7.52ba/s][1,6]<stderr>:#015 46%|████▌     | 179/393 [00:22<00:27,  7.66ba/s][1,7]<stderr>:#015 45%|████▌     | 177/393 [00:22<00:27,  7.77ba/s][1,5]<stderr>:#015 44%|████▍     | 174/393 [00:22<00:27,  8.05ba/s][1,3]<stderr>:#015 46%|████▋     | 182/393 [00:22<00:29,  7.21ba/s][1,4]<stderr>:#015 45%|████▍     | 175/393 [00:22<00:28,  7.73ba/s][1,1]<stderr>:#015 45%|████▌     | 177/393 [00:22<00:26,  8.04ba/s][1,0]<stderr>:#015 44%|████▍     | 173/393 [00:22<00:29,  7.55ba/s][1,5]<stderr>:#015 45%|████▍     | 175/393 [00:22<00:26,  8.11ba/s][1,3]<stderr>:#015 47%|████▋     | 183/393 [00:23<00:27,  7.51ba/s][1,4]<stderr>:#015 45%|████▍     | 176/393 [00:22<00:27,  7.93ba/s][1,0]<stderr>:#015 44%|████▍     | 174/393 [00:22<00:28,  7.79ba/s][1,2]<stderr>:#015 45%|████▌     | 178/393 [00:22<00:37,  5.66ba/s][1,1]<stderr>:#015 45%|████▌     | 178/393 [00:22<00:31,  6.80ba/s][1,7]<stderr>:#015 45%|████▌     | 178/393 [00:22<00:34,  6.32ba/s][1,6]<stderr>:#015 46%|████▌     | 180/393 [00:22<00:34,  6.21ba/s][1,3]<stderr>:#015 47%|████▋     | 184/393 [00:23<00:27,  7.66ba/s][1,5]<stderr>:#015 45%|████▌     | 177/393 [00:22<00:25,  8.45ba/s][1,2]<stderr>:#015 46%|████▌     | 180/393 [00:23<00:31,  6.76ba/s][1,7]<stderr>:#015 46%|████▌     | 179/393 [00:23<00:30,  6.91ba/s][1,1]<stderr>:#015 46%|████▌     | 179/393 [00:23<00:31,  6.84ba/s][1,6]<stderr>:#015 46%|████▌     | 181/393 [00:23<00:31,  6.71ba/s][1,0]<stderr>:#015 45%|████▍     | 176/393 [00:23<00:27,  7.83ba/s][1,2]<stderr>:#015 46%|████▌     | 181/393 [00:23<00:29,  7.21ba/s][1,4]<stderr>:#015 45%|████▌     | 178/393 [00:22<00:29,  7.24ba/s][1,7]<stderr>:#015 46%|████▌     | 180/393 [00:23<00:29,  7.31ba/s][1,3]<stderr>:#015 47%|████▋     | 186/393 [00:23<00:25,  8.09ba/s][1,1]<stderr>:#015 46%|████▌     | 180/393 [00:23<00:29,  7.16ba/s][1,6]<stderr>:#015 46%|████▋     | 182/393 [00:23<00:29,  7.09ba/s][1,5]<stderr>:#015 45%|████▌     | 178/393 [00:22<00:31,  6.79ba/s][1,2]<stderr>:#015 46%|████▋     | 182/393 [00:23<00:27,  7.63ba/s][1,4]<stderr>:#015 46%|████▌     | 179/393 [00:22<00:28,  7.46ba/s][1,7]<stderr>:#015 46%|████▌     | 181/393 [00:23<00:28,  7.53ba/s][1,3]<stderr>:#015 48%|████▊     | 187/393 [00:23<00:25,  8.00ba/s][1,6]<stderr>:#015 47%|████▋     | 183/393 [00:23<00:28,  7.35ba/s][1,1]<stderr>:#015 46%|████▌     | 181/393 [00:23<00:29,  7.27ba/s][1,2]<stderr>:#015 47%|████▋     | 183/393 [00:23<00:26,  7.96ba/s][1,7]<stderr>:#015 46%|████▋     | 182/393 [00:23<00:27,  7.67ba/s][1,3]<stderr>:#015 48%|████▊     | 188/393 [00:23<00:25,  8.08ba/s][1,4]<stderr>:#015 46%|████▌     | 180/393 [00:23<00:28,  7.48ba/s][1,5]<stderr>:#015 46%|████▌     | 180/393 [00:23<00:27,  7.83ba/s][1,1]<stderr>:#015 46%|████▋     | 182/393 [00:23<00:27,  7.79ba/s][1,0]<stderr>:#015 45%|████▌     | 178/393 [00:23<00:31,  6.74ba/s][1,6]<stderr>:#015 47%|████▋     | 185/393 [00:23<00:26,  7.78ba/s][1,3]<stderr>:#015 48%|████▊     | 189/393 [00:23<00:24,  8.22ba/s][1,5]<stderr>:#015 46%|████▌     | 181/393 [00:23<00:26,  8.09ba/s][1,7]<stderr>:#015 47%|████▋     | 183/393 [00:23<00:26,  7.79ba/s][1,4]<stderr>:#015 46%|████▌     | 181/393 [00:23<00:27,  7.60ba/s][1,1]<stderr>:#015 47%|████▋     | 183/393 [00:23<00:27,  7.72ba/s][1,2]<stderr>:#015 47%|████▋     | 185/393 [00:23<00:23,  8.79ba/s][1,6]<stderr>:#015 47%|████▋     | 186/393 [00:23<00:26,  7.88ba/s][1,7]<stderr>:#015 47%|████▋     | 184/393 [00:23<00:26,  7.97ba/s][1,4]<stderr>:#015 46%|████▋     | 182/393 [00:23<00:26,  7.96ba/s][1,5]<stderr>:#015 46%|████▋     | 182/393 [00:23<00:26,  8.03ba/s][1,1]<stderr>:#015 47%|████▋     | 184/393 [00:23<00:25,  8.10ba/s][1,0]<stderr>:#015 46%|████▌     | 180/393 [00:23<00:28,  7.57ba/s][1,3]<stderr>:#015 48%|████▊     | 190/393 [00:23<00:25,  7.81ba/s][1,2]<stderr>:#015 47%|████▋     | 186/393 [00:23<00:27,  7.43ba/s][1,6]<stderr>:#015 48%|████▊     | 187/393 [00:23<00:25,  7.94ba/s][1,0]<stderr>:#015 46%|████▌     | 181/393 [00:23<00:26,  7.89ba/s][1,4]<stderr>:#015 47%|████▋     | 183/393 [00:23<00:26,  7.96ba/s][1,7]<stderr>:#015 47%|████▋     | 185/393 [00:23<00:26,  7.89ba/s][1,3]<stderr>:#015 49%|████▊     | 191/393 [00:24<00:24,  8.15ba/s][1,1]<stderr>:#015 47%|████▋     | 185/393 [00:23<00:25,  8.05ba/s][1,5]<stderr>:#015 47%|████▋     | 183/393 [00:23<00:26,  7.85ba/s][1,3]<stderr>:#015 49%|████▉     | 192/393 [00:24<00:24,  8.33ba/s][1,7]<stderr>:#015 47%|████▋     | 186/393 [00:23<00:25,  8.10ba/s][1,6]<stderr>:#015 48%|████▊     | 188/393 [00:23<00:26,  7.87ba/s][1,5]<stderr>:#015 47%|████▋     | 184/393 [00:23<00:25,  8.18ba/s][1,4]<stderr>:#015 47%|████▋     | 184/393 [00:23<00:26,  8.01ba/s][1,1]<stderr>:#015 47%|████▋     | 186/393 [00:23<00:25,  8.18ba/s][1,0]<stderr>:#015 46%|████▋     | 182/393 [00:23<00:26,  7.93ba/s][1,2]<stderr>:#015 48%|████▊     | 188/393 [00:23<00:24,  8.37ba/s][1,7]<stderr>:#015 48%|████▊     | 187/393 [00:24<00:25,  8.24ba/s][1,3]<stderr>:#015 49%|████▉     | 193/393 [00:24<00:23,  8.35ba/s][1,6]<stderr>:#015 48%|████▊     | 189/393 [00:23<00:25,  8.09ba/s][1,0]<stderr>:#015 47%|████▋     | 183/393 [00:24<00:25,  8.15ba/s][1,5]<stderr>:#015 47%|████▋     | 185/393 [00:23<00:25,  8.26ba/s][1,4]<stderr>:#015 47%|████▋     | 185/393 [00:23<00:26,  7.93ba/s][1,1]<stderr>:#015 48%|████▊     | 187/393 [00:23<00:25,  8.05ba/s][1,2]<stderr>:#015 48%|████▊     | 189/393 [00:24<00:25,  8.15ba/s][1,7]<stderr>:#015 48%|████▊     | 188/393 [00:24<00:25,  8.16ba/s][1,5]<stderr>:#015 47%|████▋     | 186/393 [00:23<00:25,  8.07ba/s][1,1]<stderr>:#015 48%|████▊     | 188/393 [00:24<00:25,  8.13ba/s][1,6]<stderr>:#015 48%|████▊     | 190/393 [00:24<00:25,  7.86ba/s][1,2]<stderr>:#015 48%|████▊     | 190/393 [00:24<00:24,  8.28ba/s][1,4]<stderr>:#015 47%|████▋     | 186/393 [00:23<00:25,  8.01ba/s][1,0]<stderr>:#015 47%|████▋     | 184/393 [00:24<00:28,  7.38ba/s][1,1]<stderr>:#015 48%|████▊     | 189/393 [00:24<00:24,  8.46ba/s][1,6]<stderr>:#015 49%|████▊     | 191/393 [00:24<00:24,  8.17ba/s][1,7]<stderr>:#015 48%|████▊     | 189/393 [00:24<00:25,  8.14ba/s][1,5]<stderr>:#015 48%|████▊     | 187/393 [00:23<00:24,  8.28ba/s][1,4]<stderr>:#015 48%|████▊     | 187/393 [00:23<00:25,  8.23ba/s][1,2]<stderr>:#015 49%|████▊     | 191/393 [00:24<00:24,  8.30ba/s][1,3]<stderr>:#015 49%|████▉     | 194/393 [00:24<00:34,  5.70ba/s][1,7]<stderr>:#015 48%|████▊     | 190/393 [00:24<00:23,  8.48ba/s][1,0]<stderr>:#015 47%|████▋     | 186/393 [00:24<00:25,  8.10ba/s][1,5]<stderr>:#015 48%|████▊     | 188/393 [00:23<00:24,  8.49ba/s][1,1]<stderr>:#015 48%|████▊     | 190/393 [00:24<00:24,  8.45ba/s][1,6]<stderr>:#015 49%|████▉     | 192/393 [00:24<00:24,  8.33ba/s][1,4]<stderr>:#015 48%|████▊     | 188/393 [00:24<00:26,  7.65ba/s][1,2]<stderr>:#015 49%|████▉     | 192/393 [00:24<00:26,  7.66ba/s][1,3]<stderr>:#015 50%|████▉     | 195/393 [00:24<00:31,  6.34ba/s][1,0]<stderr>:#015 48%|████▊     | 187/393 [00:24<00:23,  8.58ba/s][1,5]<stderr>:#015 48%|████▊     | 189/393 [00:24<00:23,  8.80ba/s][1,1]<stderr>:#015 49%|████▊     | 191/393 [00:24<00:23,  8.73ba/s][1,1]<stderr>:#015 49%|████▉     | 192/393 [00:24<00:22,  8.78ba/s][1,5]<stderr>:#015 48%|████▊     | 190/393 [00:24<00:23,  8.72ba/s][1,3]<stderr>:#015 50%|█████     | 197/393 [00:24<00:26,  7.43ba/s][1,6]<stderr>:#015 49%|████▉     | 194/393 [00:24<00:23,  8.52ba/s][1,0]<stderr>:#015 48%|████▊     | 188/393 [00:24<00:24,  8.29ba/s][1,7]<stderr>:#015 49%|████▉     | 192/393 [00:24<00:23,  8.42ba/s][1,4]<stderr>:#015 48%|████▊     | 190/393 [00:24<00:25,  7.87ba/s][1,5]<stderr>:#015 49%|████▊     | 191/393 [00:24<00:22,  8.81ba/s][1,1]<stderr>:#015 49%|████▉     | 193/393 [00:24<00:22,  8.77ba/s][1,0]<stderr>:#015 48%|████▊     | 189/393 [00:24<00:23,  8.57ba/s][1,3]<stderr>:#015 50%|█████     | 198/393 [00:24<00:25,  7.64ba/s][1,2]<stderr>:#015 49%|████▉     | 194/393 [00:24<00:28,  7.07ba/s][1,6]<stderr>:#015 50%|████▉     | 195/393 [00:24<00:25,  7.64ba/s][1,5]<stderr>:#015 49%|████▉     | 192/393 [00:24<00:23,  8.64ba/s][1,3]<stderr>:#015 51%|█████     | 199/393 [00:25<00:24,  7.92ba/s][1,0]<stderr>:#015 48%|████▊     | 190/393 [00:24<00:24,  8.35ba/s][1,4]<stderr>:#015 49%|████▉     | 192/393 [00:24<00:23,  8.56ba/s][1,3]<stderr>:#015 51%|█████     | 200/393 [00:25<00:22,  8.43ba/s][1,4]<stderr>:#015 49%|████▉     | 193/393 [00:24<00:22,  8.88ba/s][1,0]<stderr>:#015 49%|████▊     | 191/393 [00:24<00:23,  8.65ba/s][1,2]<stderr>:#015 50%|████▉     | 196/393 [00:24<00:25,  7.87ba/s][1,7]<stderr>:#015 49%|████▉     | 194/393 [00:24<00:27,  7.30ba/s][1,1]<stderr>:#015 49%|████▉     | 194/393 [00:24<00:31,  6.23ba/s][1,5]<stderr>:#015 49%|████▉     | 193/393 [00:24<00:26,  7.60ba/s][1,6]<stderr>:#015 50%|█████     | 197/393 [00:24<00:26,  7.53ba/s][1,0]<stderr>:#015 49%|████▉     | 192/393 [00:25<00:23,  8.40ba/s][1,3]<stderr>:#015 51%|█████     | 201/393 [00:25<00:23,  8.09ba/s][1,2]<stderr>:#015 50%|█████     | 197/393 [00:25<00:25,  7.83ba/s][1,1]<stderr>:#015 50%|████▉     | 195/393 [00:25<00:28,  6.98ba/s][1,7]<stderr>:#015 50%|████▉     | 195/393 [00:25<00:25,  7.70ba/s][1,4]<stderr>:#015 49%|████▉     | 194/393 [00:24<00:26,  7.49ba/s][1,1]<stderr>:#015 50%|████▉     | 196/393 [00:25<00:25,  7.59ba/s][1,0]<stderr>:#015 49%|████▉     | 193/393 [00:25<00:24,  8.29ba/s][1,6]<stderr>:#015 51%|█████     | 199/393 [00:25<00:22,  8.46ba/s][1,2]<stderr>:#015 50%|█████     | 198/393 [00:25<00:25,  7.79ba/s][1,3]<stderr>:#015 51%|█████▏    | 202/393 [00:25<00:24,  7.89ba/s][1,7]<stderr>:#015 50%|████▉     | 196/393 [00:25<00:25,  7.79ba/s][1,5]<stderr>:#015 49%|████▉     | 194/393 [00:24<00:36,  5.51ba/s][1,1]<stderr>:#015 50%|█████     | 197/393 [00:25<00:24,  7.89ba/s][1,6]<stderr>:#015 51%|█████     | 200/393 [00:25<00:22,  8.67ba/s][1,4]<stderr>:#015 50%|████▉     | 196/393 [00:25<00:24,  8.13ba/s][1,3]<stderr>:#015 52%|█████▏    | 203/393 [00:25<00:23,  7.93ba/s][1,7]<stderr>:#015 50%|█████     | 197/393 [00:25<00:24,  7.89ba/s][1,2]<stderr>:#015 51%|█████     | 199/393 [00:25<00:24,  7.78ba/s][1,5]<stderr>:#015 50%|████▉     | 195/393 [00:25<00:31,  6.22ba/s][1,7]<stderr>:#015 50%|█████     | 198/393 [00:25<00:23,  8.32ba/s][1,2]<stderr>:#015 51%|█████     | 200/393 [00:25<00:23,  8.28ba/s][1,4]<stderr>:#015 50%|█████     | 197/393 [00:25<00:23,  8.32ba/s][1,6]<stderr>:#015 51%|█████     | 201/393 [00:25<00:23,  8.25ba/s][1,0]<stderr>:#015 49%|████▉     | 194/393 [00:25<00:32,  6.20ba/s][1,1]<stderr>:#015 51%|█████     | 199/393 [00:25<00:21,  8.84ba/s][1,7]<stderr>:#015 51%|█████     | 199/393 [00:25<00:22,  8.75ba/s][1,2]<stderr>:#015 51%|█████     | 201/393 [00:25<00:22,  8.49ba/s][1,6]<stderr>:#015 51%|█████▏    | 202/393 [00:25<00:22,  8.67ba/s][1,4]<stderr>:#015 50%|█████     | 198/393 [00:25<00:23,  8.40ba/s][1,3]<stderr>:#015 52%|█████▏    | 205/393 [00:25<00:22,  8.21ba/s][1,0]<stderr>:#015 50%|████▉     | 195/393 [00:25<00:28,  6.95ba/s][1,5]<stderr>:#015 50%|█████     | 197/393 [00:25<00:27,  7.13ba/s][1,2]<stderr>:#015 51%|█████▏    | 202/393 [00:25<00:21,  8.84ba/s][1,0]<stderr>:#015 50%|████▉     | 196/393 [00:25<00:26,  7.58ba/s][1,1]<stderr>:#015 51%|█████     | 201/393 [00:25<00:20,  9.20ba/s][1,7]<stderr>:#015 51%|█████     | 200/393 [00:25<00:22,  8.45ba/s][1,4]<stderr>:#015 51%|█████     | 199/393 [00:25<00:22,  8.49ba/s][1,6]<stderr>:#015 52%|█████▏    | 203/393 [00:25<00:23,  8.00ba/s][1,3]<stderr>:#015 52%|█████▏    | 206/393 [00:25<00:25,  7.39ba/s][1,1]<stderr>:#015 51%|█████▏    | 202/393 [00:25<00:20,  9.39ba/s][1,0]<stderr>:#015 50%|█████     | 197/393 [00:25<00:24,  7.97ba/s][1,2]<stderr>:#015 52%|█████▏    | 203/393 [00:25<00:21,  8.67ba/s][1,7]<stderr>:#015 51%|█████     | 201/393 [00:25<00:23,  8.11ba/s][1,4]<stderr>:#015 51%|█████     | 200/393 [00:25<00:23,  8.09ba/s][1,6]<stderr>:#015 52%|█████▏    | 204/393 [00:25<00:23,  8.12ba/s][1,5]<stderr>:#015 51%|█████     | 199/393 [00:2\u001b[0m\n",
      "\u001b[34m5<00:26,  7.30ba/s][1,3]<stderr>:#015 53%|█████▎    | 207/393 [00:26<00:24,  7.46ba/s][1,1]<stderr>:#015 52%|█████▏    | 203/393 [00:25<00:20,  9.27ba/s][1,0]<stderr>:#015 50%|█████     | 198/393 [00:25<00:23,  8.34ba/s][1,2]<stderr>:#015 52%|█████▏    | 204/393 [00:25<00:22,  8.26ba/s][1,4]<stderr>:#015 51%|█████     | 201/393 [00:25<00:22,  8.57ba/s][1,7]<stderr>:#015 51%|█████▏    | 202/393 [00:25<00:24,  7.88ba/s][1,5]<stderr>:#015 51%|█████     | 200/393 [00:25<00:24,  7.91ba/s][1,0]<stderr>:#015 51%|█████     | 199/393 [00:25<00:22,  8.55ba/s][1,1]<stderr>:#015 52%|█████▏    | 204/393 [00:25<00:22,  8.53ba/s][1,2]<stderr>:#015 52%|█████▏    | 205/393 [00:26<00:23,  8.13ba/s][1,7]<stderr>:#015 52%|█████▏    | 203/393 [00:26<00:23,  8.23ba/s][1,4]<stderr>:#015 51%|█████▏    | 202/393 [00:25<00:23,  8.12ba/s][1,3]<stderr>:#015 53%|█████▎    | 209/393 [00:26<00:22,  8.05ba/s][1,6]<stderr>:#015 52%|█████▏    | 206/393 [00:26<00:23,  8.10ba/s][1,2]<stderr>:#015 52%|█████▏    | 206/393 [00:26<00:23,  7.82ba/s][1,3]<stderr>:#015 53%|█████▎    | 210/393 [00:26<00:22,  8.05ba/s][1,1]<stderr>:#015 52%|█████▏    | 206/393 [00:26<00:20,  9.31ba/s][1,5]<stderr>:#015 51%|█████▏    | 202/393 [00:25<00:23,  8.11ba/s][1,7]<stderr>:#015 52%|█████▏    | 204/393 [00:26<00:24,  7.78ba/s][1,4]<stderr>:#015 52%|█████▏    | 203/393 [00:25<00:24,  7.72ba/s][1,0]<stderr>:#015 51%|█████     | 201/393 [00:26<00:21,  8.87ba/s][1,6]<stderr>:#015 53%|█████▎    | 207/393 [00:26<00:25,  7.36ba/s][1,7]<stderr>:#015 52%|█████▏    | 205/393 [00:26<00:22,  8.19ba/s][1,2]<stderr>:#015 53%|█████▎    | 207/393 [00:26<00:23,  7.77ba/s][1,0]<stderr>:#015 51%|█████▏    | 202/393 [00:26<00:22,  8.63ba/s][1,1]<stderr>:#015 53%|█████▎    | 207/393 [00:26<00:21,  8.46ba/s][1,5]<stderr>:#015 52%|█████▏    | 203/393 [00:25<00:24,  7.73ba/s][1,4]<stderr>:#015 52%|█████▏    | 204/393 [00:26<00:25,  7.27ba/s][1,3]<stderr>:#015 54%|█████▎    | 211/393 [00:26<00:27,  6.59ba/s][1,7]<stderr>:#015 52%|█████▏    | 206/393 [00:26<00:23,  8.06ba/s][1,2]<stderr>:#015 53%|█████▎    | 208/393 [00:26<00:24,  7.70ba/s][1,5]<stderr>:#015 52%|█████▏    | 204/393 [00:26<00:23,  8.09ba/s][1,0]<stderr>:#015 52%|█████▏    | 203/393 [00:26<00:22,  8.45ba/s][1,6]<stderr>:#015 53%|█████▎    | 209/393 [00:26<00:23,  7.83ba/s][1,4]<stderr>:#015 52%|█████▏    | 205/393 [00:26<00:24,  7.56ba/s][1,1]<stderr>:#015 53%|█████▎    | 208/393 [00:26<00:23,  7.74ba/s][1,2]<stderr>:#015 53%|█████▎    | 209/393 [00:26<00:24,  7.66ba/s][1,7]<stderr>:#015 53%|█████▎    | 207/393 [00:26<00:24,  7.55ba/s][1,6]<stderr>:#015 53%|█████▎    | 210/393 [00:26<00:23,  7.75ba/s][1,5]<stderr>:#015 52%|█████▏    | 205/393 [00:26<00:24,  7.72ba/s][1,3]<stderr>:#015 54%|█████▍    | 213/393 [00:26<00:24,  7.45ba/s][1,4]<stderr>:#015 52%|█████▏    | 206/393 [00:26<00:23,  7.79ba/s][1,1]<stderr>:#015 53%|█████▎    | 209/393 [00:26<00:23,  7.98ba/s][1,0]<stderr>:#015 52%|█████▏    | 204/393 [00:26<00:25,  7.33ba/s][1,2]<stderr>:#015 53%|█████▎    | 210/393 [00:26<00:23,  7.70ba/s][1,7]<stderr>:#015 53%|█████▎    | 208/393 [00:26<00:24,  7.60ba/s][1,4]<stderr>:#015 53%|█████▎    | 207/393 [00:26<00:23,  7.77ba/s][1,5]<stderr>:#015 52%|█████▏    | 206/393 [00:26<00:24,  7.63ba/s][1,6]<stderr>:#015 54%|█████▎    | 211/393 [00:26<00:24,  7.57ba/s][1,1]<stderr>:#015 53%|█████▎    | 210/393 [00:26<00:23,  7.96ba/s][1,0]<stderr>:#015 52%|█████▏    | 205/393 [00:26<00:25,  7.42ba/s][1,3]<stderr>:#015 55%|█████▍    | 215/393 [00:27<00:21,  8.37ba/s][1,4]<stderr>:#015 53%|█████▎    | 208/393 [00:26<00:22,  8.09ba/s][1,6]<stderr>:#015 54%|█████▍    | 212/393 [00:26<00:22,  7.94ba/s][1,5]<stderr>:#015 53%|█████▎    | 207/393 [00:26<00:24,  7.72ba/s][1,7]<stderr>:#015 53%|█████▎    | 209/393 [00:26<00:24,  7.42ba/s][1,0]<stderr>:#015 52%|█████▏    | 206/393 [00:26<00:23,  7.90ba/s][1,3]<stderr>:#015 55%|█████▍    | 216/393 [00:27<00:21,  8.39ba/s][1,2]<stderr>:#015 54%|█████▎    | 211/393 [00:26<00:26,  6.89ba/s][1,1]<stderr>:#015 54%|█████▎    | 211/393 [00:26<00:28,  6.49ba/s][1,4]<stderr>:#015 53%|█████▎    | 209/393 [00:26<00:22,  8.16ba/s][1,5]<stderr>:#015 53%|█████▎    | 208/393 [00:26<00:23,  7.95ba/s][1,0]<stderr>:#015 53%|█████▎    | 207/393 [00:26<00:22,  8.33ba/s][1,7]<stderr>:#015 53%|█████▎    | 210/393 [00:26<00:23,  7.69ba/s][1,2]<stderr>:#015 54%|█████▍    | 212/393 [00:27<00:25,  7.04ba/s][1,6]<stderr>:#015 54%|█████▍    | 213/393 [00:27<00:28,  6.41ba/s][1,5]<stderr>:#015 53%|█████▎    | 209/393 [00:26<00:22,  8.32ba/s][1,3]<stderr>:#015 55%|█████▌    | 218/393 [00:27<00:19,  8.88ba/s][1,0]<stderr>:#015 53%|█████▎    | 208/393 [00:27<00:21,  8.61ba/s][1,1]<stderr>:#015 54%|█████▍    | 212/393 [00:27<00:26,  6.72ba/s][1,4]<stderr>:#015 53%|█████▎    | 210/393 [00:26<00:23,  7.85ba/s][1,7]<stderr>:#015 54%|█████▎    | 211/393 [00:27<00:26,  6.82ba/s][1,6]<stderr>:#015 54%|█████▍    | 214/393 [00:27<00:25,  6.97ba/s][1,5]<stderr>:#015 53%|█████▎    | 210/393 [00:26<00:21,  8.41ba/s][1,1]<stderr>:#015 54%|█████▍    | 213/393 [00:27<00:24,  7.39ba/s][1,3]<stderr>:#015 56%|█████▌    | 219/393 [00:27<00:20,  8.67ba/s][1,2]<stderr>:#015 54%|█████▍    | 214/393 [00:27<00:22,  7.95ba/s][1,0]<stderr>:#015 53%|█████▎    | 209/393 [00:27<00:21,  8.55ba/s][1,7]<stderr>:#015 54%|█████▍    | 212/393 [00:27<00:24,  7.35ba/s][1,6]<stderr>:#015 55%|█████▍    | 215/393 [00:27<00:24,  7.40ba/s][1,1]<stderr>:#015 54%|█████▍    | 214/393 [00:27<00:22,  7.93ba/s][1,0]<stderr>:#015 53%|█████▎    | 210/393 [00:27<00:20,  8.79ba/s][1,2]<stderr>:#015 55%|█████▍    | 215/393 [00:27<00:21,  8.30ba/s][1,3]<stderr>:#015 56%|█████▌    | 220/393 [00:27<00:20,  8.61ba/s][1,4]<stderr>:#015 54%|█████▎    | 211/393 [00:27<00:30,  5.96ba/s][1,6]<stderr>:#015 55%|█████▍    | 216/393 [00:27<00:22,  7.78ba/s][1,1]<stderr>:#015 55%|█████▍    | 215/393 [00:27<00:21,  8.15ba/s][1,3]<stderr>:#015 56%|█████▌    | 221/393 [00:27<00:19,  8.91ba/s][1,5]<stderr>:#015 54%|█████▎    | 211/393 [00:27<00:27,  6.54ba/s][1,7]<stderr>:#015 54%|█████▍    | 214/393 [00:27<00:21,  8.26ba/s][1,2]<stderr>:#015 55%|█████▍    | 216/393 [00:27<00:22,  7.78ba/s][1,1]<stderr>:#015 55%|█████▍    | 216/393 [00:27<00:21,  8.18ba/s][1,0]<stderr>:#015 54%|█████▎    | 211/393 [00:27<00:27,  6.67ba/s][1,3]<stderr>:#015 56%|█████▋    | 222/393 [00:27<00:20,  8.42ba/s][1,2]<stderr>:#015 55%|█████▌    | 217/393 [00:27<00:21,  8.22ba/s][1,5]<stderr>:#015 54%|█████▍    | 212/393 [00:27<00:26,  6.75ba/s][1,7]<stderr>:#015 55%|█████▍    | 215/393 [00:27<00:21,  8.39ba/s][1,4]<stderr>:#015 54%|█████▍    | 213/393 [00:27<00:27,  6.63ba/s][1,6]<stderr>:#015 55%|█████▌    | 218/393 [00:27<00:20,  8.58ba/s][1,3]<stderr>:#015 57%|█████▋    | 223/393 [00:27<00:19,  8.83ba/s][1,1]<stderr>:#015 55%|█████▌    | 217/393 [00:27<00:21,  8.06ba/s][1,2]<stderr>:#015 55%|█████▌    | 218/393 [00:27<00:21,  8.12ba/s][1,4]<stderr>:#015 54%|█████▍    | 214/393 [00:27<00:24,  7.26ba/s][1,0]<stderr>:#015 54%|█████▍    | 212/393 [00:27<00:27,  6.51ba/s][1,5]<stderr>:#015 54%|█████▍    | 213/393 [00:27<00:27,  6.66ba/s][1,6]<stderr>:#015 56%|█████▌    | 219/393 [00:27<00:21,  8.25ba/s][1,7]<stderr>:#015 55%|█████▍    | 216/393 [00:27<00:23,  7.63ba/s][1,3]<stderr>:#015 57%|█████▋    | 224/393 [00:28<00:18,  9.15ba/s][1,1]<stderr>:#015 55%|█████▌    | 218/393 [00:27<00:20,  8.44ba/s][1,4]<stderr>:#015 55%|█████▍    | 215/393 [00:27<00:22,  7.80ba/s][1,2]<stderr>:#015 56%|█████▌    | 219/393 [00:27<00:22,  7.87ba/s][1,5]<stderr>:#015 54%|█████▍    | 214/393 [00:27<00:24,  7.21ba/s][1,0]<stderr>:#015 54%|█████▍    | 213/393 [00:27<00:26,  6.71ba/s][1,7]<stderr>:#015 55%|█████▌    | 217/393 [00:27<00:22,  7.78ba/s][1,6]<stderr>:#015 56%|█████▌    | 220/393 [00:27<00:21,  8.16ba/s][1,1]<stderr>:#015 56%|█████▌    | 219/393 [00:27<00:20,  8.29ba/s][1,4]<stderr>:#015 55%|█████▍    | 216/393 [00:27<00:21,  8.25ba/s][1,3]<stderr>:#015 58%|█████▊    | 226/393 [00:28<00:17,  9.55ba/s][1,2]<stderr>:#015 56%|█████▌    | 220/393 [00:27<00:22,  7.79ba/s][1,5]<stderr>:#015 55%|█████▍    | 215/393 [00:27<00:24,  7.40ba/s][1,0]<stderr>:#015 54%|█████▍    | 214/393 [00:27<00:25,  7.05ba/s][1,6]<stderr>:#015 56%|█████▌    | 221/393 [00:27<00:21,  8.18ba/s][1,7]<stderr>:#015 56%|█████▌    | 219/393 [00:28<00:21,  8.03ba/s][1,1]<stderr>:#015 56%|█████▌    | 221/393 [00:28<00:19,  8.96ba/s][1,2]<stderr>:#015 56%|█████▌    | 221/393 [00:28<00:21,  7.82ba/s][1,5]<stderr>:#015 55%|█████▍    | 216/393 [00:27<00:23,  7.56ba/s][1,4]<stderr>:#015 55%|█████▌    | 218/393 [00:27<00:19,  8.87ba/s][1,6]<stderr>:#015 56%|█████▋    | 222/393 [00:28<00:20,  8.22ba/s][1,0]<stderr>:#015 55%|█████▍    | 215/393 [00:28<00:24,  7.31ba/s][1,3]<stderr>:#015 58%|█████▊    | 227/393 [00:28<00:22,  7.45ba/s][1,2]<stderr>:#015 56%|█████▋    | 222/393 [00:28<00:21,  8.04ba/s][1,5]<stderr>:#015 55%|█████▌    | 217/393 [00:27<00:23,  7.50ba/s][1,7]<stderr>:#015 56%|█████▌    | 220/393 [00:28<00:22,  7.70ba/s][1,0]<stderr>:#015 55%|█████▍    | 216/393 [00:28<00:23,  7.43ba/s][1,4]<stderr>:#015 56%|█████▌    | 219/393 [00:27<00:21,  8.21ba/s][1,1]<stderr>:#015 56%|█████▋    | 222/393 [00:28<00:21,  8.10ba/s][1,6]<stderr>:#015 57%|█████▋    | 223/393 [00:28<00:21,  7.84ba/s][1,3]<stderr>:#015 58%|█████▊    | 228/393 [00:28<00:21,  7.77ba/s][1,2]<stderr>:#015 57%|█████▋    | 223/393 [00:28<00:20,  8.34ba/s][1,6]<stderr>:#015 57%|█████▋    | 224/393 [00:28<00:20,  8.37ba/s][1,0]<stderr>:#015 55%|█████▌    | 217/393 [00:28<00:22,  7.82ba/s][1,5]<stderr>:#015 55%|█████▌    | 218/393 [00:27<00:22,  7.79ba/s][1,1]<stderr>:#015 57%|█████▋    | 223/393 [00:28<00:20,  8.18ba/s][1,4]<stderr>:#015 56%|█████▌    | 220/393 [00:28<00:21,  8.19ba/s][1,7]<stderr>:#015 56%|█████▌    | 221/393 [00:28<00:22,  7.55ba/s][1,5]<stderr>:#015 56%|█████▌    | 219/393 [00:28<00:21,  8.13ba/s][1,4]<stderr>:#015 56%|█████▌    | 221/393 [00:28<00:20,  8.45ba/s][1,6]<stderr>:#015 57%|█████▋    | 225/393 [00:28<00:20,  8.10ba/s][1,3]<stderr>:#015 59%|█████▊    | 230/393 [00:28<00:19,  8.21ba/s][1,0]<stderr>:#015 55%|█████▌    | 218/393 [00:28<00:23,  7.57ba/s][1,1]<stderr>:#015 57%|█████▋    | 224/393 [00:28<00:21,  7.85ba/s][1,7]<stderr>:#015 56%|█████▋    | 222/393 [00:28<00:22,  7.51ba/s][1,2]<stderr>:#015 57%|█████▋    | 225/393 [00:28<00:19,  8.65ba/s][1,6]<stderr>:#015 58%|█████▊    | 226/393 [00:28<00:19,  8.41ba/s][1,4]<stderr>:#015 56%|█████▋    | 222/393 [00:28<00:20,  8.35ba/s][1,5]<stderr>:#015 56%|█████▌    | 220/393 [00:28<00:22,  7.80ba/s][1,3]<stderr>:#015 59%|█████▉    | 231/393 [00:28<00:19,  8.38ba/s][1,0]<stderr>:#015 56%|█████▌    | 219/393 [00:28<00:22,  7.89ba/s][1,1]<stderr>:#015 57%|█████▋    | 225/393 [00:28<00:20,  8.20ba/s][1,7]<stderr>:#015 57%|█████▋    | 223/393 [00:28<00:22,  7.52ba/s][1,2]<stderr>:#015 58%|█████▊    | 226/393 [00:28<00:19,  8.57ba/s][1,6]<stderr>:#015 58%|█████▊    | 227/393 [00:28<00:19,  8.38ba/s][1,4]<stderr>:#015 57%|█████▋    | 223/393 [00:28<00:20,  8.33ba/s][1,1]<stderr>:#015 58%|█████▊    | 226/393 [00:28<00:20,  8.29ba/s][1,5]<stderr>:#015 56%|█████▌    | 221/393 [00:28<00:22,  7.76ba/s][1,0]<stderr>:#015 56%|█████▌    | 220/393 [00:28<00:22,  7.82ba/s][1,7]<stderr>:#015 57%|█████▋    | 224/393 [00:28<00:20,  8.09ba/s][1,3]<stderr>:#015 59%|█████▉    | 233/393 [00:29<00:17,  9.11ba/s][1,6]<stderr>:#015 58%|█████▊    | 228/393 [00:28<00:19,  8.54ba/s][1,4]<stderr>:#015 57%|█████▋    | 224/393 [00:28<00:19,  8.69ba/s][1,0]<stderr>:#015 56%|█████▌    | 221/393 [00:28<00:21,  8.08ba/s][1,5]<stderr>:#015 56%|█████▋    | 222/393 [00:28<00:21,  7.92ba/s][1,2]<stderr>:#015 58%|█████▊    | 227/393 [00:28<00:24,  6.68ba/s][1,4]<stderr>:#015 57%|█████▋    | 225/393 [00:28<00:19,  8.69ba/s][1,7]<stderr>:#015 58%|█████▊    | 226/393 [00:28<00:19,  8.57ba/s][1,3]<stderr>:#015 60%|█████▉    | 235/393 [00:29<00:16,  9.85ba/s][1,5]<stderr>:#015 57%|█████▋    | 223/393 [00:28<00:20,  8.27ba/s][1,1]<stderr>:#015 58%|█████▊    | 227/393 [00:28<00:25,  6.40ba/s][1,2]<stderr>:#015 58%|█████▊    | 228/393 [00:28<00:22,  7.39ba/s][1,0]<stderr>:#015 56%|█████▋    | 222/393 [00:28<00:22,  7.47ba/s][1,4]<stderr>:#015 58%|█████▊    | 226/393 [00:28<00:19,  8.57ba/s][1,5]<stderr>:#015 57%|█████▋    | 224/393 [00:28<00:19,  8.66ba/s][1,1]<stderr>:#015 58%|█████▊    | 228/393 [00:29<00:23,  6.99ba/s][1,0]<stderr>:#015 57%|█████▋    | 223/393 [00:29<00:21,  7.74ba/s][1,6]<stderr>:#015 59%|█████▊    | 230/393 [00:29<00:20,  7.79ba/s][1,2]<stderr>:#015 59%|█████▊    | 230/393 [00:29<00:19,  8.20ba/s][1,3]<stderr>:#015 60%|██████    | 237/393 [00:29<00:16,  9.47ba/s][1,5]<stderr>:#015 57%|█████▋    | 225/393 [00:28<00:19,  8.50ba/s][1,7]<stderr>:#015 58%|█████▊    | 227/393 [00:29<00:26,  6.18ba/s][1,6]<stderr>:#015 59%|█████▉    | 231/393 [00:29<00:19,  8.24ba/s][1,0]<stderr>:#015 57%|█████▋    | 224/393 [00:29<00:22,  7.46ba/s][1,1]<stderr>:#015 59%|█████▊    | 230/393 [00:29<00:21,  7.64ba/s][1,2]<stderr>:#015 59%|█████▉    | 231/393 [00:29<00:19,  8.15ba/s][1,3]<stderr>:#015 61%|██████    | 238/393 [00:29<00:17,  8.65ba/s][1,7]<stderr>:#015 58%|█████▊    | 228/393 [00:29<00:24,  6.79ba/s][1,5]<stderr>:#015 58%|█████▊    | 226/393 [00:28<00:21,  7.91ba/s][1,4]<stderr>:#015 58%|█████▊    | 227/393 [00:28<00:27,  5.99ba/s][1,0]<stderr>:#015 57%|█████▋    | 225/393 [00:29<00:23,  7.30ba/s][1,2]<stderr>:#015 59%|█████▉    | 232/393 [00:29<00:20,  8.01ba/s][1,1]<stderr>:#015 59%|█████▉    | 231/393 [00:29<00:21,  7.52ba/s][1,3]<stderr>:#015 61%|██████    | 239/393 [00:29<00:17,  8.64ba/s][1,6]<stderr>:#015 59%|█████▉    | 233/393 [00:29<00:18,  8.76ba/s][1,7]<stderr>:#015 58%|█████▊    | 229/393 [00:29<00:22,  7.26ba/s][1,4]<stderr>:#015 58%|█████▊    | 228/393 [00:29<00:24,  6.66ba/s][1,5]<stderr>:#015 58%|█████▊    | 227/393 [00:29<00:23,  6.94ba/s][1,0]<stderr>:#015 58%|█████▊    | 226/393 [00:29<00:22,  7.58ba/s][1,1]<stderr>:#015 59%|█████▉    | 232/393 [00:29<00:20,  7.84ba/s][1,6]<stderr>:#015 60%|█████▉    | 234/393 [00:29<00:18,  8.44ba/s][1,4]<stderr>:#015 58%|█████▊    | 229/393 [00:29<00:22,  7.23ba/s][1,2]<stderr>:#015 59%|█████▉    | 233/393 [00:29<00:21,  7.61ba/s][1,3]<stderr>:#015 61%|██████    | 240/393 [00:29<00:18,  8.14ba/s][1,7]<stderr>:#015 59%|█████▊    | 230/393 [00:29<00:22,  7.36ba/s][1,1]<stderr>:#015 59%|█████▉    | 233/393 [00:29<00:20,  7.62ba/s][1,6]<stderr>:#015 60%|█████▉    | 235/393 [00:29<00:18,  8.32ba/s][1,7]<stderr>:#015 59%|█████▉    | 231/393 [00:29<00:20,  7.76ba/s][1,3]<stderr>:#015 61%|█████\u001b[0m\n",
      "\u001b[34m█▏   | 241/393 [00:29<00:18,  8.27ba/s][1,5]<stderr>:#015 58%|█████▊    | 229/393 [00:29<00:20,  7.90ba/s][1,2]<stderr>:#015 60%|█████▉    | 234/393 [00:29<00:20,  7.63ba/s][1,4]<stderr>:#015 59%|█████▊    | 230/393 [00:29<00:22,  7.33ba/s][1,6]<stderr>:#015 60%|██████    | 236/393 [00:29<00:18,  8.42ba/s][1,5]<stderr>:#015 59%|█████▊    | 230/393 [00:29<00:19,  8.27ba/s][1,7]<stderr>:#015 59%|█████▉    | 232/393 [00:29<00:20,  7.96ba/s][1,2]<stderr>:#015 60%|█████▉    | 235/393 [00:29<00:19,  8.01ba/s][1,3]<stderr>:#015 62%|██████▏   | 242/393 [00:30<00:18,  8.22ba/s][1,4]<stderr>:#015 59%|█████▉    | 231/393 [00:29<00:20,  7.73ba/s][1,0]<stderr>:#015 58%|█████▊    | 227/393 [00:29<00:29,  5.60ba/s][1,1]<stderr>:#015 60%|█████▉    | 234/393 [00:29<00:22,  7.00ba/s][1,2]<stderr>:#015 60%|██████    | 236/393 [00:29<00:19,  7.97ba/s][1,7]<stderr>:#015 59%|█████▉    | 233/393 [00:29<00:20,  7.81ba/s][1,0]<stderr>:#015 58%|█████▊    | 228/393 [00:29<00:26,  6.31ba/s][1,4]<stderr>:#015 59%|█████▉    | 232/393 [00:29<00:21,  7.63ba/s][1,5]<stderr>:#015 59%|█████▉    | 231/393 [00:29<00:21,  7.65ba/s][1,3]<stderr>:#015 62%|██████▏   | 243/393 [00:30<00:19,  7.76ba/s][1,1]<stderr>:#015 60%|█████▉    | 235/393 [00:29<00:20,  7.61ba/s][1,6]<stderr>:#015 60%|██████    | 237/393 [00:29<00:20,  7.68ba/s][1,6]<stderr>:#015 61%|██████    | 238/393 [00:29<00:18,  8.21ba/s][1,0]<stderr>:#015 58%|█████▊    | 229/393 [00:30<00:24,  6.69ba/s][1,7]<stderr>:#015 60%|█████▉    | 234/393 [00:30<00:20,  7.70ba/s][1,2]<stderr>:#015 60%|██████    | 237/393 [00:30<00:20,  7.74ba/s][1,1]<stderr>:#015 60%|██████    | 236/393 [00:30<00:19,  7.85ba/s][1,4]<stderr>:#015 59%|█████▉    | 233/393 [00:29<00:21,  7.50ba/s][1,5]<stderr>:#015 59%|█████▉    | 232/393 [00:29<00:22,  7.20ba/s][1,6]<stderr>:#015 61%|██████    | 239/393 [00:30<00:18,  8.23ba/s][1,3]<stderr>:#015 62%|██████▏   | 244/393 [00:30<00:24,  6.20ba/s][1,2]<stderr>:#015 61%|██████    | 238/393 [00:30<00:19,  7.82ba/s][1,0]<stderr>:#015 59%|█████▊    | 230/393 [00:30<00:23,  6.83ba/s][1,1]<stderr>:#015 60%|██████    | 237/393 [00:30<00:20,  7.70ba/s][1,4]<stderr>:#015 60%|█████▉    | 234/393 [00:29<00:20,  7.64ba/s][1,5]<stderr>:#015 59%|█████▉    | 233/393 [00:29<00:20,  7.81ba/s][1,3]<stderr>:#015 62%|██████▏   | 245/393 [00:30<00:21,  6.86ba/s][1,6]<stderr>:#015 61%|██████    | 240/393 [00:30<00:18,  8.16ba/s][1,7]<stderr>:#015 60%|██████    | 236/393 [00:30<00:19,  7.91ba/s][1,1]<stderr>:#015 61%|██████    | 238/393 [00:30<00:19,  8.12ba/s][1,4]<stderr>:#015 60%|█████▉    | 235/393 [00:29<00:19,  8.07ba/s][1,0]<stderr>:#015 59%|█████▉    | 231/393 [00:30<00:22,  7.15ba/s][1,5]<stderr>:#015 60%|█████▉    | 234/393 [00:29<00:20,  7.75ba/s][1,2]<stderr>:#015 61%|██████    | 239/393 [00:30<00:20,  7.38ba/s][1,6]<stderr>:#015 61%|██████▏   | 241/393 [00:30<00:19,  7.97ba/s][1,0]<stderr>:#015 59%|█████▉    | 232/393 [00:30<00:21,  7.66ba/s][1,4]<stderr>:#015 60%|██████    | 236/393 [00:30<00:19,  8.13ba/s][1,7]<stderr>:#015 60%|██████    | 237/393 [00:30<00:20,  7.77ba/s][1,1]<stderr>:#015 61%|██████    | 239/393 [00:30<00:19,  8.01ba/s][1,3]<stderr>:#015 63%|██████▎   | 246/393 [00:30<00:21,  6.85ba/s][1,2]<stderr>:#015 61%|██████    | 240/393 [00:30<00:20,  7.38ba/s][1,0]<stderr>:#015 59%|█████▉    | 233/393 [00:30<00:19,  8.01ba/s][1,5]<stderr>:#015 60%|██████    | 236/393 [00:30<00:19,  8.15ba/s][1,7]<stderr>:#015 61%|██████    | 238/393 [00:30<00:19,  7.95ba/s][1,1]<stderr>:#015 61%|██████    | 240/393 [00:30<00:18,  8.16ba/s][1,4]<stderr>:#015 60%|██████    | 237/393 [00:30<00:19,  8.00ba/s][1,3]<stderr>:#015 63%|██████▎   | 247/393 [00:30<00:20,  7.21ba/s][1,6]<stderr>:#015 62%|██████▏   | 242/393 [00:30<00:19,  7.74ba/s][1,2]<stderr>:#015 61%|██████▏   | 241/393 [00:30<00:20,  7.51ba/s][1,3]<stderr>:#015 63%|██████▎   | 248/393 [00:30<00:18,  7.70ba/s][1,7]<stderr>:#015 61%|██████    | 239/393 [00:30<00:19,  8.07ba/s][1,0]<stderr>:#015 60%|█████▉    | 234/393 [00:30<00:20,  7.80ba/s][1,4]<stderr>:#015 61%|██████    | 238/393 [00:30<00:19,  8.03ba/s][1,1]<stderr>:#015 61%|██████▏   | 241/393 [00:30<00:18,  8.01ba/s][1,6]<stderr>:#015 62%|██████▏   | 243/393 [00:30<00:18,  7.94ba/s][1,2]<stderr>:#015 62%|██████▏   | 242/393 [00:30<00:18,  7.97ba/s][1,5]<stderr>:#015 60%|██████    | 237/393 [00:30<00:21,  7.35ba/s][1,1]<stderr>:#015 62%|██████▏   | 242/393 [00:30<00:18,  8.38ba/s][1,4]<stderr>:#015 61%|██████    | 239/393 [00:30<00:18,  8.33ba/s][1,6]<stderr>:#015 62%|██████▏   | 244/393 [00:30<00:18,  8.15ba/s][1,7]<stderr>:#015 61%|██████    | 240/393 [00:30<00:19,  7.86ba/s][1,3]<stderr>:#015 63%|██████▎   | 249/393 [00:31<00:19,  7.47ba/s][1,0]<stderr>:#015 60%|█████▉    | 235/393 [00:30<00:21,  7.50ba/s][1,2]<stderr>:#015 62%|██████▏   | 243/393 [00:30<00:19,  7.84ba/s][1,1]<stderr>:#015 62%|██████▏   | 243/393 [00:30<00:18,  8.18ba/s][1,6]<stderr>:#015 62%|██████▏   | 245/393 [00:30<00:18,  8.13ba/s][1,4]<stderr>:#015 61%|██████    | 240/393 [00:30<00:19,  8.01ba/s][1,7]<stderr>:#015 61%|██████▏   | 241/393 [00:30<00:19,  7.92ba/s][1,5]<stderr>:#015 61%|██████    | 239/393 [00:30<00:19,  7.80ba/s][1,3]<stderr>:#015 64%|██████▎   | 250/393 [00:31<00:18,  7.57ba/s][1,0]<stderr>:#015 60%|██████    | 236/393 [00:30<00:20,  7.65ba/s][1,4]<stderr>:#015 61%|██████▏   | 241/393 [00:30<00:18,  8.43ba/s][1,7]<stderr>:#015 62%|██████▏   | 242/393 [00:31<00:18,  8.35ba/s][1,3]<stderr>:#015 64%|██████▍   | 251/393 [00:31<00:17,  8.08ba/s][1,5]<stderr>:#015 61%|██████    | 240/393 [00:30<00:18,  8.11ba/s][1,0]<stderr>:#015 60%|██████    | 237/393 [00:31<00:19,  8.14ba/s][1,2]<stderr>:#015 62%|██████▏   | 244/393 [00:31<00:24,  6.17ba/s][1,4]<stderr>:#015 62%|██████▏   | 242/393 [00:30<00:17,  8.75ba/s][1,7]<stderr>:#015 62%|██████▏   | 243/393 [00:31<00:17,  8.71ba/s][1,1]<stderr>:#015 62%|██████▏   | 244/393 [00:31<00:23,  6.32ba/s][1,0]<stderr>:#015 61%|██████    | 239/393 [00:31<00:17,  8.82ba/s][1,6]<stderr>:#015 63%|██████▎   | 246/393 [00:31<00:26,  5.52ba/s][1,3]<stderr>:#015 64%|██████▍   | 253/393 [00:31<00:17,  8.22ba/s][1,1]<stderr>:#015 62%|██████▏   | 245/393 [00:31<00:21,  6.81ba/s][1,2]<stderr>:#015 63%|██████▎   | 246/393 [00:31<00:20,  7.00ba/s][1,5]<stderr>:#015 62%|██████▏   | 242/393 [00:30<00:18,  8.10ba/s][1,6]<stderr>:#015 63%|██████▎   | 247/393 [00:31<00:23,  6.19ba/s][1,7]<stderr>:#015 62%|██████▏   | 244/393 [00:31<00:22,  6.75ba/s][1,1]<stderr>:#015 63%|██████▎   | 246/393 [00:31<00:20,  7.33ba/s][1,0]<stderr>:#015 61%|██████▏   | 241/393 [00:31<00:15,  9.67ba/s][1,5]<stderr>:#015 62%|██████▏   | 243/393 [00:31<00:17,  8.38ba/s][1,3]<stderr>:#015 65%|██████▍   | 254/393 [00:31<00:17,  7.89ba/s][1,4]<stderr>:#015 62%|██████▏   | 244/393 [00:31<00:19,  7.61ba/s][1,6]<stderr>:#015 63%|██████▎   | 248/393 [00:31<00:22,  6.53ba/s][1,7]<stderr>:#015 62%|██████▏   | 245/393 [00:31<00:21,  6.99ba/s][1,2]<stderr>:#015 63%|██████▎   | 248/393 [00:31<00:19,  7.43ba/s][1,1]<stderr>:#015 63%|██████▎   | 247/393 [00:31<00:19,  7.58ba/s][1,3]<stderr>:#015 65%|██████▍   | 255/393 [00:31<00:16,  8.30ba/s][1,0]<stderr>:#015 62%|██████▏   | 243/393 [00:31<00:15,  9.57ba/s][1,4]<stderr>:#015 62%|██████▏   | 245/393 [00:31<00:19,  7.48ba/s][1,6]<stderr>:#015 63%|██████▎   | 249/393 [00:31<00:21,  6.75ba/s][1,7]<stderr>:#015 63%|██████▎   | 246/393 [00:31<00:20,  7.16ba/s][1,1]<stderr>:#015 63%|██████▎   | 248/393 [00:31<00:18,  7.79ba/s][1,2]<stderr>:#015 63%|██████▎   | 249/393 [00:31<00:18,  7.65ba/s][1,5]<stderr>:#015 62%|██████▏   | 244/393 [00:31<00:25,  5.74ba/s][1,7]<stderr>:#015 63%|██████▎   | 247/393 [00:31<00:19,  7.52ba/s][1,1]<stderr>:#015 63%|██████▎   | 249/393 [00:31<00:17,  8.09ba/s][1,2]<stderr>:#015 64%|██████▎   | 250/393 [00:31<00:17,  7.98ba/s][1,3]<stderr>:#015 65%|██████▌   | 257/393 [00:31<00:16,  8.45ba/s][1,6]<stderr>:#015 64%|██████▎   | 250/393 [00:31<00:20,  6.94ba/s][1,4]<stderr>:#015 63%|██████▎   | 246/393 [00:31<00:20,  7.31ba/s][1,5]<stderr>:#015 62%|██████▏   | 245/393 [00:31<00:23,  6.33ba/s][1,0]<stderr>:#015 62%|██████▏   | 244/393 [00:31<00:21,  7.07ba/s][1,3]<stderr>:#015 66%|██████▌   | 258/393 [00:32<00:15,  8.51ba/s][1,6]<stderr>:#015 64%|██████▍   | 251/393 [00:31<00:18,  7.52ba/s][1,7]<stderr>:#015 63%|██████▎   | 248/393 [00:31<00:19,  7.59ba/s][1,2]<stderr>:#015 64%|██████▍   | 251/393 [00:31<00:17,  7.98ba/s][1,4]<stderr>:#015 63%|██████▎   | 247/393 [00:31<00:18,  7.74ba/s][1,1]<stderr>:#015 64%|██████▎   | 250/393 [00:31<00:18,  7.83ba/s][1,2]<stderr>:#015 64%|██████▍   | 252/393 [00:31<00:16,  8.32ba/s][1,0]<stderr>:#015 62%|██████▏   | 245/393 [00:31<00:21,  6.98ba/s][1,7]<stderr>:#015 63%|██████▎   | 249/393 [00:31<00:18,  7.93ba/s][1,4]<stderr>:#015 63%|██████▎   | 248/393 [00:31<00:18,  7.95ba/s][1,1]<stderr>:#015 64%|██████▍   | 251/393 [00:31<00:17,  8.22ba/s][1,3]<stderr>:#015 66%|██████▌   | 259/393 [00:32<00:16,  8.14ba/s][1,5]<stderr>:#015 63%|██████▎   | 247/393 [00:31<00:19,  7.32ba/s][1,6]<stderr>:#015 64%|██████▍   | 252/393 [00:31<00:20,  6.88ba/s][1,4]<stderr>:#015 63%|██████▎   | 249/393 [00:31<00:17,  8.15ba/s][1,7]<stderr>:#015 64%|██████▎   | 250/393 [00:32<00:17,  7.95ba/s][1,2]<stderr>:#015 64%|██████▍   | 253/393 [00:32<00:17,  8.07ba/s][1,0]<stderr>:#015 63%|██████▎   | 246/393 [00:32<00:20,  7.11ba/s][1,5]<stderr>:#015 63%|██████▎   | 248/393 [00:31<00:20,  7.01ba/s][1,1]<stderr>:#015 64%|██████▍   | 252/393 [00:32<00:19,  7.31ba/s][1,4]<stderr>:#015 64%|██████▎   | 250/393 [00:31<00:17,  8.35ba/s][1,7]<stderr>:#015 64%|██████▍   | 251/393 [00:32<00:17,  8.26ba/s][1,0]<stderr>:#015 63%|██████▎   | 247/393 [00:32<00:20,  7.27ba/s][1,6]<stderr>:#015 65%|██████▍   | 254/393 [00:32<00:18,  7.47ba/s][1,3]<stderr>:#015 66%|██████▌   | 260/393 [00:32<00:21,  6.15ba/s][1,2]<stderr>:#015 65%|██████▍   | 254/393 [00:32<00:18,  7.70ba/s][1,5]<stderr>:#015 63%|██████▎   | 249/393 [00:31<00:18,  7.66ba/s][1,7]<stderr>:#015 64%|██████▍   | 252/393 [00:32<00:17,  7.97ba/s][1,5]<stderr>:#015 64%|██████▎   | 250/393 [00:31<00:17,  8.09ba/s][1,4]<stderr>:#015 64%|██████▍   | 251/393 [00:32<00:18,  7.76ba/s][1,0]<stderr>:#015 63%|██████▎   | 248/393 [00:32<00:19,  7.45ba/s][1,6]<stderr>:#015 65%|██████▍   | 255/393 [00:32<00:18,  7.66ba/s][1,1]<stderr>:#015 65%|██████▍   | 254/393 [00:32<00:17,  7.82ba/s][1,2]<stderr>:#015 65%|██████▍   | 255/393 [00:32<00:17,  7.84ba/s][1,3]<stderr>:#015 66%|██████▋   | 261/393 [00:32<00:19,  6.61ba/s][1,7]<stderr>:#015 64%|██████▍   | 253/393 [00:32<00:17,  7.90ba/s][1,6]<stderr>:#015 65%|██████▌   | 256/393 [00:32<00:17,  7.96ba/s][1,0]<stderr>:#015 63%|██████▎   | 249/393 [00:32<00:18,  7.67ba/s][1,1]<stderr>:#015 65%|██████▍   | 255/393 [00:32<00:17,  7.98ba/s][1,2]<stderr>:#015 65%|██████▌   | 256/393 [00:32<00:17,  7.91ba/s][1,5]<stderr>:#015 64%|██████▍   | 251/393 [00:32<00:18,  7.86ba/s][1,3]<stderr>:#015 67%|██████▋   | 262/393 [00:32<00:18,  6.97ba/s][1,4]<stderr>:#015 64%|██████▍   | 252/393 [00:32<00:18,  7.54ba/s][1,1]<stderr>:#015 65%|██████▌   | 256/393 [00:32<00:17,  7.89ba/s][1,2]<stderr>:#015 65%|██████▌   | 257/393 [00:32<00:17,  7.93ba/s][1,3]<stderr>:#015 67%|██████▋   | 263/393 [00:32<00:17,  7.28ba/s][1,0]<stderr>:#015 64%|██████▎   | 250/393 [00:32<00:18,  7.62ba/s][1,5]<stderr>:#015 64%|██████▍   | 252/393 [00:32<00:17,  7.88ba/s][1,7]<stderr>:#015 65%|██████▍   | 254/393 [00:32<00:18,  7.54ba/s][1,4]<stderr>:#015 64%|██████▍   | 253/393 [00:32<00:18,  7.73ba/s][1,6]<stderr>:#015 65%|██████▌   | 257/393 [00:32<00:19,  7.08ba/s][1,2]<stderr>:#015 66%|██████▌   | 258/393 [00:32<00:16,  7.99ba/s][1,0]<stderr>:#015 64%|██████▍   | 251/393 [00:32<00:18,  7.73ba/s][1,5]<stderr>:#015 64%|██████▍   | 253/393 [00:32<00:17,  7.89ba/s][1,1]<stderr>:#015 65%|██████▌   | 257/393 [00:32<00:17,  7.83ba/s][1,7]<stderr>:#015 65%|██████▍   | 255/393 [00:32<00:18,  7.64ba/s][1,4]<stderr>:#015 65%|██████▍   | 254/393 [00:32<00:17,  7.77ba/s][1,3]<stderr>:#015 67%|██████▋   | 264/393 [00:33<00:18,  6.97ba/s][1,2]<stderr>:#015 66%|██████▌   | 259/393 [00:32<00:16,  8.14ba/s][1,1]<stderr>:#015 66%|██████▌   | 258/393 [00:32<00:16,  8.11ba/s][1,4]<stderr>:#015 65%|██████▍   | 255/393 [00:32<00:17,  7.81ba/s][1,0]<stderr>:#015 64%|██████▍   | 252/393 [00:32<00:18,  7.62ba/s][1,7]<stderr>:#015 65%|██████▌   | 256/393 [00:32<00:18,  7.58ba/s][1,3]<stderr>:#015 67%|██████▋   | 265/393 [00:33<00:17,  7.53ba/s][1,5]<stderr>:#015 65%|██████▍   | 254/393 [00:32<00:18,  7.59ba/s][1,6]<stderr>:#015 66%|██████▌   | 259/393 [00:32<00:18,  7.36ba/s][1,1]<stderr>:#015 66%|██████▌   | 259/393 [00:32<00:16,  8.05ba/s][1,0]<stderr>:#015 64%|██████▍   | 253/393 [00:33<00:17,  7.78ba/s][1,4]<stderr>:#015 65%|██████▌   | 256/393 [00:32<00:17,  7.76ba/s][1,7]<stderr>:#015 65%|██████▌   | 257/393 [00:33<00:17,  7.68ba/s][1,6]<stderr>:#015 66%|██████▌   | 260/393 [00:32<00:17,  7.74ba/s][1,3]<stderr>:#015 68%|██████▊   | 266/393 [00:33<00:17,  7.12ba/s][1,5]<stderr>:#015 65%|██████▍   | 255/393 [00:32<00:19,  7.23ba/s][1,2]<stderr>:#015 66%|██████▌   | 260/393 [00:33<00:20,  6.37ba/s][1,0]<stderr>:#015 65%|██████▍   | 254/393 [00:33<00:17,  7.80ba/s][1,6]<stderr>:#015 66%|██████▋   | 261/393 [00:33<00:16,  8.10ba/s][1,7]<stderr>:#015 66%|██████▌   | 258/393 [00:33<00:17,  7.85ba/s][1,4]<stderr>:#015 65%|██████▌   | 257/393 [00:32<00:17,  7.78ba/s][1,1]<stderr>:#015 66%|██████▌   | 260/393 [00:33<00:20,  6.46ba/s][1,6]<stderr>:#015 67%|██████▋   | 262/393 [00:33<00:16,  7.99ba/s][1,4]<stderr>:#015 66%|██████▌   | 258/393 [00:32<00:17,  7.83ba/s][1,0]<stderr>:#015 65%|██████▍   | 255/393 [00:33<00:17,  7.71ba/s][1,5]<stderr>:#015 65%|██████▌   | 257/393 [00:32<00:17,  7.70ba/s][1,7]<stderr>:#015 66%|██████▌   | 259/393 [00:33<00:17,  7.55ba/s][1,3]<stderr>:#015 68%|██████▊   | 268/393 [00:33<00:16,  7.46ba/s][1,2]<stderr>:#015 67%|██████▋   | 262/393 [00:33<00:18,  7.22ba/s][1,1]<stderr>:#015 66%|██████▋   | 261/393 [00:33<00:18,  7.14ba/s][1,4]<stderr>:#015 66%|██████▌   | 259/393 [00:33<00:17,  7.81ba/s][1,5]<stderr>:#015 66%|██████▌   | 258/393 [00:32<00:17,  7.76ba/s][1,3]<stderr>:#015 68%|██████▊   | 269/393 [00:33<00:15,  7.86ba/s][1,0]<stderr>:#015 65%|██████▌   | 256/393 [00:33<00:18,  7.58ba/s][1,\u001b[0m\n",
      "\u001b[34m2]<stderr>:#015 67%|██████▋   | 263/393 [00:33<00:17,  7.58ba/s][1,7]<stderr>:#015 66%|██████▌   | 260/393 [00:33<00:21,  6.30ba/s][1,3]<stderr>:#015 69%|██████▊   | 270/393 [00:33<00:14,  8.29ba/s][1,0]<stderr>:#015 65%|██████▌   | 257/393 [00:33<00:16,  8.15ba/s][1,1]<stderr>:#015 67%|██████▋   | 263/393 [00:33<00:16,  7.95ba/s][1,2]<stderr>:#015 67%|██████▋   | 264/393 [00:33<00:15,  8.16ba/s][1,5]<stderr>:#015 66%|██████▌   | 259/393 [00:33<00:16,  7.97ba/s][1,6]<stderr>:#015 67%|██████▋   | 263/393 [00:33<00:23,  5.62ba/s][1,3]<stderr>:#015 69%|██████▉   | 271/393 [00:33<00:14,  8.60ba/s][1,7]<stderr>:#015 66%|██████▋   | 261/393 [00:33<00:19,  6.87ba/s][1,0]<stderr>:#015 66%|██████▌   | 258/393 [00:33<00:16,  8.24ba/s][1,4]<stderr>:#015 66%|██████▌   | 260/393 [00:33<00:21,  6.05ba/s][1,2]<stderr>:#015 67%|██████▋   | 265/393 [00:33<00:16,  7.81ba/s][1,3]<stderr>:#015 69%|██████▉   | 272/393 [00:33<00:13,  8.69ba/s][1,1]<stderr>:#015 67%|██████▋   | 265/393 [00:33<00:15,  8.34ba/s][1,7]<stderr>:#015 67%|██████▋   | 262/393 [00:33<00:17,  7.37ba/s][1,5]<stderr>:#015 66%|██████▌   | 260/393 [00:33<00:20,  6.40ba/s][1,2]<stderr>:#015 68%|██████▊   | 266/393 [00:33<00:15,  8.18ba/s][1,4]<stderr>:#015 66%|██████▋   | 261/393 [00:33<00:20,  6.48ba/s][1,6]<stderr>:#015 67%|██████▋   | 265/393 [00:33<00:20,  6.34ba/s][1,1]<stderr>:#015 68%|██████▊   | 266/393 [00:33<00:15,  8.15ba/s][1,7]<stderr>:#015 67%|██████▋   | 263/393 [00:33<00:17,  7.50ba/s][1,3]<stderr>:#015 69%|██████▉   | 273/393 [00:34<00:14,  8.28ba/s][1,5]<stderr>:#015 66%|██████▋   | 261/393 [00:33<00:19,  6.72ba/s][1,4]<stderr>:#015 67%|██████▋   | 262/393 [00:33<00:18,  7.23ba/s][1,2]<stderr>:#015 68%|██████▊   | 267/393 [00:33<00:15,  8.14ba/s][1,6]<stderr>:#015 68%|██████▊   | 266/393 [00:33<00:17,  7.12ba/s][1,0]<stderr>:#015 66%|██████▌   | 260/393 [00:33<00:18,  7.38ba/s][1,3]<stderr>:#015 70%|██████▉   | 274/393 [00:34<00:13,  8.53ba/s][1,7]<stderr>:#015 67%|██████▋   | 264/393 [00:33<00:17,  7.58ba/s][1,1]<stderr>:#015 68%|██████▊   | 267/393 [00:33<00:15,  7.88ba/s][1,2]<stderr>:#015 68%|██████▊   | 268/393 [00:33<00:14,  8.41ba/s][1,6]<stderr>:#015 68%|██████▊   | 267/393 [00:33<00:16,  7.65ba/s][1,5]<stderr>:#015 67%|██████▋   | 262/393 [00:33<00:18,  7.05ba/s][1,3]<stderr>:#015 70%|██████▉   | 275/393 [00:34<00:13,  8.69ba/s][1,0]<stderr>:#015 66%|██████▋   | 261/393 [00:34<00:17,  7.71ba/s][1,2]<stderr>:#015 68%|██████▊   | 269/393 [00:34<00:14,  8.75ba/s][1,5]<stderr>:#015 67%|██████▋   | 263/393 [00:33<00:17,  7.58ba/s][1,1]<stderr>:#015 68%|██████▊   | 268/393 [00:34<00:15,  7.99ba/s][1,7]<stderr>:#015 67%|██████▋   | 265/393 [00:34<00:16,  7.69ba/s][1,6]<stderr>:#015 68%|██████▊   | 268/393 [00:34<00:16,  7.68ba/s][1,4]<stderr>:#015 67%|██████▋   | 264/393 [00:33<00:17,  7.27ba/s][1,0]<stderr>:#015 67%|██████▋   | 262/393 [00:34<00:16,  7.92ba/s][1,1]<stderr>:#015 68%|██████▊   | 269/393 [00:34<00:15,  8.01ba/s][1,2]<stderr>:#015 69%|██████▊   | 270/393 [00:34<00:14,  8.24ba/s][1,5]<stderr>:#015 67%|██████▋   | 264/393 [00:33<00:16,  7.65ba/s][1,7]<stderr>:#015 68%|██████▊   | 266/393 [00:34<00:16,  7.68ba/s][1,6]<stderr>:#015 68%|██████▊   | 269/393 [00:34<00:16,  7.68ba/s][1,2]<stderr>:#015 69%|██████▉   | 271/393 [00:34<00:14,  8.47ba/s][1,5]<stderr>:#015 67%|██████▋   | 265/393 [00:33<00:16,  7.93ba/s][1,1]<stderr>:#015 69%|██████▊   | 270/393 [00:34<00:15,  8.00ba/s][1,4]<stderr>:#015 68%|██████▊   | 266/393 [00:34<00:16,  7.81ba/s][1,6]<stderr>:#015 69%|██████▊   | 270/393 [00:34<00:15,  8.07ba/s][1,7]<stderr>:#015 68%|██████▊   | 267/393 [00:34<00:16,  7.80ba/s][1,0]<stderr>:#015 67%|██████▋   | 264/393 [00:34<00:14,  8.78ba/s][1,3]<stderr>:#015 70%|███████   | 277/393 [00:34<00:16,  7.21ba/s][1,0]<stderr>:#015 67%|██████▋   | 265/393 [00:34<00:14,  9.05ba/s][1,2]<stderr>:#015 69%|██████▉   | 272/393 [00:34<00:14,  8.16ba/s][1,1]<stderr>:#015 69%|██████▉   | 271/393 [00:34<00:15,  7.94ba/s][1,6]<stderr>:#015 69%|██████▉   | 271/393 [00:34<00:15,  8.04ba/s][1,5]<stderr>:#015 68%|██████▊   | 266/393 [00:34<00:16,  7.67ba/s][1,4]<stderr>:#015 68%|██████▊   | 267/393 [00:34<00:16,  7.74ba/s][1,7]<stderr>:#015 68%|██████▊   | 268/393 [00:34<00:16,  7.62ba/s][1,3]<stderr>:#015 71%|███████   | 278/393 [00:34<00:15,  7.32ba/s][1,1]<stderr>:#015 69%|██████▉   | 272/393 [00:34<00:14,  8.28ba/s][1,0]<stderr>:#015 68%|██████▊   | 266/393 [00:34<00:14,  8.62ba/s][1,2]<stderr>:#015 69%|██████▉   | 273/393 [00:34<00:15,  7.86ba/s][1,4]<stderr>:#015 68%|██████▊   | 268/393 [00:34<00:16,  7.80ba/s][1,6]<stderr>:#015 69%|██████▉   | 272/393 [00:34<00:15,  7.92ba/s][1,5]<stderr>:#015 68%|██████▊   | 267/393 [00:34<00:16,  7.54ba/s][1,7]<stderr>:#015 68%|██████▊   | 269/393 [00:34<00:16,  7.60ba/s][1,4]<stderr>:#015 68%|██████▊   | 269/393 [00:34<00:15,  8.10ba/s][1,5]<stderr>:#015 68%|██████▊   | 268/393 [00:34<00:15,  8.12ba/s][1,0]<stderr>:#015 68%|██████▊   | 267/393 [00:34<00:15,  8.14ba/s][1,3]<stderr>:#015 71%|███████   | 279/393 [00:34<00:15,  7.18ba/s][1,2]<stderr>:#015 70%|██████▉   | 274/393 [00:34<00:15,  7.90ba/s][1,1]<stderr>:#015 69%|██████▉   | 273/393 [00:34<00:15,  7.79ba/s][1,6]<stderr>:#015 69%|██████▉   | 273/393 [00:34<00:15,  7.98ba/s][1,7]<stderr>:#015 69%|██████▊   | 270/393 [00:34<00:15,  8.03ba/s][1,4]<stderr>:#015 69%|██████▊   | 270/393 [00:34<00:15,  8.07ba/s][1,5]<stderr>:#015 68%|██████▊   | 269/393 [00:34<00:15,  7.99ba/s][1,1]<stderr>:#015 70%|██████▉   | 274/393 [00:34<00:15,  7.84ba/s][1,2]<stderr>:#015 70%|██████▉   | 275/393 [00:34<00:14,  7.88ba/s][1,3]<stderr>:#015 71%|███████   | 280/393 [00:35<00:15,  7.30ba/s][1,0]<stderr>:#015 68%|██████▊   | 268/393 [00:34<00:15,  7.84ba/s][1,7]<stderr>:#015 69%|██████▉   | 271/393 [00:34<00:15,  7.84ba/s][1,6]<stderr>:#015 70%|██████▉   | 274/393 [00:34<00:15,  7.75ba/s][1,3]<stderr>:#015 72%|███████▏  | 281/393 [00:35<00:14,  7.74ba/s][1,5]<stderr>:#015 69%|██████▊   | 270/393 [00:34<00:15,  8.03ba/s][1,4]<stderr>:#015 69%|██████▉   | 271/393 [00:34<00:15,  7.96ba/s][1,6]<stderr>:#015 70%|██████▉   | 275/393 [00:34<00:14,  8.06ba/s][1,7]<stderr>:#015 69%|██████▉   | 272/393 [00:34<00:15,  7.86ba/s][1,1]<stderr>:#015 70%|██████▉   | 275/393 [00:34<00:15,  7.57ba/s][1,2]<stderr>:#015 70%|███████   | 276/393 [00:35<00:15,  7.61ba/s][1,6]<stderr>:#015 70%|███████   | 276/393 [00:35<00:14,  8.28ba/s][1,4]<stderr>:#015 69%|██████▉   | 272/393 [00:34<00:15,  7.99ba/s][1,0]<stderr>:#015 69%|██████▊   | 270/393 [00:35<00:15,  7.98ba/s][1,1]<stderr>:#015 70%|███████   | 276/393 [00:35<00:14,  8.03ba/s][1,7]<stderr>:#015 69%|██████▉   | 273/393 [00:35<00:14,  8.09ba/s][1,3]<stderr>:#015 72%|███████▏  | 282/393 [00:35<00:14,  7.42ba/s][1,5]<stderr>:#015 69%|██████▉   | 271/393 [00:34<00:16,  7.28ba/s][1,2]<stderr>:#015 70%|███████   | 277/393 [00:35<00:17,  6.79ba/s][1,6]<stderr>:#015 70%|███████   | 277/393 [00:35<00:14,  8.22ba/s][1,0]<stderr>:#015 69%|██████▉   | 271/393 [00:35<00:14,  8.15ba/s][1,4]<stderr>:#015 69%|██████▉   | 273/393 [00:34<00:14,  8.02ba/s][1,3]<stderr>:#015 72%|███████▏  | 283/393 [00:35<00:14,  7.75ba/s][1,5]<stderr>:#015 69%|██████▉   | 272/393 [00:34<00:15,  7.70ba/s][1,7]<stderr>:#015 70%|██████▉   | 274/393 [00:35<00:16,  7.28ba/s][1,2]<stderr>:#015 71%|███████   | 278/393 [00:35<00:15,  7.36ba/s][1,1]<stderr>:#015 70%|███████   | 277/393 [00:35<00:17,  6.58ba/s][1,4]<stderr>:#015 70%|██████▉   | 274/393 [00:35<00:14,  8.43ba/s][1,0]<stderr>:#015 69%|██████▉   | 272/393 [00:35<00:14,  8.42ba/s][1,3]<stderr>:#015 72%|███████▏  | 284/393 [00:35<00:13,  7.81ba/s][1,5]<stderr>:#015 69%|██████▉   | 273/393 [00:34<00:15,  7.77ba/s][1,6]<stderr>:#015 71%|███████   | 278/393 [00:35<00:15,  7.49ba/s][1,4]<stderr>:#015 70%|██████▉   | 275/393 [00:35<00:13,  8.43ba/s][1,0]<stderr>:#015 69%|██████▉   | 273/393 [00:35<00:15,  7.88ba/s][1,3]<stderr>:#015 73%|███████▎  | 285/393 [00:35<00:14,  7.71ba/s][1,2]<stderr>:#015 71%|███████   | 280/393 [00:35<00:14,  7.96ba/s][1,7]<stderr>:#015 70%|███████   | 276/393 [00:35<00:15,  7.73ba/s][1,1]<stderr>:#015 71%|███████   | 279/393 [00:35<00:15,  7.43ba/s][1,6]<stderr>:#015 71%|███████   | 279/393 [00:35<00:17,  6.44ba/s][1,5]<stderr>:#015 70%|██████▉   | 275/393 [00:35<00:14,  8.21ba/s][1,0]<stderr>:#015 70%|██████▉   | 274/393 [00:35<00:15,  7.46ba/s][1,2]<stderr>:#015 72%|███████▏  | 281/393 [00:35<00:15,  7.16ba/s][1,1]<stderr>:#015 72%|███████▏  | 281/393 [00:35<00:13,  8.25ba/s][1,3]<stderr>:#015 73%|███████▎  | 287/393 [00:35<00:12,  8.22ba/s][1,6]<stderr>:#015 71%|███████   | 280/393 [00:35<00:16,  6.95ba/s][1,5]<stderr>:#015 70%|███████   | 276/393 [00:35<00:14,  8.20ba/s][1,0]<stderr>:#015 70%|██████▉   | 275/393 [00:35<00:14,  7.90ba/s][1,7]<stderr>:#015 70%|███████   | 277/393 [00:35<00:19,  6.06ba/s][1,4]<stderr>:#015 70%|███████   | 277/393 [00:35<00:15,  7.64ba/s][1,6]<stderr>:#015 72%|███████▏  | 281/393 [00:35<00:15,  7.32ba/s][1,3]<stderr>:#015 73%|███████▎  | 288/393 [00:36<00:12,  8.18ba/s][1,1]<stderr>:#015 72%|███████▏  | 282/393 [00:35<00:14,  7.60ba/s][1,0]<stderr>:#015 70%|███████   | 276/393 [00:35<00:14,  7.82ba/s][1,2]<stderr>:#015 72%|███████▏  | 283/393 [00:35<00:14,  7.83ba/s][1,3]<stderr>:#015 74%|███████▎  | 289/393 [00:36<00:12,  8.63ba/s][1,6]<stderr>:#015 72%|███████▏  | 282/393 [00:35<00:14,  7.45ba/s][1,1]<stderr>:#015 72%|███████▏  | 283/393 [00:35<00:13,  8.05ba/s][1,7]<stderr>:#015 71%|███████   | 279/393 [00:35<00:16,  6.77ba/s][1,4]<stderr>:#015 71%|███████   | 279/393 [00:35<00:14,  8.14ba/s][1,5]<stderr>:#015 70%|███████   | 277/393 [00:35<00:19,  5.80ba/s][1,2]<stderr>:#015 73%|███████▎  | 285/393 [00:36<00:12,  8.51ba/s][1,1]<stderr>:#015 72%|███████▏  | 284/393 [00:36<00:13,  8.27ba/s][1,6]<stderr>:#015 72%|███████▏  | 283/393 [00:36<00:14,  7.77ba/s][1,0]<stderr>:#015 70%|███████   | 277/393 [00:36<00:17,  6.57ba/s][1,7]<stderr>:#015 71%|███████   | 280/393 [00:36<00:15,  7.13ba/s][1,3]<stderr>:#015 74%|███████▍  | 291/393 [00:36<00:10,  9.41ba/s][1,5]<stderr>:#015 71%|███████   | 278/393 [00:35<00:18,  6.24ba/s][1,4]<stderr>:#015 72%|███████▏  | 281/393 [00:35<00:13,  8.46ba/s][1,2]<stderr>:#015 73%|███████▎  | 286/393 [00:36<00:13,  8.12ba/s][1,0]<stderr>:#015 71%|███████   | 278/393 [00:36<00:16,  7.03ba/s][1,1]<stderr>:#015 73%|███████▎  | 285/393 [00:36<00:13,  7.92ba/s][1,6]<stderr>:#015 72%|███████▏  | 284/393 [00:36<00:14,  7.57ba/s][1,7]<stderr>:#015 72%|███████▏  | 281/393 [00:36<00:15,  7.36ba/s][1,3]<stderr>:#015 74%|███████▍  | 292/393 [00:36<00:11,  9.04ba/s][1,5]<stderr>:#015 71%|███████   | 279/393 [00:35<00:17,  6.52ba/s][1,1]<stderr>:#015 73%|███████▎  | 286/393 [00:36<00:12,  8.37ba/s][1,4]<stderr>:#015 72%|███████▏  | 282/393 [00:35<00:13,  8.35ba/s][1,0]<stderr>:#015 71%|███████   | 279/393 [00:36<00:15,  7.38ba/s][1,7]<stderr>:#015 72%|███████▏  | 282/393 [00:36<00:14,  7.82ba/s][1,6]<stderr>:#015 73%|███████▎  | 285/393 [00:36<00:14,  7.70ba/s][1,2]<stderr>:#015 73%|███████▎  | 287/393 [00:36<00:13,  7.82ba/s][1,7]<stderr>:#015 72%|███████▏  | 283/393 [00:36<00:13,  8.13ba/s][1,4]<stderr>:#015 72%|███████▏  | 283/393 [00:36<00:13,  8.29ba/s][1,0]<stderr>:#015 71%|███████   | 280/393 [00:36<00:14,  7.65ba/s][1,1]<stderr>:#015 73%|███████▎  | 287/393 [00:36<00:12,  8.20ba/s][1,5]<stderr>:#015 72%|███████▏  | 281/393 [00:36<00:14,  7.51ba/s][1,6]<stderr>:#015 73%|███████▎  | 286/393 [00:36<00:13,  7.98ba/s][1,2]<stderr>:#015 73%|███████▎  | 288/393 [00:36<00:13,  8.05ba/s][1,3]<stderr>:#015 75%|███████▍  | 293/393 [00:36<00:14,  6.71ba/s][1,0]<stderr>:#015 72%|███████▏  | 281/393 [00:36<00:14,  7.79ba/s][1,1]<stderr>:#015 73%|███████▎  | 288/393 [00:36<00:12,  8.17ba/s][1,5]<stderr>:#015 72%|███████▏  | 282/393 [00:36<00:14,  7.79ba/s][1,4]<stderr>:#015 72%|███████▏  | 284/393 [00:36<00:13,  8.12ba/s][1,6]<stderr>:#015 73%|███████▎  | 287/393 [00:36<00:13,  8.09ba/s][1,7]<stderr>:#015 72%|███████▏  | 284/393 [00:36<00:13,  7.89ba/s][1,3]<stderr>:#015 75%|███████▍  | 294/393 [00:36<00:13,  7.20ba/s][1,2]<stderr>:#015 74%|███████▎  | 289/393 [00:36<00:12,  8.10ba/s][1,1]<stderr>:#015 74%|███████▎  | 289/393 [00:36<00:12,  8.32ba/s][1,2]<stderr>:#015 74%|███████▍  | 290/393 [00:36<00:12,  8.19ba/s][1,4]<stderr>:#015 73%|███████▎  | 285/393 [00:36<00:13,  7.98ba/s][1,7]<stderr>:#015 73%|███████▎  | 285/393 [00:36<00:13,  7.89ba/s][1,3]<stderr>:#015 75%|███████▌  | 295/393 [00:36<00:13,  7.37ba/s][1,0]<stderr>:#015 72%|███████▏  | 282/393 [00:36<00:14,  7.50ba/s][1,6]<stderr>:#015 73%|███████▎  | 288/393 [00:36<00:13,  7.80ba/s][1,5]<stderr>:#015 72%|███████▏  | 283/393 [00:36<00:15,  7.13ba/s][1,1]<stderr>:#015 74%|███████▍  | 290/393 [00:36<00:12,  8.11ba/s][1,4]<stderr>:#015 73%|███████▎  | 286/393 [00:36<00:13,  8.23ba/s][1,7]<stderr>:#015 73%|███████▎  | 286/393 [00:36<00:13,  8.09ba/s][1,2]<stderr>:#015 74%|███████▍  | 291/393 [00:36<00:12,  8.14ba/s][1,6]<stderr>:#015 74%|███████▎  | 289/393 [00:36<00:12,  8.06ba/s][1,3]<stderr>:#015 75%|███████▌  | 296/393 [00:37<00:12,  7.53ba/s][1,0]<stderr>:#015 72%|███████▏  | 283/393 [00:36<00:14,  7.50ba/s][1,1]<stderr>:#015 74%|███████▍  | 291/393 [00:36<00:12,  8.15ba/s][1,6]<stderr>:#015 74%|███████▍  | 290/393 [00:36<00:12,  8.29ba/s][1,4]<stderr>:#015 73%|███████▎  | 287/393 [00:36<00:13,  7.98ba/s][1,7]<stderr>:#015 73%|███████▎  | 287/393 [00:36<00:13,  7.94ba/s][1,3]<stderr>:#015 76%|███████▌  | 297/393 [00:37<00:12,  7.75ba/s][1,5]<stderr>:#015 73%|███████▎  | 285/393 [00:36<00:14,  7.60ba/s][1,2]<stderr>:#015 74%|███████▍  | 292/393 [00:36<00:14,  7.20ba/s][1,7]<stderr>:#015 73%|███████▎  | 288/393 [00:37<00:12,  8.19ba/s][1,5]<stderr>:#015 73%|███████▎  | 286/393 [00:36<00:13,  8.07ba/s][1,4]<stderr>:#015 73%|███████▎  | 288/393 [00:36<00:12,  8.09ba/s][1,0]<stderr>:#015 73%|███████▎  | 285/393 [00:37<00:13,  7.86ba/s][1,1]<stderr>:#015 74%|███████▍  | 292/393 [00:37<00:12,  7.89ba/s][1,3]<stderr>:#015 76%|███████▌  | 298/393 [00:37<00:12,  7.86ba/s][1,6]<stderr>:#015 7\u001b[0m\n",
      "\u001b[34m4%|███████▍  | 291/393 [00:37<00:12,  7.93ba/s][1,2]<stderr>:#015 75%|███████▍  | 293/393 [00:37<00:14,  6.85ba/s][1,7]<stderr>:#015 74%|███████▎  | 289/393 [00:37<00:12,  8.31ba/s][1,4]<stderr>:#015 74%|███████▎  | 289/393 [00:36<00:12,  8.28ba/s][1,5]<stderr>:#015 73%|███████▎  | 287/393 [00:36<00:13,  8.09ba/s][1,0]<stderr>:#015 73%|███████▎  | 286/393 [00:37<00:13,  7.86ba/s][1,6]<stderr>:#015 74%|███████▍  | 292/393 [00:37<00:12,  7.96ba/s][1,3]<stderr>:#015 76%|███████▌  | 299/393 [00:37<00:13,  6.92ba/s][1,2]<stderr>:#015 75%|███████▍  | 294/393 [00:37<00:13,  7.09ba/s][1,5]<stderr>:#015 73%|███████▎  | 288/393 [00:36<00:12,  8.42ba/s][1,7]<stderr>:#015 74%|███████▍  | 290/393 [00:37<00:12,  8.26ba/s][1,0]<stderr>:#015 73%|███████▎  | 287/393 [00:37<00:12,  8.24ba/s][1,4]<stderr>:#015 74%|███████▍  | 290/393 [00:36<00:12,  8.18ba/s][1,1]<stderr>:#015 75%|███████▍  | 293/393 [00:37<00:16,  6.01ba/s][1,3]<stderr>:#015 76%|███████▋  | 300/393 [00:37<00:12,  7.55ba/s][1,0]<stderr>:#015 73%|███████▎  | 288/393 [00:37<00:12,  8.36ba/s][1,6]<stderr>:#015 75%|███████▍  | 294/393 [00:37<00:12,  8.25ba/s][1,5]<stderr>:#015 74%|███████▎  | 289/393 [00:37<00:13,  7.99ba/s][1,7]<stderr>:#015 74%|███████▍  | 291/393 [00:37<00:12,  7.94ba/s][1,4]<stderr>:#015 74%|███████▍  | 291/393 [00:37<00:12,  7.92ba/s][1,2]<stderr>:#015 75%|███████▌  | 295/393 [00:37<00:14,  6.90ba/s][1,6]<stderr>:#015 75%|███████▌  | 295/393 [00:37<00:12,  8.14ba/s][1,7]<stderr>:#015 74%|███████▍  | 292/393 [00:37<00:12,  8.06ba/s][1,0]<stderr>:#015 74%|███████▎  | 289/393 [00:37<00:13,  7.84ba/s][1,5]<stderr>:#015 74%|███████▍  | 290/393 [00:37<00:13,  7.85ba/s][1,4]<stderr>:#015 74%|███████▍  | 292/393 [00:37<00:12,  7.91ba/s][1,1]<stderr>:#015 75%|███████▌  | 295/393 [00:37<00:15,  6.53ba/s][1,3]<stderr>:#015 77%|███████▋  | 302/393 [00:37<00:11,  7.66ba/s][1,2]<stderr>:#015 75%|███████▌  | 296/393 [00:37<00:14,  6.48ba/s][1,0]<stderr>:#015 74%|███████▍  | 290/393 [00:37<00:12,  8.16ba/s][1,1]<stderr>:#015 75%|███████▌  | 296/393 [00:37<00:13,  7.05ba/s][1,5]<stderr>:#015 74%|███████▍  | 291/393 [00:37<00:12,  7.89ba/s][1,3]<stderr>:#015 77%|███████▋  | 303/393 [00:37<00:10,  8.22ba/s][1,0]<stderr>:#015 74%|███████▍  | 291/393 [00:37<00:11,  8.63ba/s][1,4]<stderr>:#015 75%|███████▍  | 293/393 [00:37<00:15,  6.47ba/s][1,7]<stderr>:#015 75%|███████▍  | 293/393 [00:37<00:15,  6.35ba/s][1,5]<stderr>:#015 74%|███████▍  | 292/393 [00:37<00:12,  8.31ba/s][1,2]<stderr>:#015 76%|███████▌  | 298/393 [00:37<00:12,  7.31ba/s][1,1]<stderr>:#015 76%|███████▌  | 297/393 [00:37<00:13,  7.25ba/s][1,6]<stderr>:#015 75%|███████▌  | 296/393 [00:37<00:16,  5.88ba/s][1,3]<stderr>:#015 78%|███████▊  | 305/393 [00:38<00:10,  8.70ba/s][1,4]<stderr>:#015 75%|███████▍  | 294/393 [00:37<00:14,  6.70ba/s][1,0]<stderr>:#015 74%|███████▍  | 292/393 [00:37<00:12,  7.78ba/s][1,6]<stderr>:#015 76%|███████▌  | 297/393 [00:37<00:14,  6.63ba/s][1,2]<stderr>:#015 76%|███████▌  | 299/393 [00:37<00:12,  7.38ba/s][1,1]<stderr>:#015 76%|███████▌  | 298/393 [00:37<00:12,  7.39ba/s][1,7]<stderr>:#015 75%|███████▍  | 294/393 [00:37<00:15,  6.21ba/s][1,3]<stderr>:#015 78%|███████▊  | 306/393 [00:38<00:10,  8.42ba/s][1,4]<stderr>:#015 75%|███████▌  | 295/393 [00:37<00:13,  7.19ba/s][1,6]<stderr>:#015 76%|███████▌  | 298/393 [00:38<00:13,  6.97ba/s][1,1]<stderr>:#015 76%|███████▌  | 299/393 [00:38<00:12,  7.57ba/s][1,2]<stderr>:#015 76%|███████▋  | 300/393 [00:38<00:12,  7.49ba/s][1,7]<stderr>:#015 75%|███████▌  | 295/393 [00:38<00:14,  6.83ba/s][1,5]<stderr>:#015 75%|███████▍  | 293/393 [00:37<00:18,  5.33ba/s][1,0]<stderr>:#015 75%|███████▍  | 293/393 [00:38<00:15,  6.33ba/s][1,4]<stderr>:#015 75%|███████▌  | 296/393 [00:37<00:12,  7.48ba/s][1,3]<stderr>:#015 78%|███████▊  | 307/393 [00:38<00:10,  8.16ba/s][1,1]<stderr>:#015 76%|███████▋  | 300/393 [00:38<00:11,  7.99ba/s][1,6]<stderr>:#015 76%|███████▌  | 299/393 [00:38<00:12,  7.29ba/s][1,5]<stderr>:#015 75%|███████▍  | 294/393 [00:37<00:16,  6.14ba/s][1,4]<stderr>:#015 76%|███████▌  | 297/393 [00:37<00:12,  7.81ba/s][1,2]<stderr>:#015 77%|███████▋  | 302/393 [00:38<00:11,  7.98ba/s][1,0]<stderr>:#015 75%|███████▍  | 294/393 [00:38<00:14,  6.76ba/s][1,3]<stderr>:#015 78%|███████▊  | 308/393 [00:38<00:10,  8.19ba/s][1,7]<stderr>:#015 76%|███████▌  | 297/393 [00:38<00:13,  7.34ba/s][1,1]<stderr>:#015 77%|███████▋  | 301/393 [00:38<00:11,  7.71ba/s][1,6]<stderr>:#015 76%|███████▋  | 300/393 [00:38<00:12,  7.18ba/s][1,2]<stderr>:#015 77%|███████▋  | 303/393 [00:38<00:11,  7.98ba/s][1,0]<stderr>:#015 75%|███████▌  | 295/393 [00:38<00:14,  6.90ba/s][1,4]<stderr>:#015 76%|███████▌  | 298/393 [00:38<00:12,  7.44ba/s][1,3]<stderr>:#015 79%|███████▊  | 309/393 [00:38<00:10,  7.91ba/s][1,7]<stderr>:#015 76%|███████▌  | 298/393 [00:38<00:12,  7.38ba/s][1,1]<stderr>:#015 77%|███████▋  | 302/393 [00:38<00:11,  7.96ba/s][1,6]<stderr>:#015 77%|███████▋  | 301/393 [00:38<00:12,  7.63ba/s][1,5]<stderr>:#015 75%|███████▌  | 296/393 [00:38<00:14,  6.88ba/s][1,4]<stderr>:#015 76%|███████▌  | 299/393 [00:38<00:11,  7.84ba/s][1,0]<stderr>:#015 75%|███████▌  | 296/393 [00:38<00:13,  6.96ba/s][1,7]<stderr>:#015 76%|███████▌  | 299/393 [00:38<00:12,  7.42ba/s][1,5]<stderr>:#015 76%|███████▌  | 297/393 [00:38<00:13,  7.35ba/s][1,6]<stderr>:#015 77%|███████▋  | 302/393 [00:38<00:11,  7.65ba/s][1,1]<stderr>:#015 77%|███████▋  | 303/393 [00:38<00:11,  7.63ba/s][1,2]<stderr>:#015 78%|███████▊  | 305/393 [00:38<00:10,  8.46ba/s][1,4]<stderr>:#015 76%|███████▋  | 300/393 [00:38<00:11,  8.16ba/s][1,3]<stderr>:#015 79%|███████▉  | 310/393 [00:38<00:13,  6.18ba/s][1,7]<stderr>:#015 76%|███████▋  | 300/393 [00:38<00:12,  7.52ba/s][1,0]<stderr>:#015 76%|███████▌  | 297/393 [00:38<00:13,  6.97ba/s][1,1]<stderr>:#015 77%|███████▋  | 304/393 [00:38<00:11,  7.45ba/s][1,5]<stderr>:#015 76%|███████▌  | 299/393 [00:38<00:11,  8.07ba/s][1,6]<stderr>:#015 77%|███████▋  | 304/393 [00:38<00:10,  8.20ba/s][1,3]<stderr>:#015 79%|███████▉  | 311/393 [00:39<00:12,  6.80ba/s][1,2]<stderr>:#015 78%|███████▊  | 307/393 [00:38<00:09,  8.96ba/s][1,4]<stderr>:#015 77%|███████▋  | 301/393 [00:38<00:12,  7.48ba/s][1,7]<stderr>:#015 77%|███████▋  | 301/393 [00:38<00:12,  7.56ba/s][1,0]<stderr>:#015 76%|███████▌  | 298/393 [00:38<00:14,  6.63ba/s][1,5]<stderr>:#015 76%|███████▋  | 300/393 [00:38<00:11,  8.24ba/s][1,6]<stderr>:#015 78%|███████▊  | 305/393 [00:38<00:10,  8.59ba/s][1,3]<stderr>:#015 79%|███████▉  | 312/393 [00:39<00:11,  7.14ba/s][1,1]<stderr>:#015 78%|███████▊  | 306/393 [00:38<00:10,  8.09ba/s][1,2]<stderr>:#015 78%|███████▊  | 308/393 [00:38<00:09,  8.70ba/s][1,4]<stderr>:#015 77%|███████▋  | 302/393 [00:38<00:12,  7.50ba/s][1,7]<stderr>:#015 77%|███████▋  | 302/393 [00:38<00:11,  7.69ba/s][1,5]<stderr>:#015 77%|███████▋  | 301/393 [00:38<00:10,  8.66ba/s][1,6]<stderr>:#015 78%|███████▊  | 306/393 [00:38<00:10,  8.65ba/s][1,0]<stderr>:#015 76%|███████▌  | 299/393 [00:39<00:14,  6.64ba/s][1,3]<stderr>:#015 80%|███████▉  | 313/393 [00:39<00:11,  7.24ba/s][1,2]<stderr>:#015 79%|███████▊  | 309/393 [00:39<00:09,  8.67ba/s][1,4]<stderr>:#015 77%|███████▋  | 303/393 [00:38<00:11,  8.03ba/s][1,7]<stderr>:#015 77%|███████▋  | 303/393 [00:39<00:11,  7.51ba/s][1,6]<stderr>:#015 78%|███████▊  | 307/393 [00:39<00:10,  8.41ba/s][1,0]<stderr>:#015 76%|███████▋  | 300/393 [00:39<00:12,  7.18ba/s][1,1]<stderr>:#015 78%|███████▊  | 308/393 [00:39<00:10,  8.36ba/s][1,3]<stderr>:#015 80%|███████▉  | 314/393 [00:39<00:10,  7.88ba/s][1,4]<stderr>:#015 77%|███████▋  | 304/393 [00:38<00:11,  7.96ba/s][1,5]<stderr>:#015 77%|███████▋  | 303/393 [00:38<00:09,  9.00ba/s][1,2]<stderr>:#015 79%|███████▉  | 310/393 [00:39<00:11,  7.36ba/s][1,6]<stderr>:#015 78%|███████▊  | 308/393 [00:39<00:10,  8.13ba/s][1,0]<stderr>:#015 77%|███████▋  | 301/393 [00:39<00:12,  7.53ba/s][1,1]<stderr>:#015 79%|███████▊  | 309/393 [00:39<00:10,  8.19ba/s][1,3]<stderr>:#015 80%|████████  | 315/393 [00:39<00:09,  7.80ba/s][1,7]<stderr>:#015 78%|███████▊  | 305/393 [00:39<00:10,  8.01ba/s][1,5]<stderr>:#015 77%|███████▋  | 304/393 [00:38<00:10,  8.71ba/s][1,0]<stderr>:#015 77%|███████▋  | 302/393 [00:39<00:11,  7.88ba/s][1,6]<stderr>:#015 79%|███████▊  | 309/393 [00:39<00:10,  8.01ba/s][1,4]<stderr>:#015 78%|███████▊  | 306/393 [00:39<00:10,  8.38ba/s][1,2]<stderr>:#015 79%|███████▉  | 312/393 [00:39<00:09,  8.17ba/s][1,7]<stderr>:#015 78%|███████▊  | 306/393 [00:39<00:10,  8.05ba/s][1,3]<stderr>:#015 80%|████████  | 316/393 [00:39<00:10,  7.29ba/s][1,5]<stderr>:#015 78%|███████▊  | 305/393 [00:39<00:10,  8.44ba/s][1,1]<stderr>:#015 79%|███████▉  | 310/393 [00:39<00:11,  7.19ba/s][1,0]<stderr>:#015 77%|███████▋  | 303/393 [00:39<00:11,  7.86ba/s][1,6]<stderr>:#015 79%|███████▉  | 310/393 [00:39<00:10,  8.01ba/s][1,4]<stderr>:#015 78%|███████▊  | 307/393 [00:39<00:10,  8.14ba/s][1,2]<stderr>:#015 80%|███████▉  | 313/393 [00:39<00:09,  8.52ba/s][1,3]<stderr>:#015 81%|████████  | 317/393 [00:39<00:09,  7.84ba/s][1,7]<stderr>:#015 78%|███████▊  | 307/393 [00:39<00:10,  8.26ba/s][1,5]<stderr>:#015 78%|███████▊  | 306/393 [00:39<00:10,  8.65ba/s][1,1]<stderr>:#015 79%|███████▉  | 311/393 [00:39<00:10,  7.83ba/s][1,3]<stderr>:#015 81%|████████  | 318/393 [00:39<00:09,  8.02ba/s][1,7]<stderr>:#015 78%|███████▊  | 308/393 [00:39<00:10,  8.30ba/s][1,6]<stderr>:#015 79%|███████▉  | 311/393 [00:39<00:10,  7.46ba/s][1,0]<stderr>:#015 77%|███████▋  | 304/393 [00:39<00:12,  7.23ba/s][1,4]<stderr>:#015 78%|███████▊  | 308/393 [00:39<00:11,  7.55ba/s][1,2]<stderr>:#015 80%|███████▉  | 314/393 [00:39<00:10,  7.72ba/s][1,1]<stderr>:#015 79%|███████▉  | 312/393 [00:39<00:10,  7.77ba/s][1,5]<stderr>:#015 78%|███████▊  | 307/393 [00:39<00:10,  7.84ba/s][1,4]<stderr>:#015 79%|███████▊  | 309/393 [00:39<00:10,  7.66ba/s][1,3]<stderr>:#015 81%|████████  | 319/393 [00:40<00:09,  7.69ba/s][1,1]<stderr>:#015 80%|███████▉  | 313/393 [00:39<00:10,  7.86ba/s][1,7]<stderr>:#015 79%|███████▊  | 309/393 [00:39<00:10,  7.77ba/s][1,2]<stderr>:#015 80%|████████  | 315/393 [00:39<00:10,  7.63ba/s][1,0]<stderr>:#015 78%|███████▊  | 305/393 [00:39<00:12,  7.00ba/s][1,5]<stderr>:#015 78%|███████▊  | 308/393 [00:39<00:10,  8.26ba/s][1,6]<stderr>:#015 79%|███████▉  | 312/393 [00:39<00:13,  6.01ba/s][1,5]<stderr>:#015 79%|███████▊  | 309/393 [00:39<00:09,  8.42ba/s][1,3]<stderr>:#015 81%|████████▏ | 320/393 [00:40<00:09,  7.82ba/s][1,2]<stderr>:#015 80%|████████  | 316/393 [00:39<00:09,  7.90ba/s][1,0]<stderr>:#015 78%|███████▊  | 306/393 [00:39<00:11,  7.29ba/s][1,1]<stderr>:#015 80%|███████▉  | 314/393 [00:39<00:10,  7.57ba/s][1,6]<stderr>:#015 80%|███████▉  | 313/393 [00:39<00:12,  6.62ba/s][1,0]<stderr>:#015 78%|███████▊  | 307/393 [00:40<00:10,  7.87ba/s][1,2]<stderr>:#015 81%|████████  | 317/393 [00:40<00:09,  8.04ba/s][1,4]<stderr>:#015 79%|███████▉  | 310/393 [00:39<00:14,  5.85ba/s][1,7]<stderr>:#015 79%|███████▉  | 310/393 [00:40<00:13,  5.98ba/s][1,5]<stderr>:#015 79%|███████▉  | 310/393 [00:39<00:12,  6.75ba/s][1,2]<stderr>:#015 81%|████████  | 318/393 [00:40<00:09,  8.09ba/s][1,7]<stderr>:#015 79%|███████▉  | 311/393 [00:40<00:12,  6.66ba/s][1,4]<stderr>:#015 79%|███████▉  | 311/393 [00:39<00:12,  6.44ba/s][1,0]<stderr>:#015 78%|███████▊  | 308/393 [00:40<00:11,  7.69ba/s][1,1]<stderr>:#015 80%|████████  | 316/393 [00:40<00:09,  7.84ba/s][1,6]<stderr>:#015 80%|████████  | 315/393 [00:40<00:10,  7.35ba/s][1,1]<stderr>:#015 81%|████████  | 317/393 [00:40<00:09,  8.29ba/s][1,2]<stderr>:#015 81%|████████  | 319/393 [00:40<00:09,  8.08ba/s][1,0]<stderr>:#015 79%|███████▊  | 309/393 [00:40<00:10,  7.90ba/s][1,3]<stderr>:#015 82%|████████▏ | 321/393 [00:40<00:14,  4.95ba/s][1,7]<stderr>:#015 79%|███████▉  | 312/393 [00:40<00:11,  6.93ba/s][1,4]<stderr>:#015 79%|███████▉  | 312/393 [00:39<00:11,  6.83ba/s][1,5]<stderr>:#015 79%|███████▉  | 312/393 [00:39<00:10,  7.73ba/s][1,6]<stderr>:#015 80%|████████  | 316/393 [00:40<00:09,  7.98ba/s][1,1]<stderr>:#015 81%|████████  | 318/393 [00:40<00:08,  8.46ba/s][1,2]<stderr>:#015 81%|████████▏ | 320/393 [00:40<00:08,  8.19ba/s][1,4]<stderr>:#015 80%|███████▉  | 313/393 [00:40<00:10,  7.36ba/s][1,7]<stderr>:#015 80%|███████▉  | 313/393 [00:40<00:10,  7.39ba/s][1,5]<stderr>:#015 80%|███████▉  | 313/393 [00:40<00:09,  8.00ba/s][1,3]<stderr>:#015 82%|████████▏ | 322/393 [00:40<00:12,  5.55ba/s][1,6]<stderr>:#015 81%|████████  | 318/393 [00:40<00:08,  8.49ba/s][1,4]<stderr>:#015 80%|███████▉  | 314/393 [00:40<00:10,  7.77ba/s][1,0]<stderr>:#015 79%|███████▉  | 310/393 [00:40<00:13,  6.35ba/s][1,3]<stderr>:#015 82%|████████▏ | 323/393 [00:40<00:11,  6.31ba/s][1,1]<stderr>:#015 81%|████████  | 319/393 [00:40<00:09,  8.04ba/s][1,5]<stderr>:#015 80%|███████▉  | 314/393 [00:40<00:09,  8.13ba/s][1,7]<stderr>:#015 80%|███████▉  | 314/393 [00:40<00:11,  6.82ba/s][1,6]<stderr>:#015 81%|████████  | 319/393 [00:40<00:08,  8.46ba/s][1,3]<stderr>:#015 82%|████████▏ | 324/393 [00:40<00:09,  6.99ba/s][1,4]<stderr>:#015 80%|████████  | 315/393 [00:40<00:09,  7.97ba/s][1,5]<stderr>:#015 80%|████████  | 315/393 [00:40<00:09,  8.31ba/s][1,0]<stderr>:#015 79%|███████▉  | 311/393 [00:40<00:12,  6.68ba/s][1,1]<stderr>:#015 81%|████████▏ | 320/393 [00:40<00:09,  7.38ba/s][1,4]<stderr>:#015 80%|████████  | 316/393 [00:40<00:09,  8.20ba/s][1,3]<stderr>:#015 83%|████████▎ | 325/393 [00:41<00:09,  7.24ba/s][1,6]<stderr>:#015 81%|████████▏ | 320/393 [00:40<00:08,  8.21ba/s][1,5]<stderr>:#015 80%|████████  | 316/393 [00:40<00:09,  8.26ba/s][1,2]<stderr>:#015 82%|████████▏ | 321/393 [00:40<00:14,  5.10ba/s][1,7]<stderr>:#015 80%|████████  | 316/393 [00:40<00:10,  7.63ba/s][1,0]<stderr>:#015 79%|███████▉  | 312/393 [00:40<00:11,  6.92ba/s][1,2]<stderr>:#015 82%|████████▏ | 322/393 [00\u001b[0m\n",
      "\u001b[34m:40<00:12,  5.82ba/s][1,4]<stderr>:#015 81%|████████  | 317/393 [00:40<00:09,  7.96ba/s][1,7]<stderr>:#015 81%|████████  | 317/393 [00:40<00:09,  7.95ba/s][1,5]<stderr>:#015 81%|████████  | 317/393 [00:40<00:09,  8.24ba/s][1,0]<stderr>:#015 80%|███████▉  | 313/393 [00:40<00:10,  7.45ba/s][1,7]<stderr>:#015 81%|████████  | 318/393 [00:40<00:08,  8.47ba/s][1,4]<stderr>:#015 81%|████████  | 318/393 [00:40<00:08,  8.34ba/s][1,1]<stderr>:#015 82%|████████▏ | 321/393 [00:40<00:13,  5.25ba/s][1,3]<stderr>:#015 83%|████████▎ | 326/393 [00:41<00:11,  5.84ba/s][1,2]<stderr>:#015 82%|████████▏ | 324/393 [00:41<00:10,  6.54ba/s][1,0]<stderr>:#015 80%|████████  | 315/393 [00:41<00:09,  7.90ba/s][1,5]<stderr>:#015 81%|████████  | 319/393 [00:40<00:08,  8.44ba/s][1,1]<stderr>:#015 82%|████████▏ | 322/393 [00:41<00:11,  6.05ba/s][1,7]<stderr>:#015 81%|████████  | 319/393 [00:41<00:09,  8.14ba/s][1,6]<stderr>:#015 82%|████████▏ | 321/393 [00:41<00:14,  5.06ba/s][1,4]<stderr>:#015 81%|████████  | 319/393 [00:40<00:09,  7.59ba/s][1,2]<stderr>:#015 83%|████████▎ | 325/393 [00:41<00:09,  6.94ba/s][1,7]<stderr>:#015 81%|████████▏ | 320/393 [00:41<00:08,  8.50ba/s][1,3]<stderr>:#015 83%|████████▎ | 328/393 [00:41<00:09,  6.54ba/s][1,5]<stderr>:#015 81%|████████▏ | 320/393 [00:40<00:09,  8.09ba/s][1,1]<stderr>:#015 82%|████████▏ | 323/393 [00:41<00:11,  6.33ba/s][1,6]<stderr>:#015 82%|████████▏ | 322/393 [00:41<00:12,  5.74ba/s][1,0]<stderr>:#015 80%|████████  | 316/393 [00:41<00:10,  7.45ba/s][1,1]<stderr>:#015 82%|████████▏ | 324/393 [00:41<00:09,  6.97ba/s][1,3]<stderr>:#015 84%|████████▎ | 329/393 [00:41<00:09,  6.72ba/s][1,6]<stderr>:#015 82%|████████▏ | 323/393 [00:41<00:10,  6.42ba/s][1,0]<stderr>:#015 81%|████████  | 317/393 [00:41<00:09,  7.91ba/s][1,2]<stderr>:#015 83%|████████▎ | 326/393 [00:41<00:11,  6.05ba/s][1,6]<stderr>:#015 82%|████████▏ | 324/393 [00:41<00:09,  6.91ba/s][1,0]<stderr>:#015 81%|████████  | 318/393 [00:41<00:09,  7.88ba/s][1,3]<stderr>:#015 84%|████████▍ | 331/393 [00:41<00:08,  7.35ba/s][1,6]<stderr>:#015 83%|████████▎ | 325/393 [00:41<00:08,  7.60ba/s][1,2]<stderr>:#015 83%|████████▎ | 328/393 [00:41<00:09,  7.16ba/s][1,4]<stderr>:#015 82%|████████▏ | 321/393 [00:41<00:11,  6.17ba/s][1,5]<stderr>:#015 82%|████████▏ | 321/393 [00:41<00:14,  4.96ba/s][1,7]<stderr>:#015 82%|████████▏ | 321/393 [00:41<00:14,  4.85ba/s][1,0]<stderr>:#015 81%|████████▏ | 320/393 [00:41<00:08,  8.54ba/s][1,1]<stderr>:#015 83%|████████▎ | 326/393 [00:41<00:10,  6.61ba/s][1,3]<stderr>:#015 84%|████████▍ | 332/393 [00:41<00:08,  7.42ba/s][1,2]<stderr>:#015 84%|████████▎ | 329/393 [00:41<00:08,  7.56ba/s][1,6]<stderr>:#015 83%|████████▎ | 326/393 [00:41<00:08,  7.66ba/s][1,5]<stderr>:#015 82%|████████▏ | 322/393 [00:41<00:12,  5.84ba/s][1,4]<stderr>:#015 82%|████████▏ | 322/393 [00:41<00:10,  6.75ba/s][1,6]<stderr>:#015 83%|████████▎ | 327/393 [00:41<00:08,  7.98ba/s][1,3]<stderr>:#015 85%|████████▍ | 333/393 [00:42<00:07,  7.51ba/s][1,7]<stderr>:#015 82%|████████▏ | 323/393 [00:41<00:12,  5.72ba/s][1,2]<stderr>:#015 84%|████████▍ | 330/393 [00:41<00:08,  7.60ba/s][1,1]<stderr>:#015 83%|████████▎ | 327/393 [00:41<00:09,  6.64ba/s][1,5]<stderr>:#015 82%|████████▏ | 323/393 [00:41<00:11,  5.97ba/s][1,6]<stderr>:#015 83%|████████▎ | 328/393 [00:41<00:07,  8.23ba/s][1,1]<stderr>:#015 83%|████████▎ | 328/393 [00:41<00:08,  7.31ba/s][1,3]<stderr>:#015 85%|████████▍ | 334/393 [00:42<00:07,  7.70ba/s][1,4]<stderr>:#015 82%|████████▏ | 324/393 [00:41<00:09,  7.22ba/s][1,2]<stderr>:#015 84%|████████▍ | 331/393 [00:41<00:07,  7.79ba/s][1,7]<stderr>:#015 82%|████████▏ | 324/393 [00:41<00:11,  6.21ba/s][1,5]<stderr>:#015 82%|████████▏ | 324/393 [00:41<00:10,  6.75ba/s][1,0]<stderr>:#015 82%|████████▏ | 321/393 [00:42<00:12,  5.56ba/s][1,3]<stderr>:#015 85%|████████▌ | 335/393 [00:42<00:07,  7.91ba/s][1,4]<stderr>:#015 83%|████████▎ | 325/393 [00:41<00:09,  7.53ba/s][1,2]<stderr>:#015 84%|████████▍ | 332/393 [00:42<00:07,  7.90ba/s][1,1]<stderr>:#015 84%|████████▎ | 329/393 [00:42<00:08,  7.17ba/s][1,7]<stderr>:#015 83%|████████▎ | 325/393 [00:42<00:10,  6.51ba/s][1,6]<stderr>:#015 84%|████████▎ | 329/393 [00:42<00:08,  7.31ba/s][1,0]<stderr>:#015 82%|████████▏ | 323/393 [00:42<00:10,  6.43ba/s][1,2]<stderr>:#015 85%|████████▍ | 333/393 [00:42<00:07,  7.63ba/s][1,6]<stderr>:#015 84%|████████▍ | 330/393 [00:42<00:08,  7.70ba/s][1,3]<stderr>:#015 85%|████████▌ | 336/393 [00:42<00:07,  7.43ba/s][1,1]<stderr>:#015 84%|████████▍ | 331/393 [00:42<00:08,  7.74ba/s][1,4]<stderr>:#015 83%|████████▎ | 326/393 [00:41<00:10,  6.16ba/s][1,5]<stderr>:#015 83%|████████▎ | 326/393 [00:41<00:10,  6.30ba/s][1,7]<stderr>:#015 83%|████████▎ | 326/393 [00:42<00:12,  5.39ba/s][1,6]<stderr>:#015 84%|████████▍ | 332/393 [00:42<00:07,  8.41ba/s][1,2]<stderr>:#015 85%|████████▌ | 335/393 [00:42<00:06,  8.29ba/s][1,1]<stderr>:#015 84%|████████▍ | 332/393 [00:42<00:07,  7.77ba/s][1,3]<stderr>:#015 86%|████████▌ | 338/393 [00:42<00:06,  7.99ba/s][1,0]<stderr>:#015 83%|████████▎ | 325/393 [00:42<00:09,  6.96ba/s][1,4]<stderr>:#015 83%|████████▎ | 327/393 [00:42<00:10,  6.52ba/s][1,7]<stderr>:#015 83%|████████▎ | 327/393 [00:42<00:10,  6.25ba/s][1,5]<stderr>:#015 83%|████████▎ | 327/393 [00:42<00:09,  6.74ba/s][1,4]<stderr>:#015 83%|████████▎ | 328/393 [00:42<00:09,  7.00ba/s][1,2]<stderr>:#015 85%|████████▌ | 336/393 [00:42<00:07,  7.78ba/s][1,7]<stderr>:#015 83%|████████▎ | 328/393 [00:42<00:09,  7.01ba/s][1,3]<stderr>:#015 86%|████████▋ | 339/393 [00:42<00:06,  7.78ba/s][1,1]<stderr>:#015 85%|████████▍ | 333/393 [00:42<00:07,  7.57ba/s][1,6]<stderr>:#015 85%|████████▍ | 334/393 [00:42<00:06,  9.18ba/s][1,3]<stderr>:#015 87%|████████▋ | 340/393 [00:42<00:06,  8.11ba/s][1,4]<stderr>:#015 84%|████████▎ | 329/393 [00:42<00:08,  7.30ba/s][1,5]<stderr>:#015 84%|████████▎ | 329/393 [00:42<00:08,  7.41ba/s][1,1]<stderr>:#015 85%|████████▍ | 334/393 [00:42<00:07,  7.92ba/s][1,7]<stderr>:#015 84%|████████▎ | 329/393 [00:42<00:09,  7.02ba/s][1,2]<stderr>:#015 86%|████████▌ | 337/393 [00:42<00:07,  7.35ba/s][1,0]<stderr>:#015 83%|████████▎ | 326/393 [00:42<00:13,  4.97ba/s][1,3]<stderr>:#015 87%|████████▋ | 341/393 [00:43<00:06,  8.29ba/s][1,7]<stderr>:#015 84%|████████▍ | 330/393 [00:42<00:08,  7.58ba/s][1,4]<stderr>:#015 84%|████████▍ | 330/393 [00:42<00:08,  7.42ba/s][1,5]<stderr>:#015 84%|████████▍ | 330/393 [00:42<00:08,  7.48ba/s][1,6]<stderr>:#015 85%|████████▌ | 336/393 [00:42<00:06,  8.81ba/s][1,1]<stderr>:#015 85%|████████▌ | 335/393 [00:42<00:07,  7.62ba/s][1,2]<stderr>:#015 86%|████████▌ | 338/393 [00:42<00:07,  7.67ba/s][1,7]<stderr>:#015 84%|████████▍ | 331/393 [00:42<00:07,  7.83ba/s][1,5]<stderr>:#015 84%|████████▍ | 331/393 [00:42<00:08,  7.69ba/s][1,1]<stderr>:#015 85%|████████▌ | 336/393 [00:42<00:07,  7.97ba/s][1,6]<stderr>:#015 86%|████████▌ | 337/393 [00:42<00:06,  8.53ba/s][1,4]<stderr>:#015 84%|████████▍ | 331/393 [00:42<00:08,  7.16ba/s][1,0]<stderr>:#015 83%|████████▎ | 328/393 [00:42<00:11,  5.90ba/s][1,2]<stderr>:#015 86%|████████▋ | 339/393 [00:42<00:07,  7.36ba/s][1,7]<stderr>:#015 84%|████████▍ | 332/393 [00:43<00:07,  8.06ba/s][1,6]<stderr>:#015 86%|████████▌ | 338/393 [00:43<00:06,  8.88ba/s][1,5]<stderr>:#015 84%|████████▍ | 332/393 [00:42<00:07,  7.83ba/s][1,1]<stderr>:#015 86%|████████▌ | 337/393 [00:43<00:06,  8.14ba/s][1,0]<stderr>:#015 84%|████████▎ | 329/393 [00:43<00:09,  6.71ba/s][1,3]<stderr>:#015 87%|████████▋ | 343/393 [00:43<00:06,  7.71ba/s][1,4]<stderr>:#015 85%|████████▍ | 333/393 [00:42<00:07,  7.73ba/s][1,2]<stderr>:#015 87%|████████▋ | 341/393 [00:43<00:06,  8.03ba/s][1,5]<stderr>:#015 85%|████████▍ | 333/393 [00:42<00:07,  8.02ba/s][1,0]<stderr>:#015 84%|████████▍ | 330/393 [00:43<00:08,  7.18ba/s][1,6]<stderr>:#015 86%|████████▋ | 339/393 [00:43<00:06,  8.55ba/s][1,1]<stderr>:#015 86%|████████▌ | 338/393 [00:43<00:07,  7.53ba/s][1,7]<stderr>:#015 85%|████████▍ | 334/393 [00:43<00:07,  8.39ba/s][1,4]<stderr>:#015 85%|████████▍ | 334/393 [00:42<00:07,  7.99ba/s][1,6]<stderr>:#015 87%|████████▋ | 340/393 [00:43<00:06,  8.75ba/s][1,2]<stderr>:#015 87%|████████▋ | 342/393 [00:43<00:06,  8.19ba/s][1,5]<stderr>:#015 85%|████████▍ | 334/393 [00:42<00:07,  8.18ba/s][1,0]<stderr>:#015 84%|████████▍ | 331/393 [00:43<00:08,  7.39ba/s][1,3]<stderr>:#015 88%|████████▊ | 345/393 [00:43<00:05,  8.19ba/s][1,1]<stderr>:#015 86%|████████▋ | 339/393 [00:43<00:07,  7.56ba/s][1,7]<stderr>:#015 85%|████████▌ | 335/393 [00:43<00:06,  8.69ba/s][1,4]<stderr>:#015 85%|████████▌ | 335/393 [00:43<00:06,  8.32ba/s][1,6]<stderr>:#015 87%|████████▋ | 341/393 [00:43<00:05,  8.88ba/s][1,3]<stderr>:#015 88%|████████▊ | 346/393 [00:43<00:05,  8.53ba/s][1,0]<stderr>:#015 84%|████████▍ | 332/393 [00:43<00:07,  7.80ba/s][1,5]<stderr>:#015 85%|████████▌ | 335/393 [00:43<00:07,  7.72ba/s][1,1]<stderr>:#015 87%|████████▋ | 340/393 [00:43<00:06,  7.84ba/s][1,7]<stderr>:#015 85%|████████▌ | 336/393 [00:43<00:06,  9.02ba/s][1,2]<stderr>:#015 87%|████████▋ | 343/393 [00:43<00:07,  6.92ba/s][1,4]<stderr>:#015 85%|████████▌ | 336/393 [00:43<00:06,  8.35ba/s][1,5]<stderr>:#015 85%|████████▌ | 336/393 [00:43<00:06,  8.22ba/s][1,6]<stderr>:#015 87%|████████▋ | 342/393 [00:43<00:06,  7.70ba/s][1,7]<stderr>:#015 86%|████████▌ | 337/393 [00:43<00:06,  8.81ba/s][1,1]<stderr>:#015 87%|████████▋ | 341/393 [00:43<00:06,  7.72ba/s][1,3]<stderr>:#015 89%|████████▊ | 348/393 [00:43<00:04,  9.07ba/s][1,2]<stderr>:#015 88%|████████▊ | 344/393 [00:43<00:06,  7.33ba/s][1,0]<stderr>:#015 85%|████████▍ | 334/393 [00:43<00:07,  8.37ba/s][1,4]<stderr>:#015 86%|████████▌ | 337/393 [00:43<00:06,  8.34ba/s][1,6]<stderr>:#015 87%|████████▋ | 343/393 [00:43<00:06,  7.40ba/s][1,3]<stderr>:#015 89%|████████▉ | 349/393 [00:43<00:04,  8.82ba/s][1,1]<stderr>:#015 87%|████████▋ | 342/393 [00:43<00:06,  7.77ba/s][1,7]<stderr>:#015 86%|████████▌ | 338/393 [00:43<00:06,  8.29ba/s][1,5]<stderr>:#015 86%|████████▌ | 338/393 [00:43<00:06,  8.88ba/s][1,0]<stderr>:#015 85%|████████▌ | 335/393 [00:43<00:06,  8.55ba/s][1,2]<stderr>:#015 88%|████████▊ | 345/393 [00:43<00:06,  7.52ba/s][1,3]<stderr>:#015 89%|████████▉ | 350/393 [00:44<00:05,  8.57ba/s][1,7]<stderr>:#015 86%|████████▋ | 339/393 [00:43<00:06,  8.25ba/s][1,5]<stderr>:#015 86%|████████▋ | 339/393 [00:43<00:06,  8.59ba/s][1,4]<stderr>:#015 86%|████████▋ | 339/393 [00:43<00:06,  8.57ba/s][1,0]<stderr>:#015 85%|████████▌ | 336/393 [00:43<00:06,  8.32ba/s][1,2]<stderr>:#015 88%|████████▊ | 346/393 [00:43<00:06,  7.54ba/s][1,6]<stderr>:#015 88%|████████▊ | 344/393 [00:43<00:06,  7.21ba/s][1,4]<stderr>:#015 87%|████████▋ | 340/393 [00:43<00:06,  8.83ba/s][1,0]<stderr>:#015 86%|████████▌ | 337/393 [00:43<00:06,  8.60ba/s][1,5]<stderr>:#015 87%|████████▋ | 340/393 [00:43<00:06,  8.53ba/s][1,3]<stderr>:#015 89%|████████▉ | 351/393 [00:44<00:05,  8.38ba/s][1,1]<stderr>:#015 87%|████████▋ | 343/393 [00:43<00:08,  5.98ba/s][1,2]<stderr>:#015 88%|████████▊ | 347/393 [00:43<00:05,  7.73ba/s][1,7]<stderr>:#015 87%|████████▋ | 341/393 [00:44<00:05,  8.75ba/s][1,4]<stderr>:#015 87%|████████▋ | 341/393 [00:43<00:06,  8.58ba/s][1,6]<stderr>:#015 88%|████████▊ | 345/393 [00:44<00:07,  6.12ba/s][1,5]<stderr>:#015 87%|████████▋ | 341/393 [00:43<00:06,  8.56ba/s][1,0]<stderr>:#015 86%|████████▌ | 338/393 [00:44<00:06,  8.42ba/s][1,1]<stderr>:#015 88%|████████▊ | 344/393 [00:44<00:07,  6.69ba/s][1,2]<stderr>:#015 89%|████████▊ | 348/393 [00:44<00:05,  8.20ba/s][1,3]<stderr>:#015 90%|████████▉ | 352/393 [00:44<00:04,  8.38ba/s][1,2]<stderr>:#015 89%|████████▉ | 349/393 [00:44<00:05,  8.35ba/s][1,6]<stderr>:#015 88%|████████▊ | 346/393 [00:44<00:07,  6.61ba/s][1,4]<stderr>:#015 87%|████████▋ | 342/393 [00:43<00:06,  8.39ba/s][1,3]<stderr>:#015 90%|████████▉ | 353/393 [00:44<00:04,  8.18ba/s][1,5]<stderr>:#015 87%|████████▋ | 342/393 [00:43<00:06,  8.17ba/s][1,1]<stderr>:#015 88%|████████▊ | 345/393 [00:44<00:06,  6.91ba/s][1,0]<stderr>:#015 86%|████████▋ | 339/393 [00:44<00:06,  7.94ba/s][1,7]<stderr>:#015 87%|████████▋ | 343/393 [00:44<00:06,  8.22ba/s][1,1]<stderr>:#015 88%|████████▊ | 346/393 [00:44<00:06,  7.44ba/s][1,0]<stderr>:#015 87%|████████▋ | 340/393 [00:44<00:06,  8.35ba/s][1,6]<stderr>:#015 88%|████████▊ | 347/393 [00:44<00:06,  6.89ba/s][1,2]<stderr>:#015 89%|████████▉ | 350/393 [00:44<00:05,  8.04ba/s][1,3]<stderr>:#015 90%|█████████ | 354/393 [00:44<00:04,  8.18ba/s][1,3]<stderr>:#015 90%|█████████ | 355/393 [00:44<00:04,  8.52ba/s][1,6]<stderr>:#015 89%|████████▊ | 348/393 [00:44<00:06,  7.30ba/s][1,2]<stderr>:#015 89%|████████▉ | 351/393 [00:44<00:05,  8.15ba/s][1,4]<stderr>:#015 87%|████████▋ | 343/393 [00:44<00:08,  6.15ba/s][1,5]<stderr>:#015 87%|████████▋ | 343/393 [00:44<00:08,  5.64ba/s][1,1]<stderr>:#015 89%|████████▊ | 348/393 [00:44<00:05,  8.10ba/s][1,0]<stderr>:#015 87%|████████▋ | 342/393 [00:44<00:05,  8.55ba/s][1,7]<stderr>:#015 88%|████████▊ | 345/393 [00:44<00:05,  8.28ba/s][1,3]<stderr>:#015 91%|█████████ | 356/393 [00:44<00:04,  8.42ba/s][1,6]<stderr>:#015 89%|████████▉ | 349/393 [00:44<00:05,  7.70ba/s][1,4]<stderr>:#015 88%|████████▊ | 344/393 [00:44<00:07,  6.93ba/s][1,2]<stderr>:#015 90%|████████▉ | 352/393 [00:44<00:04,  8.22ba/s][1,1]<stderr>:#015 89%|████████▉ | 349/393 [00:44<00:05,  8.25ba/s][1,4]<stderr>:#015 88%|████████▊ | 345/393 [00:44<00:06,  7.46ba/s][1,3]<stderr>:#015 91%|█████████ | 357/393 [00:44<00:04,  8.42ba/s][1,7]<stderr>:#015 88%|████████▊ | 346/393 [00:44<00:05,\n",
      "  7.92ba/s][1,5]<stderr>:#015 88%|████████▊ | 345/393 [00:44<00:07,  6.50ba/s][1,6]<stderr>:#015 89%|████████▉ | 350/393 [00:44<00:05,  7.27ba/s][1,2]<stderr>:#015 90%|████████▉ | 353/393 [00:44<00:05,  7.62ba/s][1,0]<stderr>:#015 87%|████████▋ | 343/393 [00:44<00:07,  6.98ba/s][1,4]<stderr>:#015 88%|████████▊ | 346/393 [00:44<00:05,  7.85ba/s][1,3]<stderr>:#015 91%|█████████ | 358/393 [00:45<00:04,  8.45ba/s][1,5]<stderr>:#015 88%|████████▊ | 346/393 [00:44<00:06,  7.26ba/s][1,7]<stderr>:#015 88%|████████▊ | 347/393 [00:44<00:05,  7.78ba/s][1,6]<stderr>:#015 89%|████████▉ | 351/393 [00:44<00:05,  7.46ba/s][1,1]<stderr>:#015 89%|████████▉ | 351/393 [00:44<00:05,  8.37ba/s][1,2]<stderr>:#015 90%|█████████ | 354/393 [00:44<00:05,  7.21ba/s][1,4]<stderr>:#015 88%|████████▊ | 347/393 [00:44<00:05,  8.14ba/s][1,0]<stderr>:#015 88%|████████▊ | 345/393 [00:44<00:06,  7.74ba/s][1,5]<stderr>:#015 88%|████████▊ | 347/393 [00:44<00:06,  7.33ba/s][1,7]<stderr>:#015 89%|████████▊ | 348/393 [00:44<00:05,  7.70ba/s][1,6]<stderr>:#015 90%|████████▉ | 352/393 [00:44<00:05,  7.51ba/s][1,1]<stderr>:#015 90%|████████▉ | 352/393 [00:44<00:04,  8.59ba/s][1,4]<stderr>:#015 89%|████████▊ | 348/393 [00:44<00:05,  8.34ba/s][1,5]<stderr>:#015 89%|████████▊ | 348/393 [00:44<00:05,  7.89ba/s][1,3]<stderr>:#015 91%|█████████▏| 359/393 [00:45<00:05,  6.18ba/s][1,6]<stderr>:#015 90%|████████▉ | 353/393 [00:45<00:05,  7.96ba/s][1,2]<stderr>:#015 91%|█████████ | 356/393 [00:45<00:04,  7.82ba/s][1,7]<stderr>:#015 89%|████████▉ | 349/393 [00:45<00:05,  7.54ba/s][1,0]<stderr>:#015 88%|████████▊ | 346/393 [00:45<00:06,  7.37ba/s][1,1]<stderr>:#015 90%|████████▉ | 353/393 [00:45<00:04,  8.36ba/s][1,4]<stderr>:#015 89%|████████▉ | 349/393 [00:44<00:05,  8.60ba/s][1,3]<stderr>:#015 92%|█████████▏| 360/393 [00:45<00:04,  6.70ba/s][1,5]<stderr>:#015 89%|████████▉ | 349/393 [00:44<00:05,  7.68ba/s][1,2]<stderr>:#015 91%|█████████ | 357/393 [00:45<00:04,  7.86ba/s][1,1]<stderr>:#015 90%|█████████ | 354/393 [00:45<00:04,  8.60ba/s][1,6]<stderr>:#015 90%|█████████ | 354/393 [00:45<00:04,  7.90ba/s][1,7]<stderr>:#015 89%|████████▉ | 350/393 [00:45<00:05,  7.55ba/s][1,4]<stderr>:#015 89%|████████▉ | 350/393 [00:44<00:04,  8.62ba/s][1,0]<stderr>:#015 88%|████████▊ | 347/393 [00:45<00:06,  7.27ba/s][1,6]<stderr>:#015 90%|█████████ | 355/393 [00:45<00:04,  8.17ba/s][1,2]<stderr>:#015 91%|█████████ | 358/393 [00:45<00:04,  7.89ba/s][1,5]<stderr>:#015 89%|████████▉ | 350/393 [00:44<00:05,  7.34ba/s][1,3]<stderr>:#015 92%|█████████▏| 362/393 [00:45<00:04,  7.70ba/s][1,1]<stderr>:#015 90%|█████████ | 355/393 [00:45<00:04,  8.12ba/s][1,0]<stderr>:#015 89%|████████▊ | 348/393 [00:45<00:05,  7.62ba/s][1,7]<stderr>:#015 89%|████████▉ | 351/393 [00:45<00:05,  7.59ba/s][1,4]<stderr>:#015 89%|████████▉ | 351/393 [00:45<00:04,  8.42ba/s][1,6]<stderr>:#015 91%|█████████ | 356/393 [00:45<00:04,  8.16ba/s][1,5]<stderr>:#015 89%|████████▉ | 351/393 [00:45<00:05,  7.73ba/s][1,3]<stderr>:#015 92%|█████████▏| 363/393 [00:45<00:03,  7.92ba/s][1,7]<stderr>:#015 90%|████████▉ | 352/393 [00:45<00:05,  7.92ba/s][1,0]<stderr>:#015 89%|████████▉ | 349/393 [00:45<00:05,  7.86ba/s][1,4]<stderr>:#015 90%|████████▉ | 352/393 [00:45<00:04,  8.31ba/s][1,1]<stderr>:#015 91%|█████████ | 356/393 [00:45<00:04,  7.58ba/s][1,6]<stderr>:#015 91%|█████████ | 357/393 [00:45<00:04,  8.48ba/s][1,0]<stderr>:#015 89%|████████▉ | 350/393 [00:45<00:05,  8.31ba/s][1,3]<stderr>:#015 93%|█████████▎| 364/393 [00:45<00:03,  8.14ba/s][1,5]<stderr>:#015 90%|████████▉ | 352/393 [00:45<00:05,  7.66ba/s][1,7]<stderr>:#015 90%|████████▉ | 353/393 [00:45<00:05,  7.99ba/s][1,2]<stderr>:#015 91%|█████████▏| 359/393 [00:45<00:05,  5.99ba/s][1,4]<stderr>:#015 90%|████████▉ | 353/393 [00:45<00:04,  8.44ba/s][1,0]<stderr>:#015 89%|████████▉ | 351/393 [00:45<00:04,  8.58ba/s][1,5]<stderr>:#015 90%|████████▉ | 353/393 [00:45<00:05,  7.82ba/s][1,2]<stderr>:#015 92%|█████████▏| 360/393 [00:45<00:05,  6.57ba/s][1,1]<stderr>:#015 91%|█████████ | 358/393 [00:45<00:04,  8.06ba/s][1,4]<stderr>:#015 90%|█████████ | 354/393 [00:45<00:04,  8.51ba/s][1,7]<stderr>:#015 90%|█████████ | 354/393 [00:45<00:04,  8.02ba/s][1,3]<stderr>:#015 93%|█████████▎| 365/393 [00:46<00:03,  7.82ba/s][1,6]<stderr>:#015 91%|█████████▏| 359/393 [00:45<00:03,  9.13ba/s][1,2]<stderr>:#015 92%|█████████▏| 361/393 [00:45<00:04,  6.94ba/s][1,7]<stderr>:#015 90%|█████████ | 355/393 [00:45<00:04,  7.99ba/s][1,5]<stderr>:#015 90%|█████████ | 354/393 [00:45<00:05,  7.72ba/s][1,3]<stderr>:#015 93%|█████████▎| 366/393 [00:46<00:03,  7.91ba/s][1,4]<stderr>:#015 90%|█████████ | 355/393 [00:45<00:04,  8.14ba/s][1,6]<stderr>:#015 92%|█████████▏| 360/393 [00:45<00:03,  8.36ba/s][1,0]<stderr>:#015 90%|████████▉ | 353/393 [00:45<00:04,  9.03ba/s][1,1]<stderr>:#015 91%|█████████▏| 359/393 [00:45<00:05,  6.26ba/s][1,7]<stderr>:#015 91%|█████████ | 356/393 [00:45<00:04,  8.00ba/s][1,5]<stderr>:#015 90%|█████████ | 355/393 [00:45<00:04,  7.79ba/s][1,2]<stderr>:#015 92%|█████████▏| 362/393 [00:45<00:04,  7.13ba/s][1,3]<stderr>:#015 93%|█████████▎| 367/393 [00:46<00:03,  7.87ba/s][1,4]<stderr>:#015 91%|█████████ | 356/393 [00:45<00:04,  8.08ba/s][1,6]<stderr>:#015 92%|█████████▏| 361/393 [00:45<00:03,  8.76ba/s][1,0]<stderr>:#015 90%|█████████ | 354/393 [00:46<00:04,  8.41ba/s][1,7]<stderr>:#015 91%|█████████ | 357/393 [00:46<00:04,  8.23ba/s][1,2]<stderr>:#015 92%|█████████▏| 363/393 [00:46<00:03,  7.53ba/s][1,3]<stderr>:#015 94%|█████████▎| 368/393 [00:46<00:03,  8.04ba/s][1,4]<stderr>:#015 91%|█████████ | 357/393 [00:45<00:04,  8.19ba/s][1,5]<stderr>:#015 91%|█████████ | 356/393 [00:45<00:04,  7.79ba/s][1,1]<stderr>:#015 92%|█████████▏| 360/393 [00:46<00:05,  6.29ba/s][1,7]<stderr>:#015 91%|█████████ | 358/393 [00:46<00:04,  8.40ba/s][1,6]<stderr>:#015 92%|█████████▏| 362/393 [00:46<00:04,  6.78ba/s][1,2]<stderr>:#015 93%|█████████▎| 364/393 [00:46<00:03,  7.78ba/s][1,5]<stderr>:#015 91%|█████████ | 357/393 [00:45<00:04,  8.19ba/s][1,3]<stderr>:#015 94%|█████████▍| 369/393 [00:46<00:02,  8.25ba/s][1,0]<stderr>:#015 91%|█████████ | 356/393 [00:46<00:04,  8.93ba/s][1,4]<stderr>:#015 91%|█████████ | 358/393 [00:45<00:04,  8.34ba/s][1,0]<stderr>:#015 91%|█████████ | 357/393 [00:46<00:04,  8.80ba/s][1,3]<stderr>:#015 94%|█████████▍| 370/393 [00:46<00:02,  8.14ba/s][1,5]<stderr>:#015 91%|█████████ | 358/393 [00:45<00:04,  8.07ba/s][1,6]<stderr>:#015 92%|█████████▏| 363/393 [00:46<00:04,  7.01ba/s][1,1]<stderr>:#015 92%|█████████▏| 362/393 [00:46<00:04,  6.92ba/s][1,2]<stderr>:#015 93%|█████████▎| 365/393 [00:46<00:03,  7.71ba/s][1,6]<stderr>:#015 93%|█████████▎| 364/393 [00:46<00:03,  7.63ba/s][1,1]<stderr>:#015 92%|█████████▏| 363/393 [00:46<00:04,  7.44ba/s][1,4]<stderr>:#015 91%|█████████▏| 359/393 [00:46<00:05,  6.05ba/s][1,7]<stderr>:#015 91%|█████████▏| 359/393 [00:46<00:05,  5.80ba/s][1,3]<stderr>:#015 95%|█████████▍| 372/393 [00:46<00:02,  8.59ba/s][1,6]<stderr>:#015 93%|█████████▎| 365/393 [00:46<00:03,  7.94ba/s][1,2]<stderr>:#015 93%|█████████▎| 367/393 [00:46<00:03,  7.95ba/s][1,5]<stderr>:#015 91%|█████████▏| 359/393 [00:46<00:05,  6.26ba/s][1,0]<stderr>:#015 91%|█████████▏| 359/393 [00:46<00:04,  8.19ba/s][1,1]<stderr>:#015 93%|█████████▎| 365/393 [00:46<00:03,  8.27ba/s][1,3]<stderr>:#015 95%|█████████▍| 373/393 [00:46<00:02,  8.82ba/s][1,2]<stderr>:#015 94%|█████████▎| 368/393 [00:46<00:03,  8.10ba/s][1,6]<stderr>:#015 93%|█████████▎| 366/393 [00:46<00:03,  7.75ba/s][1,5]<stderr>:#015 92%|█████████▏| 360/393 [00:46<00:04,  6.74ba/s][1,4]<stderr>:#015 92%|█████████▏| 361/393 [00:46<00:04,  6.54ba/s][1,7]<stderr>:#015 92%|█████████▏| 361/393 [00:46<00:05,  6.29ba/s][1,3]<stderr>:#015 95%|█████████▌| 374/393 [00:47<00:02,  8.74ba/s][1,6]<stderr>:#015 93%|█████████▎| 367/393 [00:46<00:03,  8.22ba/s][1,0]<stderr>:#015 92%|█████████▏| 361/393 [00:46<00:03,  8.65ba/s][1,2]<stderr>:#015 94%|█████████▍| 369/393 [00:46<00:02,  8.11ba/s][1,1]<stderr>:#015 93%|█████████▎| 367/393 [00:46<00:03,  8.66ba/s][1,5]<stderr>:#015 92%|█████████▏| 361/393 [00:46<00:04,  6.92ba/s][1,4]<stderr>:#015 92%|█████████▏| 362/393 [00:46<00:04,  6.95ba/s][1,2]<stderr>:#015 94%|█████████▍| 370/393 [00:46<00:02,  7.97ba/s][1,0]<stderr>:#015 92%|█████████▏| 362/393 [00:46<00:03,  8.31ba/s][1,6]<stderr>:#015 94%|█████████▎| 368/393 [00:46<00:03,  7.68ba/s][1,1]<stderr>:#015 94%|█████████▎| 368/393 [00:46<00:02,  8.55ba/s][1,5]<stderr>:#015 92%|█████████▏| 362/393 [00:46<00:04,  7.21ba/s][1,4]<stderr>:#015 92%|█████████▏| 363/393 [00:46<00:03,  7.53ba/s][1,7]<stderr>:#015 92%|█████████▏| 363/393 [00:46<00:04,  6.84ba/s][1,0]<stderr>:#015 92%|█████████▏| 363/393 [00:47<00:03,  8.36ba/s][1,1]<stderr>:#015 94%|█████████▍| 369/393 [00:47<00:02,  8.72ba/s][1,6]<stderr>:#015 94%|█████████▍| 369/393 [00:47<00:03,  7.89ba/s][1,3]<stderr>:#015 96%|█████████▌| 376/393 [00:47<00:02,  7.87ba/s][1,5]<stderr>:#015 92%|█████████▏| 363/393 [00:46<00:03,  7.60ba/s][1,4]<stderr>:#015 93%|█████████▎| 364/393 [00:46<00:03,  7.68ba/s][1,7]<stderr>:#015 93%|█████████▎| 364/393 [00:47<00:03,  7.47ba/s][1,2]<stderr>:#015 94%|█████████▍| 371/393 [00:47<00:02,  7.47ba/s][1,0]<stderr>:#015 93%|█████████▎| 364/393 [00:47<00:03,  8.19ba/s][1,5]<stderr>:#015 93%|█████████▎| 364/393 [00:46<00:03,  7.84ba/s][1,6]<stderr>:#015 94%|█████████▍| 370/393 [00:47<00:02,  7.81ba/s][1,3]<stderr>:#015 96%|█████████▌| 377/393 [00:47<00:02,  7.89ba/s][1,7]<stderr>:#015 93%|█████████▎| 365/393 [00:47<00:03,  7.55ba/s][1,1]<stderr>:#015 94%|█████████▍| 370/393 [00:47<00:02,  7.90ba/s][1,4]<stderr>:#015 93%|█████████▎| 365/393 [00:46<00:03,  7.58ba/s][1,2]<stderr>:#015 95%|█████████▍| 372/393 [00:47<00:02,  7.57ba/s][1,6]<stderr>:#015 94%|█████████▍| 371/393 [00:47<00:02,  7.99ba/s][1,3]<stderr>:#015 96%|█████████▌| 378/393 [00:47<00:01,  8.06ba/s][1,5]<stderr>:#015 93%|█████████▎| 365/393 [00:46<00:03,  7.80ba/s][1,0]<stderr>:#015 93%|█████████▎| 365/393 [00:47<00:03,  7.91ba/s][1,4]<stderr>:#015 93%|█████████▎| 366/393 [00:47<00:03,  8.08ba/s][1,1]<stderr>:#015 94%|█████████▍| 371/393 [00:47<00:02,  8.20ba/s][1,7]<stderr>:#015 93%|█████████▎| 366/393 [00:47<00:03,  7.88ba/s][1,2]<stderr>:#015 95%|█████████▍| 373/393 [00:47<00:02,  8.09ba/s][1,3]<stderr>:#015 96%|█████████▋| 379/393 [00:47<00:01,  8.16ba/s][1,4]<stderr>:#015 93%|█████████▎| 367/393 [00:47<00:03,  8.23ba/s][1,6]<stderr>:#015 95%|█████████▍| 372/393 [00:47<00:02,  7.92ba/s][1,0]<stderr>:#015 93%|█████████▎| 366/393 [00:47<00:03,  7.98ba/s][1,5]<stderr>:#015 93%|█████████▎| 366/393 [00:47<00:03,  7.79ba/s][1,1]<stderr>:#015 95%|█████████▍| 372/393 [00:47<00:02,  8.13ba/s][1,2]<stderr>:#015 95%|█████████▌| 374/393 [00:47<00:02,  8.03ba/s][1,7]<stderr>:#015 93%|█████████▎| 367/393 [00:47<00:03,  7.06ba/s][1,3]<stderr>:#015 97%|█████████▋| 380/393 [00:47<00:01,  8.35ba/s][1,4]<stderr>:#015 94%|█████████▎| 368/393 [00:47<00:02,  8.51ba/s][1,6]<stderr>:#015 95%|█████████▍| 373/393 [00:47<00:02,  8.08ba/s][1,5]<stderr>:#015 93%|█████████▎| 367/393 [00:47<00:03,  8.03ba/s][1,1]<stderr>:#015 95%|█████████▍| 373/393 [00:47<00:02,  8.29ba/s][1,2]<stderr>:#015 95%|█████████▌| 375/393 [00:47<00:02,  7.63ba/s][1,7]<stderr>:#015 94%|█████████▎| 368/393 [00:47<00:03,  7.63ba/s][1,4]<stderr>:#015 94%|█████████▍| 369/393 [00:47<00:02,  8.62ba/s][1,0]<stderr>:#015 94%|█████████▎| 368/393 [00:47<00:02,  8.34ba/s][1,3]<stderr>:#015 97%|█████████▋| 381/393 [00:47<00:01,  8.08ba/s][1,6]<stderr>:#015 95%|█████████▌| 374/393 [00:47<00:02,  7.81ba/s][1,5]<stderr>:#015 94%|█████████▎| 368/393 [00:47<00:03,  7.80ba/s][1,1]<stderr>:#015 95%|█████████▌| 374/393 [00:47<00:02,  7.97ba/s][1,4]<stderr>:#015 94%|█████████▍| 370/393 [00:47<00:02,  8.36ba/s][1,3]<stderr>:#015 97%|█████████▋| 382/393 [00:48<00:01,  8.35ba/s][1,7]<stderr>:#015 94%|█████████▍| 370/393 [00:47<00:02,  8.24ba/s][1,0]<stderr>:#015 94%|█████████▍| 370/393 [00:47<00:02,  9.31ba/s][1,2]<stderr>:#015 96%|█████████▌| 376/393 [00:47<00:02,  5.82ba/s][1,5]<stderr>:#015 94%|█████████▍| 370/393 [00:47<00:02,  8.28ba/s][1,3]<stderr>:#015 97%|█████████▋| 383/393 [00:48<00:01,  8.31ba/s][1,4]<stderr>:#015 94%|█████████▍| 371/393 [00:47<00:02,  7.82ba/s][1,0]<stderr>:#015 94%|█████████▍| 371/393 [00:47<00:02,  8.90ba/s][1,7]<stderr>:#015 94%|█████████▍| 371/393 [00:47<00:02,  7.95ba/s][1,6]<stderr>:#015 96%|█████████▌| 376/393 [00:47<00:02,  7.85ba/s][1,6]<stderr>:#015 96%|█████████▌| 377/393 [00:48<00:01,  8.31ba/s][1,1]<stderr>:#015 96%|█████████▌| 376/393 [00:48<00:02,  7.13ba/s][1,2]<stderr>:#015 96%|█████████▌| 378/393 [00:48<00:02,  6.74ba/s][1,0]<stderr>:#015 95%|█████████▍| 372/393 [00:48<00:02,  8.71ba/s][1,3]<stderr>:#015 98%|█████████▊| 385/393 [00:48<00:00,  9.04ba/s][1,4]<stderr>:#015 95%|█████████▍| 372/393 [00:47<00:02,  7.37ba/s][1,7]<stderr>:#015 95%|█████████▍| 372/393 [00:48<00:02,  7.45ba/s][1,5]<stderr>:#015 95%|█████████▍| 372/393 [00:47<00:02,  8.87ba/s][1,2]<stderr>:#015 96%|█████████▋| 379/393 [00:48<00:01,  7.26ba/s][1,1]<stderr>:#015 96%|█████████▌| 377/393 [00:48<00:02,  7.44ba/s][1,0]<stderr>:#015 95%|█████████▍| 373/393 [00:48<00:02,  8.37ba/s][1,4]<stderr>:#015 95%|█████████▍| 373/393 [00:47<00:02,  7.78ba/s][1,5]<stderr>:#015 95%|█████████▍| 3\u001b[0m\n",
      "\u001b[34m73/393 [00:47<00:02,  8.88ba/s][1,3]<stderr>:#015 98%|█████████▊| 386/393 [00:48<00:00,  8.70ba/s][1,7]<stderr>:#015 95%|█████████▍| 373/393 [00:48<00:02,  7.35ba/s][1,2]<stderr>:#015 97%|█████████▋| 380/393 [00:48<00:01,  7.55ba/s][1,1]<stderr>:#015 96%|█████████▌| 378/393 [00:48<00:01,  7.79ba/s][1,0]<stderr>:#015 95%|█████████▌| 374/393 [00:48<00:02,  8.12ba/s][1,4]<stderr>:#015 95%|█████████▌| 374/393 [00:48<00:02,  7.74ba/s][1,3]<stderr>:#015 98%|█████████▊| 387/393 [00:48<00:00,  8.57ba/s][1,5]<stderr>:#015 95%|█████████▌| 374/393 [00:47<00:02,  8.52ba/s][1,6]<stderr>:#015 96%|█████████▌| 378/393 [00:48<00:02,  5.87ba/s][1,2]<stderr>:#015 97%|█████████▋| 381/393 [00:48<00:01,  7.93ba/s][1,1]<stderr>:#015 96%|█████████▋| 379/393 [00:48<00:01,  8.04ba/s][1,3]<stderr>:#015 99%|█████████▊| 388/393 [00:48<00:00,  8.76ba/s][1,7]<stderr>:#015 95%|█████████▌| 375/393 [00:48<00:02,  7.91ba/s][1,0]<stderr>:#015 95%|█████████▌| 375/393 [00:48<00:02,  8.10ba/s][1,6]<stderr>:#015 96%|█████████▋| 379/393 [00:48<00:02,  6.56ba/s][1,4]<stderr>:#015 95%|█████████▌| 375/393 [00:48<00:02,  7.88ba/s][1,5]<stderr>:#015 95%|█████████▌| 375/393 [00:48<00:02,  8.07ba/s][1,1]<stderr>:#015 97%|█████████▋| 380/393 [00:48<00:01,  8.54ba/s][1,3]<stderr>:#015 99%|█████████▉| 389/393 [00:48<00:00,  8.36ba/s][1,6]<stderr>:#015 97%|█████████▋| 380/393 [00:48<00:01,  6.81ba/s][1,2]<stderr>:#015 97%|█████████▋| 383/393 [00:48<00:01,  8.47ba/s][1,1]<stderr>:#015 97%|█████████▋| 381/393 [00:48<00:01,  8.81ba/s][1,5]<stderr>:#015 96%|█████████▌| 376/393 [00:48<00:02,  6.92ba/s][1,7]<stderr>:#015 96%|█████████▌| 376/393 [00:48<00:02,  6.33ba/s][1,6]<stderr>:#015 97%|█████████▋| 381/393 [00:48<00:01,  7.35ba/s][1,3]<stderr>:#015 99%|█████████▉| 391/393 [00:48<00:00,  9.57ba/s][1,4]<stderr>:#015 96%|█████████▌| 376/393 [00:48<00:02,  5.94ba/s][1,0]<stderr>:#015 96%|█████████▌| 376/393 [00:48<00:02,  5.96ba/s][1,5]<stderr>:#015 96%|█████████▌| 377/393 [00:48<00:02,  7.25ba/s][1,6]<stderr>:#015 97%|█████████▋| 382/393 [00:48<00:01,  7.76ba/s][1,7]<stderr>:#015 96%|█████████▌| 377/393 [00:48<00:02,  6.54ba/s][1,1]<stderr>:#015 97%|█████████▋| 383/393 [00:48<00:01,  8.95ba/s][1,2]<stderr>:#015 98%|█████████▊| 385/393 [00:48<00:00,  8.24ba/s][1,5]<stderr>:#015 96%|█████████▌| 378/393 [00:48<00:01,  7.89ba/s][1,6]<stderr>:#015 97%|█████████▋| 383/393 [00:48<00:01,  7.94ba/s][1,4]<stderr>:#015 96%|█████████▌| 378/393 [00:48<00:02,  6.62ba/s][1,1]<stderr>:#015 98%|█████████▊| 384/393 [00:48<00:01,  8.82ba/s][1,7]<stderr>:#015 96%|█████████▌| 378/393 [00:48<00:02,  6.88ba/s][1,0]<stderr>:#015 96%|█████████▌| 378/393 [00:48<00:02,  6.61ba/s][1,2]<stderr>:#015 98%|█████████▊| 386/393 [00:48<00:00,  8.64ba/s][1,5]<stderr>:#015 97%|█████████▋| 380/393 [00:48<00:01,  8.86ba/s][1,4]<stderr>:#015 96%|█████████▋| 379/393 [00:48<00:02,  6.82ba/s][1,3]<stderr>:#015100%|██████████| 393/393 [00:49<00:00,  7.82ba/s][1,3]<stderr>:#015100%|██████████| 393/393 [00:49<00:00,  7.96ba/s]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:#015 98%|█████████▊| 385/393 [00:49<00:00,  8.17ba/s][1,0]<stderr>:#015 96%|█████████▋| 379/393 [00:49<00:02,  6.76ba/s][1,2]<stderr>:#015 98%|█████████▊| 387/393 [00:49<00:00,  8.35ba/s][1,7]<stderr>:#015 96%|█████████▋| 379/393 [00:49<00:02,  6.92ba/s][1,6]<stderr>:#015 98%|█████████▊| 385/393 [00:49<00:00,  8.52ba/s][1,3]<stderr>:#015  0%|          | 0/10 [00:00<?, ?ba/s][1,1]<stderr>:#015 98%|█████████▊| 386/393 [00:49<00:00,  8.48ba/s][1,0]<stderr>:#015 97%|█████████▋| 380/393 [00:49<00:01,  7.39ba/s][1,2]<stderr>:#015 99%|█████████▊| 388/393 [00:49<00:00,  8.46ba/s][1,7]<stderr>:#015 97%|█████████▋| 380/393 [00:49<00:01,  7.02ba/s][1,5]<stderr>:#015 97%|█████████▋| 382/393 [00:48<00:01,  9.47ba/s][1,4]<stderr>:#015 97%|█████████▋| 380/393 [00:48<00:01,  6.66ba/s][1,1]<stderr>:#015 98%|█████████▊| 387/393 [00:49<00:00,  8.27ba/s][1,2]<stderr>:#015 99%|█████████▉| 389/393 [00:49<00:00,  8.39ba/s][1,6]<stderr>:#015 98%|█████████▊| 387/393 [00:49<00:00,  8.76ba/s][1,7]<stderr>:#015 97%|█████████▋| 381/393 [00:49<00:01,  7.56ba/s][1,3]<stderr>:#015 10%|█         | 1/10 [00:00<00:01,  6.45ba/s][1,0]<stderr>:#015 97%|█████████▋| 381/393 [00:49<00:01,  6.98ba/s][1,3]<stderr>:#015 20%|██        | 2/10 [00:00<00:01,  7.15ba/s][1,6]<stderr>:#015 99%|█████████▊| 388/393 [00:49<00:00,  8.77ba/s][1,4]<stderr>:#015 97%|█████████▋| 382/393 [00:49<00:01,  7.23ba/s][1,1]<stderr>:#015 99%|█████████▊| 388/393 [00:49<00:00,  7.87ba/s][1,2]<stderr>:#015 99%|█████████▉| 390/393 [00:49<00:00,  7.91ba/s][1,0]<stderr>:#015 97%|█████████▋| 382/393 [00:49<00:01,  7.41ba/s][1,5]<stderr>:#015 98%|█████████▊| 384/393 [00:49<00:00,  9.01ba/s][1,7]<stderr>:#015 97%|█████████▋| 382/393 [00:49<00:01,  7.01ba/s][1,6]<stderr>:#015 99%|█████████▉| 389/393 [00:49<00:00,  8.57ba/s][1,2]<stderr>:#015 99%|█████████▉| 391/393 [00:49<00:00,  8.29ba/s][1,3]<stderr>:#015 30%|███       | 3/10 [00:00<00:00,  7.28ba/s][1,4]<stderr>:#015 97%|█████████▋| 383/393 [00:49<00:01,  7.46ba/s][1,0]<stderr>:#015 97%|█████████▋| 383/393 [00:49<00:01,  7.81ba/s][1,5]<stderr>:#015 98%|█████████▊| 385/393 [00:49<00:00,  9.13ba/s][1,1]<stderr>:#015 99%|█████████▉| 389/393 [00:49<00:00,  7.91ba/s][1,7]<stderr>:#015 97%|█████████▋| 383/393 [00:49<00:01,  7.51ba/s][1,3]<stderr>:#015 40%|████      | 4/10 [00:00<00:00,  7.70ba/s][1,0]<stderr>:#015 98%|█████████▊| 384/393 [00:49<00:01,  7.97ba/s][1,4]<stderr>:#015 98%|█████████▊| 384/393 [00:49<00:01,  7.55ba/s][1,1]<stderr>:#015 99%|█████████▉| 390/393 [00:49<00:00,  7.81ba/s][1,5]<stderr>:#015 98%|█████████▊| 386/393 [00:49<00:00,  8.48ba/s][1,6]<stderr>:#015 99%|█████████▉| 390/393 [00:49<00:00,  7.79ba/s][1,7]<stderr>:#015 98%|█████████▊| 384/393 [00:49<00:01,  7.64ba/s][1,0]<stderr>:#015 98%|█████████▊| 385/393 [00:49<00:00,  8.35ba/s][1,3]<stderr>:#015 50%|█████     | 5/10 [00:00<00:00,  7.90ba/s][1,1]<stderr>:#015 99%|█████████▉| 391/393 [00:49<00:00,  8.28ba/s][1,5]<stderr>:#015 98%|█████████▊| 387/393 [00:49<00:00,  8.82ba/s][1,7]<stderr>:#015 98%|█████████▊| 385/393 [00:49<00:00,  8.21ba/s][1,4]<stderr>:#015 98%|█████████▊| 385/393 [00:49<00:01,  7.51ba/s][1,2]<stderr>:#015100%|█████████▉| 392/393 [00:49<00:00,  5.85ba/s][1,0]<stderr>:#015 98%|█████████▊| 386/393 [00:49<00:00,  8.37ba/s][1,2]<stderr>:#015100%|██████████| 393/393 [00:49<00:00,  7.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:#015100%|█████████▉| 392/393 [00:49<00:00,  8.18ba/s][1,7]<stderr>:#015 98%|█████████▊| 386/393 [00:49<00:00,  8.51ba/s][1,3]<stderr>:#015 60%|██████    | 6/10 [00:00<00:00,  7.69ba/s][1,5]<stderr>:#015 99%|█████████▊| 388/393 [00:49<00:00,  8.58ba/s][1,4]<stderr>:#015 98%|█████████▊| 386/393 [00:49<00:00,  7.85ba/s][1,2]<stderr>:#015  0%|          | 0/10 [00:00<?, ?ba/s][1,6]<stderr>:#015100%|██████████| 393/393 [00:50<00:00,  7.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:#015100%|█████████▉| 392/393 [00:50<00:00,  6.60ba/s][1,0]<stderr>:#015 98%|█████████▊| 387/393 [00:50<00:00,  8.32ba/s][1,7]<stderr>:#015 98%|█████████▊| 387/393 [00:50<00:00,  8.77ba/s][1,3]<stderr>:#015 70%|███████   | 7/10 [00:00<00:00,  8.13ba/s][1,5]<stderr>:#015 99%|█████████▉| 389/393 [00:49<00:00,  8.80ba/s][1,4]<stderr>:#015 98%|█████████▊| 387/393 [00:49<00:00,  8.30ba/s][1,6]<stderr>:#015  0%|          | 0/10 [00:00<?, ?ba/s][1,1]<stderr>:#015100%|██████████| 393/393 [00:50<00:00,  7.25ba/s][1,1]<stderr>:#015100%|██████████| 393/393 [00:50<00:00,  7.84ba/s][1,1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:#015 99%|█████████▉| 390/393 [00:49<00:00,  8.92ba/s][1,0]<stderr>:#015 99%|█████████▊| 388/393 [00:50<00:00,  8.38ba/s][1,4]<stderr>:#015 99%|█████████▊| 388/393 [00:49<00:00,  8.65ba/s][1,2]<stderr>:#015 10%|█         | 1/10 [00:00<00:01,  7.14ba/s][1,3]<stderr>:#015 80%|████████  | 8/10 [00:00<00:00,  8.12ba/s][1,7]<stderr>:#015 99%|█████████▊| 388/393 [00:50<00:00,  8.22ba/s][1,1]<stderr>:#015  0%|          | 0/10 [00:00<?, ?ba/s][1,4]<stderr>:#015 99%|█████████▉| 389/393 [00:49<00:00,  8.55ba/s][1,0]<stderr>:#015 99%|█████████▉| 389/393 [00:50<00:00,  7.98ba/s][1,5]<stderr>:#015 99%|█████████▉| 391/393 [00:49<00:00,  8.16ba/s][1,3]<stderr>:#015 90%|█████████ | 9/10 [00:01<00:00,  7.89ba/s][1,2]<stderr>:#015 20%|██        | 2/10 [00:00<00:01,  6.98ba/s][1,3]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  8.37ba/s][1,3]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  8.14ba/s]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 99%|█████████▉| 390/393 [00:50<00:00,  8.25ba/s][1,4]<stderr>:#015 99%|█████████▉| 390/393 [00:50<00:00,  8.28ba/s][1,7]<stderr>:#015 99%|█████████▉| 390/393 [00:50<00:00,  8.39ba/s][1,1]<stderr>:#015 20%|██        | 2/10 [00:00<00:00, 10.20ba/s][1,6]<stderr>:#015 20%|██        | 2/10 [00:00<00:01,  6.79ba/s][1,3]<stderr>:#015  0%|          | 0/10 [00:00<?, ?ba/s][1,5]<stderr>:#015100%|█████████▉| 392/393 [00:50<00:00,  6.78ba/s][1,2]<stderr>:#015 40%|████      | 4/10 [00:00<00:00,  7.67ba/s][1,7]<stderr>:#015 99%|█████████▉| 391/393 [00:50<00:00,  8.60ba/s][1,4]<stderr>:#015 99%|█████████▉| 391/393 [00:50<00:00,  8.45ba/s][1,1]<stderr>:#015 30%|███       | 3/10 [00:00<00:00, 10.03ba/s][1,0]<stderr>:#015 99%|█████████▉| 391/393 [00:50<00:00,  8.35ba/s][1,6]<stderr>:#015 30%|███       | 3/10 [00:00<00:00,  7.04ba/s][1,3]<stderr>:#015 10%|█         | 1/10 [00:00<00:01,  8.88ba/s][1,5]<stderr>:#015100%|██████████| 393/393 [00:50<00:00,  7.82ba/s][1,5]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:#015 50%|█████     | 5/10 [00:00<00:00,  8.03ba/s][1,1]<stderr>:#015 40%|████      | 4/10 [00:00<00:00,  9.29ba/s][1,6]<stderr>:#015 40%|████      | 4/10 [00:00<00:00,  7.71ba/s][1,5]<stderr>:#015  0%|          | 0/10 [00:00<?, ?ba/s][1,0]<stderr>:#015100%|█████████▉| 392/393 [00:50<00:00,  6.86ba/s][1,7]<stderr>:#015100%|█████████▉| 392/393 [00:50<00:00,  6.69ba/s][1,1]<stderr>:#015 50%|█████     | 5/10 [00:00<00:00,  9.43ba/s][1,4]<stderr>:#015100%|█████████▉| 392/393 [00:50<00:00,  6.49ba/s][1,3]<stderr>:#015 30%|███       | 3/10 [00:00<00:00,  9.59ba/s][1,2]<stderr>:#015 70%|███████   | 7/10 [00:00<00:00,  8.61ba/s][1,4]<stderr>:#015100%|██████████| 393/393 [00:50<00:00,  7.78ba/s][1,4]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:#015 60%|██████    | 6/10 [00:00<00:00,  8.31ba/s][1,7]<stderr>:#015100%|██████████| 393/393 [00:50<00:00,  7.32ba/s][1,0]<stderr>:#015100%|██████████| 393/393 [00:50<00:00,  7.21ba/s][1,7]<stderr>:#015100%|██████████| 393/393 [00:50<00:00,  7.72ba/s]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015100%|██████████| 393/393 [00:50<00:00,  7.72ba/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:#015 60%|██████    | 6/10 [00:00<00:00,  9.13ba/s][1,5]<stderr>:#015 20%|██        | 2/10 [00:00<00:00,  9.68ba/s][1,3]<stderr>:#015 40%|████      | 4/10 [00:00<00:00,  8.67ba/s][1,4]<stderr>:#015  0%|          | 0/10 [00:00<?, ?ba/s][1,7]<stderr>:#015  0%|          | 0/10 [00:00<?, ?ba/s][1,0]<stderr>:#015  0%|          | 0/10 [00:00<?, ?ba/s][1,6]<stderr>:#015 70%|███████   | 7/10 [00:00<00:00,  8.59ba/s][1,1]<stderr>:#015 70%|███████   | 7/10 [00:00<00:00,  9.28ba/s][1,4]<stderr>:#015 10%|█         | 1/10 [00:00<00:01,  7.76ba/s][1,7]<stderr>:#015 10%|█         | 1/10 [00:00<00:01,  8.55ba/s][1,2]<stderr>:#015 90%|█████████ | 9/10 [00:01<00:00,  8.73ba/s][1,3]<stderr>:#015 50%|█████     | 5/10 [00:00<00:00,  8.09ba/s][1,0]<stderr>:#015 10%|█         | 1/10 [00:00<00:01,  8.54ba/s][1,6]<stderr>:#015 80%|████████  | 8/10 [00:00<00:00,  8.97ba/s][1,5]<stderr>:#015 40%|████      | 4/10 [00:00<00:00,  9.67ba/s][1,7]<stderr>:#015 20%|██        | 2/10 [00:00<00:00,  8.55ba/s][1,2]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  8.63ba/s][1,2]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  8.80ba/s][1,2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:#015 20%|██        | 2/10 [00:00<00:01,  7.72ba/s][1,1]<stderr>:#015 90%|█████████ | 9/10 [00:00<00:00,  9.46ba/s][1,3]<stderr>:#015 60%|██████    | 6/10 [00:00<00:00,  7.95ba/s][1,0]<stderr>:#015 20%|██        | 2/10 [00:00<00:00,  8.40ba/s][1,6]<stderr>:#015 90%|█████████ | 9/10 [00:01<00:00,  8.23ba/s][1,2]<stderr>:#015  0%|          | 0/10 [00:00<?, ?ba/s][1,1]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  9.55ba/s][1,1]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  9.44ba/s]\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:#015 30%|███       | 3/10 [00:00<00:00,  7.71ba/s][1,0]<stderr>:#015 30%|███       | 3/10 [00:00<00:00,  8.48ba/s][1,7]<stderr>:#015 30%|███       | 3/10 [00:00<00:00,  8.10ba/s][1,6]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  8.56ba/s]\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:#015 60%|██████    | 6/10 [00:00<00:00,  9.57ba/s][1,3]<stderr>:#015 70%|███████   | 7/10 [00:00<00:00,  7.98ba/s][1,6]<stderr>:#015  0%|          | 0/10 [00:00<?, ?ba/s][1,1]<stderr>:#015  0%|          | 0/10 [00:00<?, ?ba/s][1,4]<stderr>:#015 40%|████      | 4/10 [00:00<00:00,  8.12ba/s][1,5]<stderr>:#015 70%|███████   | 7/10 [00:00<00:00,  8.94ba/s][1,0]<stderr>:#015 40%|████      | 4/10 [00:00<00:00,  7.99ba/s][1,7]<stderr>:#015 40%|████      | 4/10 [00:00<00:00,  7.76ba/s][1,6]<stderr>:#015 10%|█         | 1/10 [00:00<00:00,  9.24ba/s][1,1]<stderr>:#015 10%|█         | 1/10 [00:00<00:01,  8.60ba/s][1,2]<stderr>:#015 20%|██        | 2/10 [00:00<00:00,  8.83ba/s][1,3]<stderr>:#015 80%|████████  | 8/10 [00:01<00:00,  6.25ba/s][1,6]<stderr>:#015 20%|██        | 2/10 [00:00<00:00,  9.31ba/s][1,7]<stderr>:#015 50%|█████     | 5/10 [00:00<00:00,  7.72ba/s][1,0]<stderr>:#015 50%|█████     | 5/10 [00:00<00:00,  7.68ba/s][1,2]<stderr>:#015 30%|███       | 3/10 [00:00<00:00,  8.65ba/s][1,1]<stderr>:#015 20%|██        | 2/10 [00:00<00:00,  8.36ba/s][1,4]<stderr>:#015 60%|██████    | 6/10 [00:00<00:00,  8.37ba/s][1,5]<stderr>:#015 90%|█████████ | 9/10 [00:00<00:00,  9.22ba/s][1,6]<stderr>:#015 30%|███       | 3/10 [00:00<00:00,  9.43ba/s][1,7]<stderr>:#015 60%|██████    | 6/10 [00:00<00:00,  7.58ba/s][1,0]<stderr>:#015 60%|██████    | 6/10 [00:00<00:00,  7.45ba/s][1,3]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  7.15ba/s][1,3]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  8.01ba/s]\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:#015 40%|████      | 4/10 [00:00<00:00,  8.60ba/s][1,5]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  9.39ba/s][1,5]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  9.43ba/s]\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:#015 70%|███████   | 7/10 [00:00<00:00,  8.37ba/s][1,1]<stderr>:#015 30%|███       | 3/10 [00:00<00:00,  7.93ba/s][1,3]<stderr>:#015  0%|          | 0/10 [00:00<?, ?ba/s][1,5]<stderr>:#015  0%|          | 0/10 [00:00<?, ?ba/s][1,7]<stderr>:#015 70%|███████   | 7/10 [00:00<00:00,  7.93ba/s][1,2]<stderr>:#015 50%|█████     | 5/10 [00:00<00:00,  8.88ba/s][1,4]<stderr>:#015 80%|████████  | 8/10 [00:00<00:00,  8.42ba/s][1,0]<stderr>:#015 70%|███████   | 7/10 [00:00<00:00,  7.25ba/s][1,1]<stderr>:#015 40%|████      | 4/10 [00:00<00:00,  7.79ba/s][1,5]<stderr>:#015 10%|█         | 1/10 [00:00<00:00,  9.87ba/s][1,6]<stderr>:#015 50%|█████     | 5/10 [00:00<00:00,  9.27ba/s][1,7]<stderr>:#015 80%|████████  | 8/10 [00:01<00:00,  8.24ba/s][1,2]<stderr>:#015 60%|██████    | 6/10 [00:00<00:00,  8.80ba/s][1,4]<stderr>:#015 90%|█████████ | 9/10 [00:01<00:00,  8.72ba/s][1,3]<stderr>:#015 20%|██        | 2/10 [00:00<00:00,  9.82ba/s][1,6]<stderr>:#015 60%|██████    | 6/10 [00:00<00:00,  9.13ba/s][1,1]<stderr>:#015 50%|█████     | 5/10 [00:00<00:00,  7.85ba/s][1,5]<stderr>:#015 20%|██        | 2/10 [00:00<00:00,  9.16ba/s][1,7]<stderr>:#015 90%|█████████ | 9/10 [00:01<00:00,  8.38ba/s][1,0]<stderr>:#015 80%|████████  | 8/10 [00:01<00:00,  6.55ba/s][1,4]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  8.56ba/s][1,4]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  8.44ba/s]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:#015 30%|███       | 3/10 [00:00<00:00,  9.24ba/s][1,1]<stderr>:#015 60%|██████    | 6/10 [00:00<00:00,  8.39ba/s][1,6]<stderr>:#015 70%|███████   | 7/10 [00:00<00:00,  8.71ba/s][1,4]<stderr>:#015  0%|          | 0/10 [00:00<?, ?ba/s][1,7]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  8.25ba/s]\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:#015 30%|███       | 3/10 [00:00<00:00,  8.41ba/s][1,0]<stderr>:#015 90%|█████████ | 9/10 [00:01<00:00,  7.03ba/s][1,7]<stderr>:#015  0%|          | 0/10 [00:00<?, ?ba/s][1,1]<stderr>:#015 70%|███████   | 7/10 [00:00<00:00,  8.44ba/s][1,6]<stderr>:#015 80%|████████  | 8/10 [00:00<00:00,  9.02ba/s][1,3]<stderr>:#015 40%|████      | 4/10 [00:00<00:00,  8.82ba/s][1,0]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  7.65ba/s]\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:#015 10%|█         | 1/10 [00:00<00:01,  8.15ba/s][1,5]<stderr>:#015 40%|████      | 4/10 [00:00<00:00,  8.42ba/s][1,2]<stderr>:#015 80%|████████  | 8/10 [00:01<00:00,  7.69ba/s][1,0]<stderr>:#015  0%|          | 0/10 [00:00<?, ?ba/s][1,7]<stderr>:#015 10%|█         | 1/10 [00:00<00:00,  9.42ba/s][1,4]<stderr>:#015 20%|██        | 2/10 [00:00<00:00,  8.32ba/s][1,3]<stderr>:#015 50%|█████     | 5/10 [00:00<00:00,  8.20ba/s][1,5]<stderr>:#015 50%|█████     | 5/10 [00:00<00:00,  8.77ba/s][1,2]<stderr>:#015 90%|█████████ | 9/10 [00:01<00:00,  8.17ba/s][1,0]<stderr>:#015 10%|█         | 1/10 [00:00<00:01,  8.85ba/s][1,1]<stderr>:#015 80%|████████  | 8/10 [00:01<00:00,  6.74ba/s][1,3]<stderr>:#015 60%|██████    | 6/10 [00:00<00:00,  8.38ba/s][1,4]<stderr>:#015 30%|███       | 3/10 [00:00<00:00,  8.26ba/s][1,2]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  8.46ba/s][1,2]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  8.10ba/s]\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:#015 60%|██████    | 6/10 [00:00<00:00,  8.68ba/s][1,7]<stderr>:#015 30%|███       | 3/10 [00:00<00:00,  9.94ba/s][1,6]<stderr>:#015 90%|█████████ | 9/10 [00:01<00:00,  6.19ba/s][1,0]<stderr>:#015 20%|██        | 2/10 [00:00<00:00,  9.03ba/s][1,2]<stderr>:#015  0%|          | 0/10 [00:00<?, ?ba/s][1,3]<stderr>:#015 70%|███████   | 7/10 [00:00<00:00,  8.07ba/s][1,7]<stderr>:#015 40%|████      | 4/10 [00:00<00:00,  9.15ba/s][1,6]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  6.70ba/s][1,0]<stderr>:#015 30%|███       | 3/10 [00:00<00:00,  9.25ba/s][1,6]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  7.79ba/s]\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:#015 70%|███████   | 7/10 [00:00<00:00,  8.24ba/s][1,4]<stderr>:#015 40%|████      | 4/10 [00:00<00:00,  7.89ba/s][1,1]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  7.58ba/s][1,1]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  7.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:#015 10%|█         | 1/10 [00:00<00:00,  9.13ba/s][1,6]<stderr>:#015  0%|          | 0/10 [00:00<?, ?ba/s][1,1]<stderr>:#015  0%|          | 0/10 [00:00<?, ?ba/s][1,0]<stderr>:#015 40%|████      | 4/10 [00:00<00:00,  9.15ba/s][1,4]<stderr>:#015 50%|█████     | 5/10 [00:00<00:00,  8.08ba/s][1,3]<stderr>:#015 80%|████████  | 8/10 [00:00<00:00,  8.10ba/s][1,7]<stderr>:#015 50%|█████     | 5/10 [00:00<00:00,  8.89ba/s][1,1]<stderr>:#015 10%|█         | 1/10 [00:00<00:01,  7.63ba/s][1,4]<stderr>:#015 60%|██████    | 6/10 [00:00<00:00,  8.26ba/s][1,5]<stderr>:#015 80%|████████  | 8/10 [00:01<00:00,  6.41ba/s][1,6]<stderr>:#015 20%|██        | 2/10 [00:00<00:00, 12.05ba/s][1,3]<stderr>:#015 90%|█████████ | 9/10 [00:01<00:00,  8.04ba/s][1,0]<stderr>:#015 50%|█████     | 5/10 [00:00<00:00,  8.60ba/s][1,7]<stderr>:#015 60%|██████    | 6/10 [00:00<00:00,  8.58ba/s][1,2]<stderr>:#015 30%|███       | 3/10 [00:00<00:00,  9.12ba/s][1,3]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  8.30ba/s][1,3]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  8.32ba/s]\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:#015 70%|███████   | 7/10 [00:00<00:00,  8.05ba/s][1,6]<stderr>:#015 30%|███       | 3/10 [00:00<00:00, 10.55ba/s][1,7]<stderr>:#015 70%|███████   | 7/10 [00:00<00:00,  8.41ba/s][1,0]<stderr>:#015 60%|██████    | 6/10 [00:00<00:00,  8.39ba/s][1,2]<stderr>:#015 40%|████      | 4/10 [00:00<00:00,  8.82ba/s][1,1]<stderr>:#015 20%|██        | 2/10 [00:00<00:01,  7.11ba/s][1,5]<stderr>:#015 90%|█████████ | 9/10 [00:01<00:00,  6.29ba/s][1,3]<stderr>:#015  0%|          | 0/10 [00:00<?, ?ba/s][1,6]<stderr>:#015 40%|████      | 4/10 [00:00<00:00,  9.93ba/s][1,5]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  7.47ba/s]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:#015 30%|███       | 3/10 [00:00<00:00,  7.78ba/s][1,2]<stderr>:#015 50%|█████     | 5/10 [00:00<00:00,  8.57ba/s][1,0]<stderr>:#015 70%|███████   | 7/10 [00:00<00:00,  8.15ba/s][1,5]<stderr>:#015  0%|          | 0/10 [00:00<?, ?ba/s][1,1]<stderr>:#015 40%|████      | 4/10 [00:00<00:00,  8.33ba/s][1,7]<stderr>:#015 80%|████████  | 8/10 [00:01<00:00,  6.50ba/s][1,4]<stderr>:#015 80%|████████  | 8/10 [00:01<00:00,  6.10ba/s][1,3]<stderr>:#015 20%|██        | 2/10 [00:00<00:00, 10.82ba/s][1,5]<stderr>:#015 10%|█         | 1/10 [00:00<00:01,  8.52ba/s][1,0]<stderr>:#015 80%|████████  | 8/10 [00:01<00:00,  6.73ba/s][1,2]<stderr>:#015 70%|███████   | 7/10 [00:00<00:00,  8.69ba/s][1,6]<stderr>:#015 60%|██████    | 6/10 [00:00<00:00,  9.39ba/s][1,7]<stderr>:#015 90%|█████████ | 9/10 [00:01<00:00,  7.00ba/s][1,1]<stderr>:#015 50%|█████     | 5/10 [00:00<00:00,  8.14ba/s][1,4]<stderr>:#015 90%|█████████ | 9/10 [00:01<00:00,  6.82ba/s][1,5]<stderr>:#015 20%|██        | 2/10 [00:00<00:00,  8.32ba/s][1,6]<stderr>:#015 70%|███████   | 7/10 [00:00<00:00,  9.14ba/s][1,4]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  7.33ba/s][1,7]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  7.40ba/s][1,4]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  7.47ba/s]\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  7.99ba/s]\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:#015 80%|████████  | 8/10 [00:00<00:00,  8.48ba/s][1,3]<stderr>:#015 40%|████      | 4/10 [00:00<00:00, 10.25ba/s][1,1]<stderr>:#015 60%|██████    | 6/10 [00:00<00:00,  7.92ba/s][1,0]<stderr>:#015 90%|█████████ | 9/10 [00:01<00:00,  6.76ba/s][1,4]<stderr>:#015  0%|          | 0/10 [00:00<?, ?ba/s][1,7]<stderr>:#015  0%|          | 0/10 [00:00<?, ?ba/s][1,6]<stderr>:#015 80%|████████  | 8/10 [00:00<00:00,  8.49ba/s][1,0]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  7.20ba/s][1,0]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  7.72ba/s]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:#015 70%|███████   | 7/10 [00:00<00:00,  7.95ba/s][1,2]<stderr>:#015 90%|█████████ | 9/10 [00:01<00:00,  8.02ba/s][1,3]<stderr>:#015 50%|█████     | 5/10 [00:00<00:00,  9.01ba/s][1,7]<stderr>:#015 10%|█         | 1/10 [00:00<00:00,  9.17ba/s][1,5]<stderr>:#015 40%|████      | 4/10 [00:00<00:00,  8.89ba/s][1,0]<stderr>:#015  0%|          | 0/10 [00:00<?, ?ba/s][1,2]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  8.34ba/s][1,2]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  8.53ba/s]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:#015 60%|██████    | 6/10 [00:00<00:00,  9.04ba/s][1,4]<stderr>:#015 20%|██        | 2/10 [00:00<00:00,  9.35ba/s][1,1]<stderr>:#015 80%|████████  | 8/10 [00:01<00:00,  7.90ba/s][1,5]<stderr>:#015 50%|█████     | 5/10 [00:00<00:00,  8.83ba/s][1,7]<stderr>:#015 20%|██        | 2/10 [00:00<00:00,  9.31ba/s][1,6]<stderr>:#015 90%|█████████ | 9/10 [00:01<00:00,  7.53ba/s][1,2]<stderr>:#015  0%|          | 0/10 [00:00<?, ?ba/s][1,3]<stderr>:#015 70%|███████   | 7/10 [00:00<00:00,  8.70ba/s][1,4]<stderr>:#015 30%|███       | 3/10 [00:00<00:00,  8.73ba/s][1,6]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  8.61ba/s][1,6]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:#015 30%|███       | 3/10 [00:00<00:00,  8.76ba/s][1,0]<stderr>:#015 20%|██        | 2/10 [00:00<00:00,  9.93ba/s][1,5]<stderr>:#015 60%|██████    | 6/10 [00:00<00:00,  8.36ba/s][1,1]<stderr>:#015 90%|█████████ | 9/10 [00:01<00:00,  7.75ba/s][1,2]<stderr>:#015 10%|█         | 1/10 [00:00<00:01,  8.70ba/s][1,6]<stderr>:#015  0%|          | 0/10 [00:00<?, ?ba/s][1,3]<stderr>:#015 80%|████████  | 8/10 [00:00<00:00,  8.59ba/s][1,0]<stderr>:#015 30%|███       | 3/10 [00:00<00:00,  9.45ba/s][1,5]<stderr>:#015 70%|███████   | 7/10 [00:00<00:00,  8.33ba/s][1,1]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  7.83ba/s][1,1]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  7.86ba/s][1,1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:#015 20%|██        | 2/10 [00:00<00:00,  8.70ba/s][1,7]<stderr>:#015 40%|████      | 4/10 [00:00<00:00,  8.00ba/s][1,4]<stderr>:#015 40%|████      | 4/10 [00:00<00:00,  7.75ba/s][1,1]<stderr>:#015  0%|          | 0/10 [00:00<?, ?ba/s][1,3]<stderr>:#015 90%|█████████ | 9/10 [00:01<00:00,  8.52ba/s][1,5]<stderr>:#015 80%|████████  | 8/10 [00:00<00:00,  8.21ba/s][1,7]<stderr>:#015 50%|█████     | 5/10 [00:00<00:00,  8.26ba/s][1,2]<stderr>:#015 30%|███       | 3/10 [00:00<00:00,  8.61ba/s][1,4]<stderr>:#015 50%|█████     | 5/10 [00:00<00:00,  7.98ba/s][1,6]<stderr>:#015 20%|██        | 2/10 [00:00<00:00,  9.23ba/s][1,0]<stderr>:#015 40%|████      | 4/10 [00:00<00:00,  8.14ba/s][1,3]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  9.13ba/s][1,3]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:__main__:Sample 335243 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'hypothesis': \"Parents are busy and it's sometimes hard to get them out.\", 'idx': 335243, 'input_ids': [0, 6968, 216, 77, 49, 1041, 283, 8, 24, 18, 543, 7, 120, 106, 66, 8, 10, 319, 9, 1041, 33, 2127, 7, 213, 8, 8, 383, 101, 14, 8, 24, 18, 628, 23, 363, 98, 2, 2, 35835, 32, 3610, 8, 24, 18, 2128, 543, 7, 120, 106, 66, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 0, 'premise': \"you know when their parents come and it's hard to get them out and a lot of parents have places to go and and things like that and it's late at night so\"}.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:__main__:Sample 58369 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'hypothesis': 'Where and what is art? ', 'idx': 58369, 'input_ids': [0, 13841, 16, 1808, 116, 2, 2, 13841, 8, 99, 16, 1808, 116, 1437, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 1, 'premise': 'Where is art?'}.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:__main__:Sample 13112 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'hypothesis': 'The list says alcohol and injury are negatives facing staff.', 'idx': 13112, 'input_ids': [0, 7083, 45270, 8, 1356, 6, 25, 157, 25, 4315, 15985, 6, 32, 15, 5, 889, 4, 2, 2, 133, 889, 161, 3766, 8, 1356, 32, 34784, 2114, 813, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 1, 'premise': 'Alcohol and injury, as well as brief interventions, are on the list.'}.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:#015 10%|█         | 1/10 [00:00<00:01,  7.50ba/s][1,7]<stderr>:#015 60%|██████    | 6/10 [00:00<00:00,  8.36ba/s][1,4]<stderr>:#015 60%|██████    | 6/10 [00:00<00:00,  8.42ba/s][1,2]<stderr>:#015 40%|████      | 4/10 [00:00<00:00,  8.64ba/s][1,6]<stderr>:#015 30%|███       | 3/10 [00:00<00:00,  9.36ba/s][1,0]<stderr>:#015 50%|█████     | 5/10 [00:00<00:00,  8.24ba/s][1,5]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  9.31ba/s][1,5]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  9.09ba/s]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:#015 20%|██        | 2/10 [00:00<00:00,  8.06ba/s][1,5]<stderr>:#015  0%|          | 0/10 [00:00<?, ?ba/s][1,7]<stderr>:#015 70%|███████   | 7/10 [00:00<00:00,  8.64ba/s][1,4]<stderr>:#015 70%|███████   | 7/10 [00:00<00:00,  8.33ba/s][1,2]<stderr>:#015 50%|█████     | 5/10 [00:00<00:00,  8.43ba/s][1,6]<stderr>:#015 40%|████      | 4/10 [00:00<00:00,  8.92ba/s][1,3]<stderr>:#015Downloading:   0%|          | 0.00/1.85k [00:00<?, ?B/s][1,3]<stderr>:#015Downloading: 5.75kB [00:00, 2.22MB/s]                   \u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:#015 10%|█         | 1/10 [00:00<00:01,  7.92ba/s][1,7]<stderr>:#015 80%|████████  | 8/10 [00:00<00:00,  8.70ba/s][1,6]<stderr>:#015 50%|█████     | 5/10 [00:00<00:00,  9.12ba/s][1,2]<stderr>:#015 60%|██████    | 6/10 [00:00<00:00,  8.64ba/s][1,0]<stderr>:#015 70%|███████   | 7/10 [00:00<00:00,  8.47ba/s][1,1]<stderr>:#015 40%|████      | 4/10 [00:00<00:00,  8.38ba/s][1,5]<stderr>:#015 20%|██        | 2/10 [00:00<00:00,  8.08ba/s][1,0]<stderr>:#015 80%|████████  | 8/10 [00:00<00:00,  8.59ba/s][1,4]<stderr>:#015 90%|█████████ | 9/10 [00:01<00:00,  8.45ba/s][1,6]<stderr>:#015 60%|██████    | 6/10 [00:00<00:00,  8.86ba/s][1,7]<stderr>:#015 90%|█████████ | 9/10 [00:01<00:00,  8.25ba/s][1,2]<stderr>:#015 70%|███████   | 7/10 [00:00<00:00,  8.54ba/s][1,3]<stderr>:[INFO|trainer.py:398] 2022-05-02 13:47:21,678 >> max_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:#015 60%|██████    | 6/10 [00:00<00:00,  8.98ba/s][1,4]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  8.54ba/s][1,4]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  8.36ba/s][1,4]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  8.41ba/s][1,7]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  8.33ba/s][1,7]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 90%|█████████ | 9/10 [00:01<00:00,  8.34ba/s][1,2]<stderr>:#015 80%|████████  | 8/10 [00:00<00:00,  8.37ba/s][1,5]<stderr>:#015 40%|████      | 4/10 [00:00<00:00,  9.15ba/s][1,6]<stderr>:#015 70%|███████   | 7/10 [00:00<00:00,  8.32ba/s][1,4]<stderr>:#015  0%|          | 0/10 [00:00<?, ?ba/s][1,7]<stderr>:#015  0%|          | 0/10 [00:00<?, ?ba/s][1,0]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  8.55ba/s][1,0]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  8.44ba/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:#015 80%|████████  | 8/10 [00:00<00:00,  8.38ba/s][1,2]<stderr>:#015 90%|█████████ | 9/10 [00:01<00:00,  8.16ba/s][1,5]<stderr>:#015 50%|█████     | 5/10 [00:00<00:00,  8.76ba/s][1,4]<stderr>:#015 10%|█         | 1/10 [00:00<00:00,  9.72ba/s][1,1]<stderr>:#015 80%|████████  | 8/10 [00:00<00:00,  9.34ba/s][1,0]<stderr>:#015  0%|          | 0/10 [00:00<?, ?ba/s][1,5]<stderr>:#015 60%|██████    | 6/10 [00:00<00:00,  8.86ba/s][1,3]<stderr>:[INFO|trainer.py:516] 2022-05-02 13:47:21,956 >> The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: premise, hypothesis, idx.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  8.19ba/s][1,2]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  8.34ba/s]\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:__main__:Sample 335243 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'hypothesis': \"Parents are busy and it's sometimes hard to get them out.\", 'idx': 335243, 'input_ids': [0, 6968, 216, 77, 49, 1041, 283, 8, 24, 18, 543, 7, 120, 106, 66, 8, 10, 319, 9, 1041, 33, 2127, 7, 213, 8, 8, 383, 101, 14, 8, 24, 18, 628, 23, 363, 98, 2, 2, 35835, 32, 3610, 8, 24, 18, 2128, 543, 7, 120, 106, 66, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 0, 'premise': \"you know when their parents come and it's hard to get them out and a lot of parents have places to go and and things like that and it's late at night so\"}.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:#015 20%|██        | 2/10 [00:00<00:00,  9.30ba/s][1,2]<stderr>:INFO:__main__:Sample 58369 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'hypothesis': 'Where and what is art? ', 'idx': 58369, 'input_ids': [0, 13841, 16, 1808, 116, 2, 2, 13841, 8, 99, 16, 1808, 116, 1437, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 1, 'premise': 'Where is art?'}.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:__main__:Sample 13112 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'hypothesis': 'The list says alcohol and injury are negatives facing staff.', 'idx': 13112, 'input_ids': [0, 7083, 45270, 8, 1356, 6, 25, 157, 25, 4315, 15985, 6, 32, 15, 5, 889, 4, 2, 2, 133, 889, 161, 3766, 8, 1356, 32, 34784, 2114, 813, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 1, 'premise': 'Alcohol and injury, as well as brief interventions, are on the list.'}.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:#015 90%|█████████ | 9/10 [00:01<00:00,  7.97ba/s][1,7]<stderr>:#015 20%|██        | 2/10 [00:00<00:00, 11.15ba/s][1,1]<stderr>:#015 90%|█████████ | 9/10 [00:00<00:00,  8.87ba/s][1,3]<stderr>:[INFO|trainer.py:1156] 2022-05-02 13:47:22,009 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[INFO|trainer.py:1157] 2022-05-02 13:47:22,021 >>   Num examples = 392702\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[INFO|trainer.py:1158] 2022-05-02 13:47:22,021 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[INFO|trainer.py:1159] 2022-05-02 13:47:22,021 >>   Instantaneous batch size per device = 16\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[INFO|trainer.py:1160] 2022-05-02 13:47:22,021 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[INFO|trainer.py:1161] 2022-05-02 13:47:22,022 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[INFO|trainer.py:1162] 2022-05-02 13:47:22,036 >>   Total optimization steps = 500\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:#015 70%|███████   | 7/10 [00:00<00:00,  8.77ba/s][1,6]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  8.46ba/s][1,6]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  8.59ba/s]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:INFO:__main__:Sample 335243 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'hypothesis': \"Parents are busy and it's sometimes hard to get them out.\", 'idx': 335243, 'input_ids': [0, 6968, 216, 77, 49, 1041, 283, 8, 24, 18, 543, 7, 120, 106, 66, 8, 10, 319, 9, 1041, 33, 2127, 7, 213, 8, 8, 383, 101, 14, 8, 24, 18, 628, 23, 363, 98, 2, 2, 35835, 32, 3610, 8, 24, 18, 2128, 543, 7, 120, 106, 66, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 0, 'premise': \"you know when their parents come and it's hard to get them out and a lot of parents have places to go and and things like that and it's late at night so\"}.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:#015 30%|███       | 3/10 [00:00<00:00,  9.13ba/s][1,6]<stderr>:INFO:__main__:Sample 58369 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'hypothesis': 'Where and what is art? ', 'idx': 58369, 'input_ids': [0, 13841, 16, 1808, 116, 2, 2, 13841, 8, 99, 16, 1808, 116, 1437, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 1, 'premise': 'Where is art?'}.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:INFO:__main__:Sample 13112 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'hypothesis': 'The list says alcohol and injury are negatives facing staff.', 'idx': 13112, 'input_ids': [0, 7083, 45270, 8, 1356, 6, 25, 157, 25, 4315, 15985, 6, 32, 15, 5, 889, 4, 2, 2, 133, 889, 161, 3766, 8, 1356, 32, 34784, 2114, 813, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 1, 'premise': 'Alcohol and injury, as well as brief interventions, are on the list.'}.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:#015100%|██████████| 10/10 [00:01<00:00,  9.57ba/s]\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:#015 30%|███       | 3/10 [00:00<00:00, 10.73ba/s][1,1]<stderr>:INFO:__main__:Sample 335243 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'hypothesis': \"Parents are busy and it's sometimes hard to get them out.\", 'idx': 335243, 'input_ids': [0, 6968, 216, 77, 49, 1041, 283, 8, 24, 18, 543, 7, 120, 106, 66, 8, 10, 319, 9, 1041, 33, 2127, 7, 213, 8, 8, 383, 101, 14, 8, 24, 18, 628, 23, 363, 98, 2, 2, 35835, 32, 3610, 8, 24, 18, 2128, 543, 7, 120, 106, 66, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 0, 'premise': \"you know when their parents come and it's hard to get them out and a lot of parents have places to go and and things like that and it's late at night so\"}.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:__main__:Sample 58369 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'hypothesis': 'Where and what is art? ', 'idx': 58369, 'input_ids': [0, 13841, 16, 1808, 116, 2, 2, 13841, 8, 99, 16, 1808, 116, 1437, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 1, 'premise': 'Where is art?'}.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:__main__:Sample 13112 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'hypothesis': 'The list says alcohol and injury are negatives facing staff.', 'idx': 13112, 'input_ids': [0, 7083, 45270, 8, 1356, 6, 25, 157, 25, 4315, 15985, 6, 32, 15, 5, 889, 4, 2, 2, 133, 889, 161, 3766, 8, 1356, 32, 34784, 2114, 813, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 1, 'premise': 'Alcohol and injury, as well as brief interventions, are on the list.'}.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 20%|██        | 2/10 [00:00<00:00, 11.06ba/s][1,5]<stderr>:#015 90%|█████████ | 9/10 [00:00<00:00, 10.03ba/s][1,4]<stderr>:#015 40%|████      | 4/10 [00:00<00:00,  9.02ba/s][1,2]<stderr>:[INFO|trainer.py:398] 2022-05-02 13:47:22,210 >> max_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:#015100%|██████████| 10/10 [00:00<00:00, 10.48ba/s]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 40%|████      | 4/10 [00:00<00:00, 11.18ba/s][1,7]<stderr>:#015 50%|█████     | 5/10 [00:00<00:00, 10.84ba/s][1,5]<stderr>:INFO:__main__:Sample 335243 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'hypothesis': \"Parents are busy and it's sometimes hard to get them out.\", 'idx': 335243, 'input_ids': [0, 6968, 216, 77, 49, 1041, 283, 8, 24, 18, 543, 7, 120, 106, 66, 8, 10, 319, 9, 1041, 33, 2127, 7, 213, 8, 8, 383, 101, 14, 8, 24, 18, 628, 23, 363, 98, 2, 2, 35835, 32, 3610, 8, 24, 18, 2128, 543, 7, 120, 106, 66, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 0, 'premise': \"you know when their parents come and it's hard to get them out and a lot of parents have places to go and and things like that and it's late at night so\"}.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:INFO:__main__:Sample 58369 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'hypothesis': 'Where and what is art? ', 'idx': 58369, 'input_ids': [0, 13841, 16, 1808, 116, 2, 2, 13841, 8, 99, 16, 1808, 116, 1437, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 1, 'premise': 'Where is art?'}.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:INFO:__main__:Sample 13112 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'hypothesis': 'The list says alcohol and injury are negatives facing staff.', 'idx': 13112, 'input_ids': [0, 7083, 45270, 8, 1356, 6, 25, 157, 25, 4315, 15985, 6, 32, 15, 5, 889, 4, 2, 2, 133, 889, 161, 3766, 8, 1356, 32, 34784, 2114, 813, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 1, 'premise': 'Alcohol and injury, as well as brief interventions, are on the list.'}.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|trainer.py:398] 2022-05-02 13:47:22,309 >> max_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[INFO|trainer.py:398] 2022-05-02 13:47:22,330 >> max_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:#015 60%|██████    | 6/10 [00:00<00:00,  9.95ba/s][1,2]<stderr>:[INFO|trainer.py:516] 2022-05-02 13:47:22,377 >> The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[INFO|trainer.py:1156] 2022-05-02 13:47:22,423 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[INFO|trainer.py:1157] 2022-05-02 13:47:22,423 >>   Num examples = 392702\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[INFO|trainer.py:1158] 2022-05-02 13:47:22,423 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[INFO|trainer.py:1159] 2022-05-02 13:47:22,423 >>   Instantaneous batch size per device = 16\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[INFO|trainer.py:1160] 2022-05-02 13:47:22,423 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[INFO|trainer.py:1161] 2022-05-02 13:47:22,423 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[INFO|trainer.py:1162] 2022-05-02 13:47:22,424 >>   Total optimization steps = 500\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 60%|██████    | 6/10 [00:00<00:00, 11.16ba/s][1,7]<stderr>:#015 70%|███████   | 7/10 [00:00<00:00, 10.80ba/s][1,4]<stderr>:#015 80%|████████  | 8/10 [00:00<00:00, 11.01ba/s][1,5]<stderr>:[INFO|trainer.py:398] 2022-05-02 13:47:22,539 >> max_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 70%|███████   | 7/10 [00:00<00:00, 10.76ba/s][1,6]<stderr>:[INFO|trainer.py:516] 2022-05-02 13:47:22,555 >> The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: hypothesis, premise, idx.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[INFO|trainer.py:516] 2022-05-02 13:47:22,560 >> The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[INFO|trainer.py:1156] 2022-05-02 13:47:22,613 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[INFO|trainer.py:1157] 2022-05-02 13:47:22,614 >>   Num examples = 392702\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[INFO|trainer.py:1158] 2022-05-02 13:47:22,614 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[INFO|trainer.py:1159] 2022-05-02 13:47:22,614 >>   Instantaneous batch size per device = 16\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[INFO|trainer.py:1160] 2022-05-02 13:47:22,614 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[INFO|trainer.py:1161] 2022-05-02 13:47:22,614 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[INFO|trainer.py:1162] 2022-05-02 13:47:22,614 >>   Total optimization steps = 500\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|trainer.py:1156] 2022-05-02 13:47:22,627 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|trainer.py:1157] 2022-05-02 13:47:22,627 >>   Num examples = 392702\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|trainer.py:1158] 2022-05-02 13:47:22,627 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|trainer.py:1159] 2022-05-02 13:47:22,627 >>   Instantaneous batch size per device = 16\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|trainer.py:1160] 2022-05-02 13:47:22,627 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|trainer.py:1161] 2022-05-02 13:47:22,627 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|trainer.py:1162] 2022-05-02 13:47:22,627 >>   Total optimization steps = 500\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:#015100%|██████████| 10/10 [00:00<00:00, 11.32ba/s][1,4]<stderr>:#015100%|██████████| 10/10 [00:00<00:00, 11.06ba/s]\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:#015 90%|█████████ | 9/10 [00:00<00:00, 10.60ba/s][1,4]<stderr>:INFO:__main__:Sample 335243 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'hypothesis': \"Parents are busy and it's sometimes hard to get them out.\", 'idx': 335243, 'input_ids': [0, 6968, 216, 77, 49, 1041, 283, 8, 24, 18, 543, 7, 120, 106, 66, 8, 10, 319, 9, 1041, 33, 2127, 7, 213, 8, 8, 383, 101, 14, 8, 24, 18, 628, 23, 363, 98, 2, 2, 35835, 32, 3610, 8, 24, 18, 2128, 543, 7, 120, 106, 66, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 0, 'premise': \"you know when their parents come and it's hard to get them out and a lot of parents have places to go and and things like that and it's late at night so\"}.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:INFO:__main__:Sample 58369 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'hypothesis': 'Where and what is art? ', 'idx': 58369, 'input_ids': [0, 13841, 16, 1808, 116, 2, 2, 13841, 8, 99, 16, 1808, 116, 1437, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 1, 'premise': 'Where is art?'}.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:INFO:__main__:Sample 13112 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'hypothesis': 'The list says alcohol and injury are negatives facing staff.', 'idx': 13112, 'input_ids': [0, 7083, 45270, 8, 1356, 6, 25, 157, 25, 4315, 15985, 6, 32, 15, 5, 889, 4, 2, 2, 133, 889, 161, 3766, 8, 1356, 32, 34784, 2114, 813, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 1, 'premise': 'Alcohol and injury, as well as brief interventions, are on the list.'}.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:#015100%|██████████| 10/10 [00:00<00:00, 11.11ba/s]\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:INFO:__main__:Sample 335243 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'hypothesis': \"Parents are busy and it's sometimes hard to get them out.\", 'idx': 335243, 'input_ids': [0, 6968, 216, 77, 49, 1041, 283, 8, 24, 18, 543, 7, 120, 106, 66, 8, 10, 319, 9, 1041, 33, 2127, 7, 213, 8, 8, 383, 101, 14, 8, 24, 18, 628, 23, 363, 98, 2, 2, 35835, 32, 3610, 8, 24, 18, 2128, 543, 7, 120, 106, 66, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 0, 'premise': \"you know when their parents come and it's hard to get them out and a lot of parents have places to go and and things like that and it's late at night so\"}.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:INFO:__main__:Sample 58369 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'hypothesis': 'Where and what is art? ', 'idx': 58369, 'input_ids': [0, 13841, 16, 1808, 116, 2, 2, 13841, 8, 99, 16, 1808, 116, 1437, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 1, 'premise': 'Where is art?'}.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:INFO:__main__:Sample 13112 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'hypothesis': 'The list says alcohol and injury are negatives facing staff.', 'idx': 13112, 'input_ids': [0, 7083, 45270, 8, 1356, 6, 25, 157, 25, 4315, 15985, 6, 32, 15, 5, 889, 4, 2, 2, 133, 889, 161, 3766, 8, 1356, 32, 34784, 2114, 813, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 1, 'premise': 'Alcohol and injury, as well as brief interventions, are on the list.'}.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 90%|█████████ | 9/10 [00:00<00:00, 11.05ba/s][1,5]<stderr>:[INFO|trainer.py:516] 2022-05-02 13:47:22,739 >> The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: premise, idx, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015100%|██████████| 10/10 [00:00<00:00, 11.65ba/s]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:__main__:Sample 335243 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'hypothesis': \"Parents are busy and it's sometimes hard to get them out.\", 'idx': 335243, 'input_ids': [0, 6968, 216, 77, 49, 1041, 283, 8, 24, 18, 543, 7, 120, 106, 66, 8, 10, 319, 9, 1041, 33, 2127, 7, 213, 8, 8, 383, 101, 14, 8, 24, 18, 628, 23, 363, 98, 2, 2, 35835, 32, 3610, 8, 24, 18, 2128, 543, 7, 120, 106, 66, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 0, 'premise': \"you know when their parents come and it's hard to get them out and a lot of parents have places to go and and things like that and it's late at night so\"}.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:__main__:Sample 58369 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'hypothesis': 'Where and what is art? ', 'idx': 58369, 'input_ids': [0, 13841, 16, 1808, 116, 2, 2, 13841, 8, 99, 16, 1808, 116, 1437, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 1, 'premise': 'Where is art?'}.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:__main__:Sample 13112 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'hypothesis': 'The list says alcohol and injury are negatives facing staff.', 'idx': 13112, 'input_ids': [0, 7083, 45270, 8, 1356, 6, 25, 157, 25, 4315, 15985, 6, 32, 15, 5, 889, 4, 2, 2, 133, 889, 161, 3766, 8, 1356, 32, 34784, 2114, 813, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 1, 'premise': 'Alcohol and injury, as well as brief interventions, are on the list.'}.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:[INFO|trainer.py:398] 2022-05-02 13:47:22,829 >> max_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[INFO|trainer.py:1156] 2022-05-02 13:47:22,844 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[INFO|trainer.py:1157] 2022-05-02 13:47:22,844 >>   Num examples = 392702\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[INFO|trainer.py:1158] 2022-05-02 13:47:22,845 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[INFO|trainer.py:1159] 2022-05-02 13:47:22,845 >>   Instantaneous batch size per device = 16\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[INFO|trainer.py:1160] 2022-05-02 13:47:22,845 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[INFO|trainer.py:1161] 2022-05-02 13:47:22,845 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[INFO|trainer.py:1162] 2022-05-02 13:47:22,845 >>   Total optimization steps = 500\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:[INFO|trainer.py:398] 2022-05-02 13:47:22,945 >> max_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:[INFO|trainer.py:516] 2022-05-02 13:47:22,952 >> The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: premise, idx, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:398] 2022-05-02 13:47:22,955 >> max_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:[INFO|trainer.py:1156] 2022-05-02 13:47:23,005 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:[INFO|trainer.py:1157] 2022-05-02 13:47:23,005 >>   Num examples = 392702\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:[INFO|trainer.py:1158] 2022-05-02 13:47:23,005 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:[INFO|trainer.py:1159] 2022-05-02 13:47:23,006 >>   Instantaneous batch size per device = 16\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:[INFO|trainer.py:1160] 2022-05-02 13:47:23,006 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:[INFO|trainer.py:1161] 2022-05-02 13:47:23,006 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:[INFO|trainer.py:1162] 2022-05-02 13:47:23,006 >>   Total optimization steps = 500\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:516] 2022-05-02 13:47:23,096 >> The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: hypothesis, idx, premise.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:[INFO|trainer.py:516] 2022-05-02 13:47:23,096 >> The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:1156] 2022-05-02 13:47:23,141 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:1157] 2022-05-02 13:47:23,141 >>   Num examples = 392702\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:1158] 2022-05-02 13:47:23,141 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:1159] 2022-05-02 13:47:23,141 >>   Instantaneous batch size per device = 16\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:1160] 2022-05-02 13:47:23,141 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:1161] 2022-05-02 13:47:23,141 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:1162] 2022-05-02 13:47:23,141 >>   Total optimization steps = 500\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:[INFO|trainer.py:1156] 2022-05-02 13:47:23,146 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:[INFO|trainer.py:1157] 2022-05-02 13:47:23,146 >>   Num examples = 392702\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:[INFO|trainer.py:1158] 2022-05-02 13:47:23,147 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:[INFO|trainer.py:1159] 2022-05-02 13:47:23,147 >>   Instantaneous batch size per device = 16\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:[INFO|trainer.py:1160] 2022-05-02 13:47:23,147 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:[INFO|trainer.py:1161] 2022-05-02 13:47:23,147 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:[INFO|trainer.py:1162] 2022-05-02 13:47:23,147 >>   Total optimization steps = 500\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  0%|          | 0/500 [00:00<?, ?it/s][1,0]<stderr>:#015  0%|          | 1/500 [00:06<56:54,  6.84s/it][1,0]<stderr>:#015  0%|          | 2/500 [00:08<43:13,  5.21s/it][1,0]<stderr>:#015  1%|          | 3/500 [00:09<33:29,  4.04s/it][1,0]<stderr>:#015  1%|          | 4/500 [00:10<26:36,  3.22s/it][1,0]<stderr>:#015  1%|          | 5/500 [00:12<21:55,  2.66s/it][1,0]<stderr>:#015  1%|          | 6/500 [00:13<18:28,  2.24s/it][1,0]<stderr>:#015  1%|▏         | 7/500 [00:14<16:06,  1.96s/it][1,0]<stderr>:#015  2%|▏         | 8/500 [00:16<14:21,  1.75s/it][1,0]<stderr>:#015  2%|▏         | 9/500 [00:17<13:13,  1.62s/it][1,0]<stderr>:#015  2%|▏         | 10/500 [00:18<12:34,  1.54s/it][1,0]<stderr>:#015  2%|▏         | 11/500 [00:19<11:53,  1.46s/it][1,0]<stderr>:#015  2%|▏         | 12/500 [00:21<11:22,  1.40s/it][1,0]<stderr>:#015  3%|▎         | 13/500 [00:22<11:04,  1.36s/it][1,0]<stderr>:#015  3%|▎         | 14/500 [00:23<10:50,  1.34s/it][1,0]<stderr>:#015  3%|▎         | 15/500 [00:25<10:39,  1.32s/it][1,0]<stderr>:#015  3%|▎         | 16/500 [00:26<10:27,  1.30s/it][1,0]<stderr>:#015  3%|▎         | 17/500 [00:27<10:21,  1.29s/it][1,0]<stderr>:#015  4%|▎         | 18/500 [00:28<10:13,  1.27s/it][1,0]<stderr>:#015  4%|▍         | 19/500 [00:30<10:08,  1.26s/it][1,0]<stderr>:#015  4%|▍         | 20/500 [00:31<10:04,  1.26s/it][1,0]<stderr>:#015  4%|▍         | 21/500 [00:32<10:06,  1.27s/it][1,0]<stderr>:#015  4%|▍         | 22/500 [00:33<10:11,  1.28s/it][1,0]<stderr>:#015  5%|▍         | 23/500 [00:35<10:08,  1.27s/it][1,0]<stderr>:#015  5%|▍         | 24/500 [00:36<10:17,  1.30s/it][1,0]<stderr>:#015  5%|▌         | 25/500 [00:37<10:12,  1.29s/it][1,0]<stderr>:#015  5%|▌         | 26/500 [00:39<10:15,  1.30s/it][1,0]<stderr>:#015  5%|▌         | 27/500 [00:40<10:07,  1.28s/it][1,0]<stderr>:#015  6%|▌         | 28/500 [00:41<10:10,  1.29s/it][1,0]<stderr>:#015  6%|▌         | 29/500 [00:43<10:16,  1.31s/it][1,0]<stderr>:#015  6%|▌         | 30/500 [00:44<10:15,  1.31s/it][1,0]<stderr>:#015  6%|▌         | 31/500 [00:45<10:07,  1.29s/it][1,0]<stderr>:#015  6%|▋         | 32/500 [00:46<10:03,  1.29s/it][1,0]<stderr>:#015  7%|▋         | 33/500 [00:48<09:58,  1.28s/it][1,0]<stderr>:#015  7%|▋         | 34/500 [00:49<09:53,  1.27s/it][1,0]<stderr>:#015  7%|▋         | 35/500 [00:50<09:59,  1.29s/it][1,0]<stderr>:#015  7%|▋         | 36/500 [00:52<10:03,  1.30s/it][1,0]<stderr>:#015  7%|▋         | 37/500 [00:53<10:03,  1.30s/it][1,0]<stderr>:#015  8%|▊         | 38/500 [00:54<10:01,  1.30s/it][1,0]<stderr>:#015  8%|▊         | 39/500 [00:55<09:54,  1.29s/it][1,0]<stderr>:#015  8%|▊         | 40/500 [00:57<09:56,  1.30s/it][1,0]<stderr>:#015  8%|▊         | 41/500 [00:58<09:48,  1.28s/it][1,0]<stderr>:#015  8%|▊         | 42/500 [00:59<09:39,  1.27s/it][1,0]<stderr>:#015  9%|▊         | 43/500 [01:00<09:36,  1.26s/it][1,0]<stderr>:#015  9%|▉         | 44/500 [01:02<09:34,  1.26s/it][1,0]<stderr>:#015  9%|▉         | 45/500 [01:03<09:34,  1.26s/it][1,0]<stderr>:#015  9%|▉         | 46/500 [01:04<09:40,  1.28s/it][1,0]<stderr>:#015  9%|▉         | 47/500 [01:06<09:42,  1.29s/it][1,0]<stderr>:#015 10%|▉         | 48/500 [01:07<09:43,  1.29s/it][1,0]<stderr>:#015 10%|▉         | 49/500 [01:08<09:43,  1.29s/it][1,0]<stderr>:#015 10%|█         | 50/500 [01:09<09:38,  1.29s/it][1,0]<stderr>:#015 10%|█         | 51/500 [01:11<09:34,  1.28s/it][1,0]<stderr>:#015 10%|█         | 52/500 [01:12<09:39,  1.29s/it][1,0]<stderr>:#015 11%|█         | 53/500 [01:13<09:37,  1.29s/it][1,0]<stderr>:#015 11%|█         | 54/500 [01:15<09:34,  1.29s/it][1,0]<stderr>:#015 11%|█         | 55/500 [01:16<09:28,  1.28s/it][1,0]<stderr>:#015 11%|█         | 56/500 [01:17<09:26,  1.27s/it][1,0]<stderr>:#015 11%|█▏        | 57/500 [01:18<09:25,  1.28s/it][1,0]<stderr>:#015 12%|█▏        | 58/500 [01:20<09:31,  1.29s/it][1,0]<stderr>:#015 12%|█▏        | 59/500 [01:21<09:25,  1.28s/it][1,0]<stderr>:#015 12%|█▏        | 60/500 [01:22<09:28,  1.29s/it][1,0]<stderr>:#015 12%|█▏        | 61/500 [01:24<09:24,  1.29s/it][1,0]<stderr>:#015 12%|█▏        | 62/500 [01:25<09:27,  1.29s/it][1,0]<stderr>:#015 13%|█▎        | 63/500 [01:26<09:21,  1.28s/it][1,0]<stderr>:#015 13%|█▎        | 64/500 [01:27<09:19,  1.28s/it][1,0]<stderr>:#015 13%|█▎        | 65/500 [01:29<09:15,  1.28s/it][1,0]<stderr>:#015 13%|█▎        | 66/500 [01:30<09:12,  1.27s/it][1,0]<stderr>:#015 13%|█▎        | 67/500 [01:31<09:16,  1.28s/it][1,0]<stderr>:#015 14%|█▎        | 68/500 [01:33<09:12,  1.28s/it][1,0]<stderr>:#015 14%|█▍        | 69/500 [01:34<09:10,  1.28s/it][1,0]<stderr>:#015 14%|█▍        | 70/500 [01:35<09:13,  1.29s/it][1,0]<stderr>:#015 14%|█▍        | 71/500 [01:36<09:19,  1.31s/it][1,0]<stderr>:#015 14%|█▍        | 72/500 [01:38<09:17,  1.30s/it][1,0]<stderr>:#015 15%|█▍        | 73/500 [01:39<09:34,  1.34s/it][1,0]<stderr>:#015 15%|█▍        | 74/500 [01:41<09:26,  1.33s/it][1,0]<stderr>:#015 15%|█▌        | 75/500 [01:42<09:26,  1.33s/it][1,0]<stderr>:#015 15%|█▌        | 76/500 [01:43<09:32,  1.35s/it][1,0]<stderr>:#015 15%|█▌        | 77/500 [01:44<09:14,  1.31s/it][1,0]<stderr>:#015 16%|█▌        | 78/500 [01:46<09:11,  1.31s/it][1,0]<stderr>:#015 16%|█▌        | 79/500 [01:47<09:05,  1.30s/it][1,0]<stderr>:#015 16%|█▌        | 80/500 [01:48<08:59,  1.28s/it][1,0]<stderr>:#015 16%|█▌        | 81/500 [01:50<09:01,  1.29s/it][1,0]<stderr>:#015 16%|█▋        | 82/500 [01:51<09:08,  1.31s/it][1,0]<stderr>:#015 17%|█▋        | 83/500 [01:52<09:02,  1.30s/it][1,0]<stderr>:#015 17%|█▋        | 84/500 [01:54<09:03,  1.31s/it][1,0]<stderr>:#015 17%|█▋        | 85/500 [01:55<09:03,  1.31s/it][1,0]<stderr>:#015 17%|█▋        | 86/500 [01:56<09:01,  1.31s/it][1,0]<stderr>:#015 17%|█▋        | 87/500 [01:57<08:58,  1.30s/it][1,0]<stderr>:#015 18%|█▊        | 88/500 [01:59<08:59,  1.31s/it][1,0]<stderr>:#015 18%|█▊        | 89/500 [02:00<08:52,  1.30s/it][1,0]<stderr>:#015 18%|█▊        | 90/500 [02:01<08:47,  1.29s/it][1,0]<stderr>:#015 18%|█▊        | 91/500 [02:03<08:45,  1.28s/it][1,0]<stderr>:#015 18%|█▊        | 92/500 [02:04<08:46,  1.29s/it][1,0]<stderr>:#015 19%|█▊        | 93/500 [02:05<08:40,  1.28s/it][1,0]<stderr>:#015 19%|█▉        | 94/500 [02:07<08:48,  1.30s/it][1,0]<stderr>:#015 19%|█▉        | 95/500 [02:08<08:49,  1.31s/it][1,0]<stderr>:#015 19%|█▉        | 96/500 [02:09<08:44,  1.30s/it][1,0]<stderr>:#015 19%|█▉        | 97/500 [02:10<08:40,  1.29s/it][1,0]<stderr>:#015 20%|█▉        | 98/500 [02:12<08:36,  1.28s/it][1,0]<stderr>:#015 20%|█▉        | 99/500 [02:13<08:36,  1.29s/it][1,0]<stderr>:#015 20%|██        | 100/500 [02:14<08:32,  1.28s/it][1,0]<stderr>:#015 20%|██        | 101/500 [02:16<08:31,  1.28s/it][1,0]<stderr>:#015 20%|██        | 102/500 [02:17<08:36,  1.30s/it][1,0]<stderr>:#015 21%|██        | 103/500 [02:18<08:34,  1.29s/it][1,0]<stderr>:#015 21%|██        | 104/500 [02:19<08:33,  1.30s/it][1,0]<stderr>:#015 21%|██        | 105/500 [02:21<08:32,  1.30s/it][1,0]<stderr>:#015 21%|██        | 106/500 [02:22<08:36,  1.31s/it][1,0]<stderr>:#015 21%|██▏       | 107/500 [02:23<08:29,  1.30s/it][1,0]<stderr>:#015 22%|██▏       | 108/500 [02:25<08:22,  1.28s/it][1,0]<stderr>:#015 22%|██▏       | 109/500 [02:26<08:26,  1.30s/it][1,0]<stderr>:#015 22%|██▏       | 110/500 [02:27<08:23,  1.29s/it][1,0]<stderr>:#015 22%|██▏       | 111/500 [02:28<08:21,  1.29s/it][1,0]<stderr>:#015 22%|██▏       | 112/500 [02:30<08:16,  1.28s/it][1,0]<stderr>:#015 23%|██▎       | 113/500 [02:31<08:11,  1.27s/it][1,0]<stderr>:#015 23%|██▎       | 114/500 [02:32<08:11,  1.27s/it][1,0]<stderr>:#015 23%|██▎       | 115/500 [02:34<08:10,  1.27s/it][1,0]<stderr>:#015 23%|██▎       | 116/500 [02:35<08:13,  1.29s/it][1,0]<stderr>:#015 23%|██▎       | 117/500 [02:36<08:13,  1.29s/it][1,0]<stderr>:#015 24%|██▎       | 118/500 [02:37<08:14,  1.29s/it][1,0]<stderr>:#015 24%|██▍       | 119/500 [02:39<08:07,  1.28s/it][1,0]<stderr>:#015 24%|██▍       | 120/500 [02:40<08:10,  1.29s/it][1,0]<stderr>:#015 24%|██▍       | 121/500 [02:41<08:06,  1.28s/it][1,0]<stderr>:#015 24%|██▍       | 122/500 [02:43<08:09,  1.30s/it][1,0]<stderr>:#015 25%|██▍       | 123/500 [02:44<08:05,  1.29s/it][1,0]<stderr>:#015 25%|██▍       | 124/500 [02:45<08:14,  1.32s/it][1,0]<stderr>:#015 25%|██▌       | 125/500 [02:47<08:08,  1.30s/it][1,0]<stderr>:#015 25%|██▌       | 126/500 [02:48<08:02,  1.29s/it][1,0]<stderr>:#015 25%|██▌       | 127/500 [02:49<08:00,  1.29s/it][1,0]<stderr>:#015 26%|██▌       | 128/500 [02:50<08:00,  1.29s/it][1,0]<stderr>:#015 26%|██▌       | 129/500 [02:52<07:58,  1.29s/it][1,0]<stderr>:#015 26%|██▌       | 130/500 [02:53<07:55,  1.29s/it][1,0]<stderr>:#015 26%|██▌       | 131/500 [02:54<07:51,  1.28s/it][1,0]<stderr>:#015 26%|██▋       | 132/500 [02:55<07:50,  1.28s/it][1,0]<stderr>:#015 27%|██▋       | 133/500 [02:57<07:45,  1.27s/it][1,0]<stderr>:#015 27%|██▋       | 134/500 [02:58<07:57,  1.31s/it][1,0]<stderr>:#015 27%|██▋       | 135/500 [02:59<07:53,  1.30s/it][1,0]<stderr>:#015 27%|██▋       | 136/500 [03:01<07:50,  1.29s/it][1,0]<stderr>:#015 27%|██▋       | 137/500 [03:02<07:45,  1.28s/it][1,0]<stderr>:#015 28%|██▊       | 138/500 [03:03<07:46,  1.29s/it][1,0]<stderr>:#015 28%|██▊       | 139/500 [03:05<07:42,  1.28s/it][1,0]<stderr>:#015 28%|██▊       | 140/500 [03:06<07:48,  1.30s/it][1,0]<stderr>:#015 28%|██▊       | 141/500 [03:07<07:44,  1.29s/it][1,0]<stderr>:#015 28%|██▊       | 142/500 [03:08<07:43,  1.30s/it][1,0]<stderr>:#015 29%|██▊       | 143/500 [03:10<07:41,  1.29s/it][1,0]<stderr>:#015 29%|██▉       | 144/500 [03:11<07:39,  1.29s/it][1,0]<stderr>:#015 29%|██▉       | 145/500 [03:12<07:32,  1.28s/it][1,0]<stderr>:#015 29%|██▉       | 146/500 [03:14<07:30,  1.27s/it][1,0]<stderr>:#015 29%|██▉       | 147/500 [03:15<07:31,  1.28s/it][1,0]<stderr>:#015 30%|██▉       | 148/500 [03:16<07:33,  1.29s/it][1,0]<stderr>:#015 30%|██▉       | 149/500 [03:17<07:35,  1.30s/it][1,0]<stderr>:#015 30%|███       | 150/500 [03:19<07:37,  1.31s/it][1,0]<stderr>:#015 30%|███       | 151/500 [03:20<07:32,  1.30s/it][1,0]<stderr>:#015 30%|███       | 152/500 [03:21<07:26,  1.28s/it][1,0]<stderr>:#015 31%|███       | 153/500 [03:23<07:22,  1.27s/it][1,0]<stderr>:#015 31%|███       | 154/500 [03:24<07:23,  1.28s/it][1,0]<stderr>:#015 31%|███       | 155/500 [03:25<07:23,  1.28s/it][1,0]<stderr>:#015 31%|███       | 156/500 [03:26<07:18,  1.28s/it][1,0]<stderr>:#015 31%|███▏      | 157/500 [03:28<07:14,  1.27s/it][1,0]<stderr>:#015 32%|███▏      | 158/500 [03:29<07:16,  1.28s/it][1,0]<stderr>:#015 32%|███▏      | 159/500 [03:30<07:17,  1.28s/it][1,0]<stderr>:#015 32%|███▏      | 160/500 [03:32<07:15,  1.28s/it][1,0]<stderr>:#015 32%|███▏      | 161/500 [03:33<07:15,  1.29s/it][1,0]<stderr>:#015 32%|███▏      | 162/500 [03:34<07:20,  1.30s/it][1,0]<stderr>:#015 33%|███▎      | 163/500 [03:35<07:21,  1.31s/it][1,0]<stderr>:#015 33%|███▎      | 164/500 [03:37<07:23,  1.32s/it][1,0]<stderr>:#015 33%|███▎      | 165/500 [03:38<07:18,  1.31s/it][1,0]<stderr>:#015 33%|███▎      | 166/500 [03:39<07:16,  1.31s/it][1,0]<stderr>:#015 33%|███▎      | 167/500 [03:41<07:13,  1.30s/it][1,0]<stderr>:#015 34%|███▎      | 168/500 [03:42<07:13,  1.31s/it][1,0]<stderr>:#015 34%|███▍      | 169/500 [03:43<07:07,  1.29s/it][1,0]<stderr>:#015 34%|███▍      | 170/500 [03:44<07:01,  1.28s/it][1,0]<stderr>:#015 34%|███▍      | 171/500 [03:46<07:02,  1.28s/it][1,0]<stderr>:#015 34%|███▍      | 172/500 [03:47<07:03,  1.29s/it][1,0]<stderr>:#015 35%|███▍      | 173/500 [03:48<06:58,  1.28s/it][1,0]<stderr>:#015 35%|███▍      | 174/500 [03:50<06:55,  1.27s/it][1,0]<stderr>:#015 35%|███▌      | 175/500 [03:51<06:55,  1.28s/it][1,0]<stderr>:#015 35%|███▌      | 176/500 [03:52<06:52,  1.27s/it][1,0]<stderr>:#015 35%|███▌      | 177/500 [03:53<06:46,  1.26s/it][1,0]<stderr>:#015 36%|███▌      | 178/500 [03:55<06:47,  1.27s/it][1,0]<stderr>:#015 36%|███▌      | 179/500 [03:56<06:51,  1.28s/it][1,0]<stderr>:#015 36%|███▌      | 180/500 [03:57<06:55,  1.30s/it][1,0]<stderr>:#015 36%|███▌      | 181/500 [03:59<06:54,  1.30s/it][1,0]<stderr>:#015 36%|███▋      | 182/500 [04:00<06:49,  1.29s/it][1,0]<stderr>:#015 37%|███▋      | 183/500 [04:01<06:57,  1.32s/it][1,0]<stderr>:#015 37%|███▋      | 184/500 [04:03<06:52,  1.30s/it][1,0]<stderr>:#015 37%|███▋      | 185/500 [04:04<06:46,  1.29s/it][1,0]<stderr>:#015 37%|███▋      | 186/500 [04:05<06:49,  1.30s/it][1,0]<stderr>:#015 37%|███▋      | 187/500 [04:06<06:42,  1.28s/it][1,0]<stderr>:#015 38%|███▊      | 188/500 [04:08<06:41,  1.29s/it][1,0]<stderr>:#015 38%|███▊      | 189/500 [04:09<06:38,  1.28s/it][1,0]<stderr>:#015 38%|███▊      | 190/500 [04:10<06:41,  1.29s/it][1,0]<stderr>:#015 38%|███▊      | 191/500 [04:12<06:39,  1.29s/it][1,0]<stderr>:#015 38%|███▊      | 192/500 [04:13<06:37,  1.29s/it][1,0]<stderr>:#015 39%|███▊      | 193/500 [04:14<06:47,  1.33s/it][1,0]<stderr>:#015 39%|███▉      | 194/500 [04:16<06:46,  1.33s/it][1,0]<stderr>:#015 39%|███▉      | 195/500 [04:17<06:35,  1.30s/it][1,0]<stderr>:#015 39%|███▉      | 196/500 [04:18<06:35,  1.30s/it][1,0]<stderr>:#015 39%|███▉      | 197/500 [04:19<06:33,  1.30s/it][1,0]<stderr>:#015 40%|███▉      | 198/500 [04:21<06:33,  1.30s/it][1,0]<stderr>:#015 40%|███▉      | 199/500 [04:22<06:32,  1.31s/it][1,0]<stderr>:#015 40%|████      | 200/500 [04:23<06:32,  1.31s/it][1,0]<stderr>:#015 40%|████      | 201/500 [04:25<06:34,  1.32s/it][1,0]<stderr>:#015 40%|████      | 202/500 [04:26<06:29,  1.31s/it][1,0]<stderr>:#015 41%|████      | 203/500 [04:27<06:22,  1.29s/it][1,0]<stderr>:#015 41%|████      | 204/500 [04:28<06:18,  1.28s/it][1,0]<stderr>:#015 41%|████      | 205/500 [04:30<06:16,  1.28s/it][1,0]<stderr>:#015 41%|████      | 206/500 [04:31<06:15,  1.28s/it][1,0]<stderr>:#015 41%|████▏     | 207/500 [04:32<06:12,  1.27s/it][1,0]<stderr>:#015 42%|████▏     | 208/500 [04:34<06:11,  1.27s/it][1,0]<stderr>:#015 42%|████▏     | 209/500 [04:35<06:10,  1.27s/it][1,0]<stderr>:#015 42%|████▏     | 210/500 [04:36<06:07,  1.27s/it][1,0]<stderr>:#015 42%|████▏     | 211/500 [04:37<06:14,  1.29s/it][1,0]<stderr>:#015 42%|████▏     | 212/500 [04:39<06:11,  1.29s/it][1,0]<stderr>:#015 43%|████▎     | 213/500 [04:40<06:11,  1.30s/it][1,0]<stderr>:#015 43%|████▎     | 214/500 [04:41<06:16,  1.32s/it][1,0]<stderr>:#015 43%|████▎     | 215/500 [04:43<06:11,  1.30s/it][1,0]<stderr>:#015 43%|████▎     | 216/500 [04:44<06:08,  1.30s/it][1,0]<stderr>:#015 43%|████▎     | 217/500 [04:45<06:06,  1.29s/it][1,0]<stderr>:#015 44%|████▎     | 218/500 [04:47<06:06,  1.30s/it][1,0]<stderr>:#015 44%|████▍     | 219/500 [04:48<06:04,  1.30s/it][1,0]<stderr>:#015 44%|████▍     | 220/500 [04:49<06:03,  1.30s/it][1,0]<stderr>:#015 44%|████▍     | 221/500 [04:50<05:58,  1.29s/it][1,0]<stderr>:#015 44%|████▍     | 222/500 [04:52<05:56,  1.28s/it][1,0]<stderr>:#015 45%|████▍     | 223/500 [04:53<05:55,  1.28s/it][1,0]<stderr>:#015 45%|████▍     | 224/500 [04:54<05:59,  1.30s/it][1,0]<stderr>:#015 45%|████▌     | 225/500 [04:56<05:56,  1.30s/it][1,0]<stderr>:#015 45%|████▌     | 226/500 [04:57<06:00,  1.32s/it][1,0]<stderr>:#015 45%|████▌     | 227/500 [04:58<05:58,  1.31s/it][1,0]<stderr>:#015 46%|████▌     | 228/500 [05:00<05:57,  1.32s/it][1,0]<stderr>:#015 46%|████▌     | 229/500 [05:01<05:52,  1.30s/it][1,0]<stderr>:#015 46%|████▌     | 230/500 [05:02<05:52,  1.31s/it][1,0]<stderr>:#015 46%|████▌     | 231/500 [05:03<05:51,  1.31s/it][1,0]<stderr>:#015 46%|████▋     | 232/500 [05:05<05:47,  1.29s/it][1,0]<stderr>:#015 47%|████▋     | 233/500 [05:06<05:44,  1.29s/it][1,0]<stderr>:#015 47%|████▋     | 234/500 [05:07<05:42,  1.29s/it][1,0]<stderr>:#015 47%|████▋     | 235/500 [05:09<05:42,  1.29s/it][1,0]<stderr>:#015 47%|████▋     | 236/500 [05:10<05:42,  1.30s/it][1,0]<stderr>:#015 47%|████�\u001b[0m\n",
      "\u001b[34m�     | 237/500 [05:11<05:44,  1.31s/it][1,0]<stderr>:#015 48%|████▊     | 238/500 [05:13<05:42,  1.31s/it][1,0]<stderr>:#015 48%|████▊     | 239/500 [05:14<05:43,  1.31s/it][1,0]<stderr>:#015 48%|████▊     | 240/500 [05:15<05:39,  1.31s/it][1,0]<stderr>:#015 48%|████▊     | 241/500 [05:16<05:35,  1.30s/it][1,0]<stderr>:#015 48%|████▊     | 242/500 [05:18<05:33,  1.29s/it][1,0]<stderr>:#015 49%|████▊     | 243/500 [05:19<05:29,  1.28s/it][1,0]<stderr>:#015 49%|████▉     | 244/500 [05:20<05:28,  1.28s/it][1,0]<stderr>:#015 49%|████▉     | 245/500 [05:22<05:23,  1.27s/it][1,0]<stderr>:#015 49%|████▉     | 246/500 [05:23<05:26,  1.28s/it][1,0]<stderr>:#015 49%|████▉     | 247/500 [05:24<05:24,  1.28s/it][1,0]<stderr>:#015 50%|████▉     | 248/500 [05:25<05:23,  1.28s/it][1,0]<stderr>:#015 50%|████▉     | 249/500 [05:27<05:22,  1.28s/it][1,0]<stderr>:#015 50%|█████     | 250/500 [05:28<05:25,  1.30s/it][1,0]<stderr>:#015 50%|█████     | 251/500 [05:29<05:21,  1.29s/it][1,0]<stderr>:#015 50%|█████     | 252/500 [05:31<05:17,  1.28s/it][1,0]<stderr>:#015 51%|█████     | 253/500 [05:32<05:20,  1.30s/it][1,0]<stderr>:#015 51%|█████     | 254/500 [05:33<05:14,  1.28s/it][1,0]<stderr>:#015 51%|█████     | 255/500 [05:34<05:15,  1.29s/it][1,0]<stderr>:#015 51%|█████     | 256/500 [05:36<05:14,  1.29s/it][1,0]<stderr>:#015 51%|█████▏    | 257/500 [05:37<05:13,  1.29s/it][1,0]<stderr>:#015 52%|█████▏    | 258/500 [05:38<05:11,  1.29s/it][1,0]<stderr>:#015 52%|█████▏    | 259/500 [05:40<05:12,  1.30s/it][1,0]<stderr>:#015 52%|█████▏    | 260/500 [05:41<05:10,  1.30s/it][1,0]<stderr>:#015 52%|█████▏    | 261/500 [05:42<05:12,  1.31s/it][1,0]<stderr>:#015 52%|█████▏    | 262/500 [05:44<05:08,  1.30s/it][1,0]<stderr>:#015 53%|█████▎    | 263/500 [05:45<05:06,  1.29s/it][1,0]<stderr>:#015 53%|█████▎    | 264/500 [05:46<05:06,  1.30s/it][1,0]<stderr>:#015 53%|█████▎    | 265/500 [05:47<05:05,  1.30s/it][1,0]<stderr>:#015 53%|█████▎    | 266/500 [05:49<05:02,  1.29s/it][1,0]<stderr>:#015 53%|█████▎    | 267/500 [05:50<05:04,  1.31s/it][1,0]<stderr>:#015 54%|█████▎    | 268/500 [05:51<05:01,  1.30s/it][1,0]<stderr>:#015 54%|█████▍    | 269/500 [05:53<05:00,  1.30s/it][1,0]<stderr>:#015 54%|█████▍    | 270/500 [05:54<04:57,  1.29s/it][1,0]<stderr>:#015 54%|█████▍    | 271/500 [05:55<04:55,  1.29s/it][1,0]<stderr>:#015 54%|█████▍    | 272/500 [05:56<04:55,  1.29s/it][1,0]<stderr>:#015 55%|█████▍    | 273/500 [05:58<04:52,  1.29s/it][1,0]<stderr>:#015 55%|█████▍    | 274/500 [05:59<04:49,  1.28s/it][1,0]<stderr>:#015 55%|█████▌    | 275/500 [06:00<04:46,  1.27s/it][1,0]<stderr>:#015 55%|█████▌    | 276/500 [06:02<04:43,  1.27s/it][1,0]<stderr>:#015 55%|█████▌    | 277/500 [06:03<04:42,  1.27s/it][1,0]<stderr>:#015 56%|█████▌    | 278/500 [06:04<04:44,  1.28s/it][1,0]<stderr>:#015 56%|█████▌    | 279/500 [06:05<04:40,  1.27s/it][1,0]<stderr>:#015 56%|█████▌    | 280/500 [06:07<04:41,  1.28s/it][1,0]<stderr>:#015 56%|█████▌    | 281/500 [06:08<04:38,  1.27s/it][1,0]<stderr>:#015 56%|█████▋    | 282/500 [06:09<04:45,  1.31s/it][1,0]<stderr>:#015 57%|█████▋    | 283/500 [06:11<04:42,  1.30s/it][1,0]<stderr>:#015 57%|█████▋    | 284/500 [06:12<04:38,  1.29s/it][1,0]<stderr>:#015 57%|█████▋    | 285/500 [06:13<04:42,  1.31s/it][1,0]<stderr>:#015 57%|█████▋    | 286/500 [06:14<04:37,  1.30s/it][1,0]<stderr>:#015 57%|█████▋    | 287/500 [06:16<04:34,  1.29s/it][1,0]<stderr>:#015 58%|█████▊    | 288/500 [06:17<04:35,  1.30s/it][1,0]<stderr>:#015 58%|█████▊    | 289/500 [06:18<04:36,  1.31s/it][1,0]<stderr>:#015 58%|█████▊    | 290/500 [06:20<04:31,  1.30s/it][1,0]<stderr>:#015 58%|█████▊    | 291/500 [06:21<04:26,  1.28s/it][1,0]<stderr>:#015 58%|█████▊    | 292/500 [06:22<04:30,  1.30s/it][1,0]<stderr>:#015 59%|█████▊    | 293/500 [06:24<04:30,  1.31s/it][1,0]<stderr>:#015 59%|█████▉    | 294/500 [06:25<04:30,  1.31s/it][1,0]<stderr>:#015 59%|█████▉    | 295/500 [06:26<04:28,  1.31s/it][1,0]<stderr>:#015 59%|█████▉    | 296/500 [06:28<04:26,  1.31s/it][1,0]<stderr>:#015 59%|█████▉    | 297/500 [06:29<04:27,  1.32s/it][1,0]<stderr>:#015 60%|█████▉    | 298/500 [06:30<04:25,  1.32s/it][1,0]<stderr>:#015 60%|█████▉    | 299/500 [06:31<04:22,  1.30s/it][1,0]<stderr>:#015 60%|██████    | 300/500 [06:33<04:25,  1.33s/it][1,0]<stderr>:#015 60%|██████    | 301/500 [06:34<04:21,  1.32s/it][1,0]<stderr>:#015 60%|██████    | 302/500 [06:35<04:18,  1.31s/it][1,0]<stderr>:#015 61%|██████    | 303/500 [06:37<04:21,  1.33s/it][1,0]<stderr>:#015 61%|██████    | 304/500 [06:38<04:17,  1.32s/it][1,0]<stderr>:#015 61%|██████    | 305/500 [06:39<04:13,  1.30s/it][1,0]<stderr>:#015 61%|██████    | 306/500 [06:41<04:12,  1.30s/it][1,0]<stderr>:#015 61%|██████▏   | 307/500 [06:42<04:08,  1.29s/it][1,0]<stderr>:#015 62%|██████▏   | 308/500 [06:43<04:09,  1.30s/it][1,0]<stderr>:#015 62%|██████▏   | 309/500 [06:44<04:06,  1.29s/it][1,0]<stderr>:#015 62%|██████▏   | 310/500 [06:46<04:02,  1.27s/it][1,0]<stderr>:#015 62%|██████▏   | 311/500 [06:47<04:07,  1.31s/it][1,0]<stderr>:#015 62%|██████▏   | 312/500 [06:48<04:04,  1.30s/it][1,0]<stderr>:#015 63%|██████▎   | 313/500 [06:50<04:01,  1.29s/it][1,0]<stderr>:#015 63%|██████▎   | 314/500 [06:51<04:08,  1.33s/it][1,0]<stderr>:#015 63%|██████▎   | 315/500 [06:52<04:06,  1.33s/it][1,0]<stderr>:#015 63%|██████▎   | 316/500 [06:54<04:02,  1.32s/it][1,0]<stderr>:#015 63%|██████▎   | 317/500 [06:55<04:00,  1.31s/it][1,0]<stderr>:#015 64%|██████▎   | 318/500 [06:56<03:56,  1.30s/it][1,0]<stderr>:#015 64%|██████▍   | 319/500 [06:58<03:54,  1.30s/it][1,0]<stderr>:#015 64%|██████▍   | 320/500 [06:59<03:50,  1.28s/it][1,0]<stderr>:#015 64%|██████▍   | 321/500 [07:00<03:46,  1.27s/it][1,0]<stderr>:#015 64%|██████▍   | 322/500 [07:01<03:45,  1.26s/it][1,0]<stderr>:#015 65%|██████▍   | 323/500 [07:03<03:44,  1.27s/it][1,0]<stderr>:#015 65%|██████▍   | 324/500 [07:04<03:42,  1.27s/it][1,0]<stderr>:#015 65%|██████▌   | 325/500 [07:05<03:43,  1.27s/it][1,0]<stderr>:#015 65%|██████▌   | 326/500 [07:06<03:44,  1.29s/it][1,0]<stderr>:#015 65%|██████▌   | 327/500 [07:08<03:41,  1.28s/it][1,0]<stderr>:#015 66%|██████▌   | 328/500 [07:09<03:43,  1.30s/it][1,0]<stderr>:#015 66%|██████▌   | 329/500 [07:10<03:43,  1.30s/it][1,0]<stderr>:#015 66%|██████▌   | 330/500 [07:12<03:41,  1.31s/it][1,0]<stderr>:#015 66%|██████▌   | 331/500 [07:13<03:38,  1.29s/it][1,0]<stderr>:#015 66%|██████▋   | 332/500 [07:14<03:35,  1.28s/it][1,0]<stderr>:#015 67%|██████▋   | 333/500 [07:16<03:37,  1.30s/it][1,0]<stderr>:#015 67%|██████▋   | 334/500 [07:17<03:38,  1.32s/it][1,0]<stderr>:#015 67%|██████▋   | 335/500 [07:18<03:34,  1.30s/it][1,0]<stderr>:#015 67%|██████▋   | 336/500 [07:19<03:30,  1.28s/it][1,0]<stderr>:#015 67%|██████▋   | 337/500 [07:21<03:28,  1.28s/it][1,0]<stderr>:#015 68%|██████▊   | 338/500 [07:22<03:28,  1.28s/it][1,0]<stderr>:#015 68%|██████▊   | 339/500 [07:23<03:28,  1.29s/it][1,0]<stderr>:#015 68%|██████▊   | 340/500 [07:25<03:24,  1.28s/it][1,0]<stderr>:#015 68%|██████▊   | 341/500 [07:26<03:24,  1.29s/it][1,0]<stderr>:#015 68%|██████▊   | 342/500 [07:27<03:22,  1.28s/it][1,0]<stderr>:#015 69%|██████▊   | 343/500 [07:28<03:21,  1.29s/it][1,0]<stderr>:#015 69%|██████▉   | 344/500 [07:30<03:22,  1.30s/it][1,0]<stderr>:#015 69%|██████▉   | 345/500 [07:31<03:21,  1.30s/it][1,0]<stderr>:#015 69%|██████▉   | 346/500 [07:32<03:21,  1.31s/it][1,0]<stderr>:#015 69%|██████▉   | 347/500 [07:34<03:21,  1.32s/it][1,0]<stderr>:#015 70%|██████▉   | 348/500 [07:35<03:17,  1.30s/it][1,0]<stderr>:#015 70%|██████▉   | 349/500 [07:36<03:17,  1.31s/it][1,0]<stderr>:#015 70%|███████   | 350/500 [07:38<03:21,  1.34s/it][1,0]<stderr>:#015 70%|███████   | 351/500 [07:39<03:18,  1.34s/it][1,0]<stderr>:#015 70%|███████   | 352/500 [07:40<03:14,  1.31s/it][1,0]<stderr>:#015 71%|███████   | 353/500 [07:42<03:12,  1.31s/it][1,0]<stderr>:#015 71%|███████   | 354/500 [07:43<03:10,  1.30s/it][1,0]<stderr>:#015 71%|███████   | 355/500 [07:44<03:07,  1.29s/it][1,0]<stderr>:#015 71%|███████   | 356/500 [07:45<03:05,  1.29s/it][1,0]<stderr>:#015 71%|███████▏  | 357/500 [07:47<03:03,  1.29s/it][1,0]<stderr>:#015 72%|███████▏  | 358/500 [07:48<03:01,  1.28s/it][1,0]<stderr>:#015 72%|███████▏  | 359/500 [07:49<03:05,  1.32s/it][1,0]<stderr>:#015 72%|███████▏  | 360/500 [07:51<03:00,  1.29s/it][1,0]<stderr>:#015 72%|███████▏  | 361/500 [07:52<02:58,  1.28s/it][1,0]<stderr>:#015 72%|███████▏  | 362/500 [07:53<02:54,  1.27s/it][1,0]<stderr>:#015 73%|███████▎  | 363/500 [07:54<02:54,  1.28s/it][1,0]<stderr>:#015 73%|███████▎  | 364/500 [07:56<02:52,  1.27s/it][1,0]<stderr>:#015 73%|███████▎  | 365/500 [07:57<02:54,  1.29s/it][1,0]<stderr>:#015 73%|███████▎  | 366/500 [07:58<02:54,  1.30s/it][1,0]<stderr>:#015 73%|███████▎  | 367/500 [08:00<02:53,  1.31s/it][1,0]<stderr>:#015 74%|███████▎  | 368/500 [08:01<02:49,  1.28s/it][1,0]<stderr>:#015 74%|███████▍  | 369/500 [08:02<02:49,  1.29s/it][1,0]<stderr>:#015 74%|███████▍  | 370/500 [08:03<02:48,  1.30s/it][1,0]<stderr>:#015 74%|███████▍  | 371/500 [08:05<02:46,  1.29s/it][1,0]<stderr>:#015 74%|███████▍  | 372/500 [08:06<02:44,  1.28s/it][1,0]<stderr>:#015 75%|███████▍  | 373/500 [08:07<02:44,  1.29s/it][1,0]<stderr>:#015 75%|███████▍  | 374/500 [08:09<02:42,  1.29s/it][1,0]<stderr>:#015 75%|███████▌  | 375/500 [08:10<02:41,  1.29s/it][1,0]<stderr>:#015 75%|███████▌  | 376/500 [08:11<02:40,  1.29s/it][1,0]<stderr>:#015 75%|███████▌  | 377/500 [08:13<02:40,  1.30s/it][1,0]<stderr>:#015 76%|███████▌  | 378/500 [08:14<02:39,  1.31s/it][1,0]<stderr>:#015 76%|███████▌  | 379/500 [08:15<02:39,  1.32s/it][1,0]<stderr>:#015 76%|███████▌  | 380/500 [08:16<02:37,  1.31s/it][1,0]<stderr>:#015 76%|███████▌  | 381/500 [08:18<02:35,  1.31s/it][1,0]<stderr>:#015 76%|███████▋  | 382/500 [08:19<02:34,  1.31s/it][1,0]<stderr>:#015 77%|███████▋  | 383/500 [08:20<02:31,  1.29s/it][1,0]<stderr>:#015 77%|███████▋  | 384/500 [08:22<02:30,  1.30s/it][1,0]<stderr>:#015 77%|███████▋  | 385/500 [08:23<02:30,  1.31s/it][1,0]<stderr>:#015 77%|███████▋  | 386/500 [08:24<02:27,  1.29s/it][1,0]<stderr>:#015 77%|███████▋  | 387/500 [08:26<02:25,  1.28s/it][1,0]<stderr>:#015 78%|███████▊  | 388/500 [08:27<02:23,  1.28s/it][1,0]<stderr>:#015 78%|███████▊  | 389/500 [08:28<02:20,  1.26s/it][1,0]<stderr>:#015 78%|███████▊  | 390/500 [08:29<02:19,  1.26s/it][1,0]<stderr>:#015 78%|███████▊  | 391/500 [08:31<02:18,  1.27s/it][1,0]<stderr>:#015 78%|███████▊  | 392/500 [08:32<02:16,  1.27s/it][1,0]<stderr>:#015 79%|███████▊  | 393/500 [08:33<02:17,  1.28s/it][1,0]<stderr>:#015 79%|███████▉  | 394/500 [08:34<02:16,  1.29s/it][1,0]<stderr>:#015 79%|███████▉  | 395/500 [08:36<02:15,  1.29s/it][1,0]<stderr>:#015 79%|███████▉  | 396/500 [08:37<02:14,  1.30s/it][1,0]<stderr>:#015 79%|███████▉  | 397/500 [08:38<02:13,  1.30s/it][1,0]<stderr>:#015 80%|███████▉  | 398/500 [08:40<02:13,  1.31s/it][1,0]<stderr>:#015 80%|███████▉  | 399/500 [08:41<02:13,  1.32s/it][1,0]<stderr>:#015 80%|████████  | 400/500 [08:42<02:12,  1.32s/it][1,0]<stderr>:#015 80%|████████  | 401/500 [08:44<02:08,  1.30s/it][1,0]<stderr>:#015 80%|████████  | 402/500 [08:45<02:06,  1.29s/it][1,0]<stderr>:#015 81%|████████  | 403/500 [08:46<02:04,  1.29s/it][1,0]<stderr>:#015 81%|████████  | 404/500 [08:47<02:03,  1.29s/it][1,0]<stderr>:#015 81%|████████  | 405/500 [08:49<02:03,  1.29s/it][1,0]<stderr>:#015 81%|████████  | 406/500 [08:50<02:01,  1.29s/it][1,0]<stderr>:#015 81%|████████▏ | 407/500 [08:51<01:59,  1.29s/it][1,0]<stderr>:#015 82%|████████▏ | 408/500 [08:53<01:57,  1.27s/it][1,0]<stderr>:#015 82%|████████▏ | 409/500 [08:54<01:56,  1.28s/it][1,0]<stderr>:#015 82%|████████▏ | 410/500 [08:55<01:54,  1.28s/it][1,0]<stderr>:#015 82%|████████▏ | 411/500 [08:56<01:52,  1.26s/it][1,0]<stderr>:#015 82%|████████▏ | 412/500 [08:58<01:52,  1.27s/it][1,0]<stderr>:#015 83%|████████▎ | 413/500 [08:59<01:51,  1.28s/it][1,0]<stderr>:#015 83%|████████▎ | 414/500 [09:00<01:49,  1.27s/it][1,0]<stderr>:#015 83%|████████▎ | 415/500 [09:02<01:49,  1.29s/it][1,0]<stderr>:#015 83%|████████▎ | 416/500 [09:03<01:48,  1.29s/it][1,0]<stderr>:#015 83%|████████▎ | 417/500 [09:04<01:45,  1.27s/it][1,0]<stderr>:#015 84%|████████▎ | 418/500 [09:05<01:43,  1.26s/it][1,0]<stderr>:#015 84%|████████▍ | 419/500 [09:07<01:42,  1.26s/it][1,0]<stderr>:#015 84%|████████▍ | 420/500 [09:08<01:43,  1.29s/it][1,0]<stderr>:#015 84%|████████▍ | 421/500 [09:09<01:41,  1.28s/it][1,0]<stderr>:#015 84%|████████▍ | 422/500 [09:10<01:39,  1.28s/it][1,0]<stderr>:#015 85%|████████▍ | 423/500 [09:12<01:38,  1.28s/it][1,0]<stderr>:#015 85%|████████▍ | 424/500 [09:13<01:39,  1.31s/it][1,0]<stderr>:#015 85%|████████▌ | 425/500 [09:14<01:37,  1.30s/it][1,0]<stderr>:#015 85%|████████▌ | 426/500 [09:16<01:35,  1.29s/it][1,0]<stderr>:#015 85%|████████▌ | 427/500 [09:17<01:34,  1.29s/it][1,0]<stderr>:#015 86%|████████▌ | 428/500 [09:18<01:33,  1.29s/it][1,0]<stderr>:#015 86%|████████▌ | 429/500 [09:20<01:32,  1.30s/it][1,0]<stderr>:#015 86%|████████▌ | 430/500 [09:21<01:30,  1.29s/it][1,0]<stderr>:#015 86%|████████▌ | 431/500 [09:22<01:29,  1.29s/it][1,0]<stderr>:#015 86%|████████▋ | 432/500 [09:23<01:27,  1.29s/it][1,0]<stderr>:#015 87%|████████▋ | 433/500 [09:25<01:27,  1.30s/it][1,0]<stderr>:#015 87%|████████▋ | 434/500 [09:26<01:28,  1.33s/it][1,0]<stderr>:#015 87%|████████▋ | 435/500 [09:27<01:25,  1.32s/it][1,0]<stderr>:#015 87%|████████▋ | 436/500 [09:29<01:23,  1.30s/it][1,0]<stderr>:#015 87%|████████▋ | 437/500 [09:30<01:22,  1.30s/it][1,0]<stderr>:#015 88%|████████▊ | 438/500 [09:31<01:20,  1.30s/it][1,0]<stderr>:#015 88%|████████▊ | 439/500 [09:33<01:19,  1.31s/it][1,0]<stderr>:#015 88%|████████▊ | 440/500 [09:34<01:17,  1.29s/it][1,0]<stderr>:#015 88%|████████▊ | 441/500 [09:35<01:16,  1.30s/it][1,0]<stderr>:#015 88%|████████▊ | 442/500 [09:37<01:17,  1.33s/it][1,0]<stderr>:#015 89%|████████▊ | 443/500 [09:38<01:15,  1.33s/it][1,0]<stderr>:#015 89%|████████▉ | 444/500 [09:39<01:14,  1.33s/it][1,0]<stderr>:#015 89%|████████▉ | 445/500 [09:41<01:12,  1.32s/it][1,0]<stderr>:#015 89%|██████�\u001b[0m\n",
      "\u001b[34m�█▉ | 446/500 [09:42<01:10,  1.31s/it][1,0]<stderr>:#015 89%|████████▉ | 447/500 [09:43<01:10,  1.34s/it][1,0]<stderr>:#015 90%|████████▉ | 448/500 [09:45<01:09,  1.33s/it][1,0]<stderr>:#015 90%|████████▉ | 449/500 [09:46<01:07,  1.32s/it][1,0]<stderr>:#015 90%|█████████ | 450/500 [09:47<01:05,  1.31s/it][1,0]<stderr>:#015 90%|█████████ | 451/500 [09:48<01:03,  1.30s/it][1,0]<stderr>:#015 90%|█████████ | 452/500 [09:50<01:03,  1.32s/it][1,0]<stderr>:#015 91%|█████████ | 453/500 [09:51<01:02,  1.32s/it][1,0]<stderr>:#015 91%|█████████ | 454/500 [09:52<01:00,  1.31s/it][1,0]<stderr>:#015 91%|█████████ | 455/500 [09:54<00:58,  1.29s/it][1,0]<stderr>:#015 91%|█████████ | 456/500 [09:55<00:56,  1.29s/it][1,0]<stderr>:#015 91%|█████████▏| 457/500 [09:56<00:55,  1.30s/it][1,0]<stderr>:#015 92%|█████████▏| 458/500 [09:58<00:54,  1.30s/it][1,0]<stderr>:#015 92%|█████████▏| 459/500 [09:59<00:52,  1.29s/it][1,0]<stderr>:#015 92%|█████████▏| 460/500 [10:00<00:51,  1.29s/it][1,0]<stderr>:#015 92%|█████████▏| 461/500 [10:01<00:50,  1.29s/it][1,0]<stderr>:#015 92%|█████████▏| 462/500 [10:03<00:49,  1.29s/it][1,0]<stderr>:#015 93%|█████████▎| 463/500 [10:04<00:47,  1.29s/it][1,0]<stderr>:#015 93%|█████████▎| 464/500 [10:05<00:46,  1.30s/it][1,0]<stderr>:#015 93%|█████████▎| 465/500 [10:07<00:45,  1.29s/it][1,0]<stderr>:#015 93%|█████████▎| 466/500 [10:08<00:43,  1.29s/it][1,0]<stderr>:#015 93%|█████████▎| 467/500 [10:09<00:42,  1.29s/it][1,0]<stderr>:#015 94%|█████████▎| 468/500 [10:10<00:41,  1.29s/it][1,0]<stderr>:#015 94%|█████████▍| 469/500 [10:12<00:40,  1.30s/it][1,0]<stderr>:#015 94%|█████████▍| 470/500 [10:13<00:39,  1.31s/it][1,0]<stderr>:#015 94%|█████████▍| 471/500 [10:14<00:37,  1.30s/it][1,0]<stderr>:#015 94%|█████████▍| 472/500 [10:16<00:36,  1.30s/it][1,0]<stderr>:#015 95%|█████████▍| 473/500 [10:17<00:34,  1.28s/it][1,0]<stderr>:#015 95%|█████████▍| 474/500 [10:18<00:33,  1.29s/it][1,0]<stderr>:#015 95%|█████████▌| 475/500 [10:19<00:32,  1.29s/it][1,0]<stderr>:#015 95%|█████████▌| 476/500 [10:21<00:30,  1.29s/it][1,0]<stderr>:#015 95%|█████████▌| 477/500 [10:22<00:29,  1.29s/it][1,0]<stderr>:#015 96%|█████████▌| 478/500 [10:23<00:28,  1.30s/it][1,0]<stderr>:#015 96%|█████████▌| 479/500 [10:25<00:27,  1.30s/it][1,0]<stderr>:#015 96%|█████████▌| 480/500 [10:26<00:25,  1.30s/it][1,0]<stderr>:#015 96%|█████████▌| 481/500 [10:27<00:24,  1.30s/it][1,0]<stderr>:#015 96%|█████████▋| 482/500 [10:29<00:23,  1.30s/it][1,0]<stderr>:#015 97%|█████████▋| 483/500 [10:30<00:22,  1.30s/it][1,0]<stderr>:#015 97%|█████████▋| 484/500 [10:31<00:21,  1.32s/it][1,0]<stderr>:#015 97%|█████████▋| 485/500 [10:33<00:19,  1.33s/it][1,0]<stderr>:#015 97%|█████████▋| 486/500 [10:34<00:18,  1.33s/it][1,0]<stderr>:#015 97%|█████████▋| 487/500 [10:35<00:17,  1.32s/it][1,0]<stderr>:#015 98%|█████████▊| 488/500 [10:37<00:15,  1.32s/it][1,0]<stderr>:#015 98%|█████████▊| 489/500 [10:38<00:14,  1.31s/it][1,0]<stderr>:#015 98%|█████████▊| 490/500 [10:39<00:13,  1.31s/it][1,0]<stderr>:#015 98%|█████████▊| 491/500 [10:40<00:11,  1.31s/it][1,0]<stderr>:#015 98%|█████████▊| 492/500 [10:42<00:10,  1.32s/it][1,0]<stderr>:#015 99%|█████████▊| 493/500 [10:43<00:09,  1.31s/it][1,0]<stderr>:#015 99%|█████████▉| 494/500 [10:44<00:07,  1.30s/it][1,0]<stderr>:#015 99%|█████████▉| 495/500 [10:46<00:06,  1.29s/it][1,0]<stderr>:#015 99%|█████████▉| 496/500 [10:47<00:05,  1.28s/it][1,0]<stderr>:#015 99%|█████████▉| 497/500 [10:48<00:03,  1.29s/it][1,0]<stderr>:#015100%|█████████▉| 498/500 [10:49<00:02,  1.28s/it][1,0]<stderr>:#015100%|█████████▉| 499/500 [10:51<00:01,  1.27s/it][1,0]<stderr>:#015100%|██████████| 500/500 [10:52<00:00,  1.29s/it][1,0]<stderr>:#015                                                 #015[1,0]<stderr>:#015100%|██████████| 500/500 [10:52<00:00,  1.29s/it][1,1]<stderr>:[INFO|trainer.py:1352] 2022-05-02 13:58:17,227 >> \u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:Training completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:1885] 2022-05-02 13:58:17,235 >> Saving model checkpoint to /opt/ml/model/checkpoint-500\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|configuration_utils.py:351] 2022-05-02 13:58:17,236 >> Configuration saved in /opt/ml/model/checkpoint-500/config.json\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[INFO|trainer.py:1352] 2022-05-02 13:58:17,942 >> \u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:Training completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[INFO|trainer.py:1352] 2022-05-02 13:58:18,196 >> \u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:Training completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:[INFO|trainer.py:1352] 2022-05-02 13:58:18,212 >> \u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:Training completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|modeling_utils.py:889] 2022-05-02 13:58:19,199 >> Model weights saved in /opt/ml/model/checkpoint-500/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|tokenization_utils_base.py:1924] 2022-05-02 13:58:19,200 >> tokenizer config file saved in /opt/ml/model/checkpoint-500/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|tokenization_utils_base.py:1930] 2022-05-02 13:58:19,200 >> Special tokens file saved in /opt/ml/model/checkpoint-500/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:__main__:*** Evaluate ***\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[INFO|trainer.py:516] 2022-05-02 13:58:19,702 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[INFO|trainer.py:2115] 2022-05-02 13:58:19,707 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[INFO|trainer.py:2117] 2022-05-02 13:58:19,707 >>   Num examples = 9815\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[INFO|trainer.py:2120] 2022-05-02 13:58:19,707 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:__main__:*** Evaluate ***\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[INFO|trainer.py:516] 2022-05-02 13:58:20,126 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: premise, hypothesis, idx.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[INFO|trainer.py:2115] 2022-05-02 13:58:20,131 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[INFO|trainer.py:2117] 2022-05-02 13:58:20,131 >>   Num examples = 9815\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[INFO|trainer.py:2120] 2022-05-02 13:58:20,131 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:INFO:__main__:*** Evaluate ***\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:INFO:__main__:*** Evaluate ***\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[INFO|trainer.py:516] 2022-05-02 13:58:20,515 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: premise, idx, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[INFO|trainer.py:2115] 2022-05-02 13:58:20,521 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[INFO|trainer.py:2117] 2022-05-02 13:58:20,521 >>   Num examples = 9815\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[INFO|trainer.py:2120] 2022-05-02 13:58:20,521 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:[INFO|trainer.py:516] 2022-05-02 13:58:20,524 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:[INFO|trainer.py:2115] 2022-05-02 13:58:20,529 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:[INFO|trainer.py:2117] 2022-05-02 13:58:20,529 >>   Num examples = 9815\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:[INFO|trainer.py:2120] 2022-05-02 13:58:20,529 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:[INFO|trainer.py:1352] 2022-05-02 13:58:23,576 >> \u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:Training completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|trainer.py:1352] 2022-05-02 13:58:23,597 >> \u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:Training completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[INFO|trainer.py:1352] 2022-05-02 13:58:23,616 >> \u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:Training completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:1352] 2022-05-02 13:58:27,547 >> \u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:Training completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015                                                 [1,0]<stderr>:#015[1,0]<stderr>:#015100%|██████████| 500/500 [11:04<00:00,  1.29s/it][1,0]<stderr>:#015100%|██████████| 500/500 [11:04<00:00,  1.33s/it]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:1885] 2022-05-02 13:58:28,778 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|configuration_utils.py:351] 2022-05-02 13:58:28,781 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:__main__:*** Evaluate ***\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:INFO:__main__:*** Evaluate ***\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:INFO:__main__:*** Evaluate ***\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[INFO|trainer.py:516] 2022-05-02 13:58:29,544 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[INFO|trainer.py:2115] 2022-05-02 13:58:29,548 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[INFO|trainer.py:2117] 2022-05-02 13:58:29,548 >>   Num examples = 9815\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[INFO|trainer.py:2120] 2022-05-02 13:58:29,548 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|trainer.py:516] 2022-05-02 13:58:29,564 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: hypothesis, premise, idx.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|trainer.py:2115] 2022-05-02 13:58:29,569 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|trainer.py:2117] 2022-05-02 13:58:29,569 >>   Num examples = 9815\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|trainer.py:2120] 2022-05-02 13:58:29,569 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:[INFO|trainer.py:516] 2022-05-02 13:58:29,579 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: premise, idx, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:[INFO|trainer.py:2115] 2022-05-02 13:58:29,584 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:[INFO|trainer.py:2117] 2022-05-02 13:58:29,584 >>   Num examples = 9815\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:[INFO|trainer.py:2120] 2022-05-02 13:58:29,584 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|modeling_utils.py:889] 2022-05-02 13:58:30,675 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|tokenization_utils_base.py:1924] 2022-05-02 13:58:30,675 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|tokenization_utils_base.py:1930] 2022-05-02 13:58:30,675 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:907] 2022-05-02 13:58:30,805 >> ***** train metrics *****\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2022-05-02 13:58:30,805 >>   epoch                      =       0.04\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2022-05-02 13:58:30,805 >>   init_mem_cpu_alloc_delta   =       -5MB\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2022-05-02 13:58:30,805 >>   init_mem_cpu_peaked_delta  =        4MB\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2022-05-02 13:58:30,805 >>   init_mem_gpu_alloc_delta   =        0MB\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2022-05-02 13:58:30,805 >>   init_mem_gpu_peaked_delta  =        0MB\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2022-05-02 13:58:30,805 >>   train_mem_cpu_alloc_delta  =     1993MB\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2022-05-02 13:58:30,805 >>   train_mem_cpu_peaked_delta =     1863MB\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2022-05-02 13:58:30,805 >>   train_mem_gpu_alloc_delta  =     2213MB\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2022-05-02 13:58:30,805 >>   train_mem_gpu_peaked_delta =     1541MB\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2022-05-02 13:58:30,806 >>   train_runtime              = 0:11:04.40\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2022-05-02 13:58:30,806 >>   train_samples              =     392702\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2022-05-02 13:58:30,806 >>   train_samples_per_second   =      0.753\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:__main__:*** Evaluate ***\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:516] 2022-05-02 13:58:30,907 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: hypothesis, idx, premise.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:2115] 2022-05-02 13:58:30,911 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:2117] 2022-05-02 13:58:30,911 >>   Num examples = 9815\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:2120] 2022-05-02 13:58:30,911 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  0%|          | 0/307 [00:00<?, ?it/s][1,0]<stderr>:#015  1%|          | 2/307 [00:00<01:41,  2.99it/s][1,0]<stderr>:#015  1%|          | 3/307 [00:01<02:12,  2.29it/s][1,0]<stderr>:#015  1%|▏         | 4/307 [00:02<02:33,  1.97it/s][1,0]<stderr>:#015  2%|▏         | 5/307 [00:02<02:44,  1.84it/s][1,0]<stderr>:#015  2%|▏         | 6/307 [00:03<02:50,  1.77it/s][1,0]<stderr>:#015  2%|▏         | 7/307 [00:03<02:57,  1.69it/s][1,0]<stderr>:#015  3%|▎         | 8/307 [00:04<03:00,  1.66it/s][1,0]<stderr>:#015  3%|▎         | 9/307 [00:05<03:01,  1.64it/s][1,0]<stderr>:#015  3%|▎         | 10/307 [00:05<03:03,  1.62it/s][1,0]<stderr>:#015  4%|▎         | 11/307 [00:06<03:03,  1.62it/s][1,0]<stderr>:#015  4%|▍         | 12/307 [00:07<03:03,  1.61it/s][1,0]<stderr>:#015  4%|▍         | 13/307 [00:07<03:02,  1.61it/s][1,0]<stderr>:#015  5%|▍         | 14/307 [00:08<03:06,  1.57it/s][1,0]<stderr>:#015  5%|▍         | 15/307 [00:09<03:09,  1.54it/s][1,0]<stderr>:#015  5%|▌         | 16/307 [00:09<03:08,  1.55it/s][1,0]<stderr>:#015  6%|▌         | 17/307 [00:10<03:08,  1.54it/s][1,0]<stderr>:#015  6%|▌         | 18/307 [00:10<03:08,  1.53it/s][1,0]<stderr>:#015  6%|▌         | 19/307 [00:11<03:06,  1.54it/s][1,0]<stderr>:#015  7%|▋         | 20/307 [00:12<03:08,  1.52it/s][1,0]<stderr>:#015  7%|▋         | 21/307 [00:12<03:07,  1.53it/s][1,0]<stderr>:#015  7%|▋         | 22/307 [00:13<03:05,  1.54it/s][1,0]<stderr>:#015  7%|▋         | 23/307 [00:14<03:07,  1.51it/s][1,0]<stderr>:#015  8%|▊         | 24/307 [00:14<03:07,  1.51it/s][1,0]<stderr>:#015  8%|▊         | 25/307 [00:15<03:04,  1.53it/s][1,0]<stderr>:#015  8%|▊         | 26/307 [00:16<03:04,  1.53it/s][1,0]<stderr>:#015  9%|▉         | 27/307 [00:16<03:04,  1.51it/s][1,0]<stderr>:#015  9%|▉         | 28/307 [00:17<03:01,  1.54it/s][1,0]<stderr>:#015  9%|▉         | 29/307 [00:18<02:59,  1.55it/s][1,0]<stderr>:#015 10%|▉         | 30/307 [00:18<02:56,  1.57it/s][1,0]<stderr>:#015 10%|█         | 31/307 [00:19<02:54,  1.58it/s][1,0]<stderr>:#015 10%|█         | 32/307 [00:20<02:56,  1.56it/s][1,0]<stderr>:#015 11%|█         | 33/307 [00:20<02:54,  1.57it/s][1,0]<stderr>:#015 11%|█         | 34/307 [00:21<02:55,  1.56it/s][1,0]<stderr>:#015 11%|█▏        | 35/307 [00:22<02:55,  1.55it/s][1,0]<stderr>:#015 12%|█▏        | 36/307 [00:22<02:54,  1.55it/s][1,0]<stderr>:#015 12%|█▏        | 37/307 [00:23<02:52,  1.56it/s][1,0]<stderr>:#015 12%|█▏        | 38/307 [00:23<02:53,  1.55it/s][1,0]<stderr>:#015 13%|█▎        | 39/307 [00:24<02:53,  1.54it/s][1,0]<stderr>:#015 13%|█▎        | 40/307 [00:25<02:55,  1.53it/s][1,0]<stderr>:#015 13%|█▎        | 41/307 [00:25<02:55,  1.52it/s][1,0]<stderr>:#015 14%|█▎        | 42/307 [00:26<02:52,  1.53it/s][1,0]<stderr>:#015 14%|█▍        | 43/307 [00:27<02:50,  1.55it/s][1,0]<stderr>:#015 14%|█▍        | 44/307 [00:27<02:48,  1.56it/s][1,0]<stderr>:#015 15%|█▍        | 45/307 [00:28<02:53,  1.51it/s][1,0]<stderr>:#015 15%|█▍        | 46/307 [00:29<02:51,  1.52it/s][1,0]<stderr>:#015 15%|█▌        | 47/307 [00:29<02:50,  1.52it/s][1,0]<stderr>:#015 16%|█▌        | 48/307 [00:30<02:51,  1.51it/s][1,0]<stderr>:#015 16%|█▌        | 49/307 [00:31<02:51,  1.51it/s][1,0]<stderr>:#015 16%|█▋        | 50/307 [00:31<02:50,  1.51it/s][1,0]<stderr>:#015 17%|█▋        | 51/307 [00:32<02:49,  1.51it/s][1,0]<stderr>:#015 17%|█▋        | 52/307 [00:33<02:47,  1.52it/s][1,0]<stderr>:#015 17%|█▋        | 53/307 [00:33<02:49,  1.50it/s][1,0]<stderr>:#015 18%|█▊        | 54/307 [00:34<02:47,  1.51it/s][1,0]<stderr>:#015 18%|█▊        | 55/307 [00:35<02:46,  1.51it/s][1,0]<stderr>:#015 18%|█▊        | 56/307 [00:35<02:44,  1.53it/s][1,0]<stderr>:#015 19%|█▊        | 57/307 [00:36<02:44,  1.52it/s][1,0]<stderr>:#015 19%|█▉        | 58/307 [00:37<02:42,  1.53it/s][1,0]<stderr>:#015 19%|█▉        | 59/307 [00:37<02:43,  1.51it/s][1,0]<stderr>:#015 20%|█▉        | 60/307 [00:38<02:44,  1.51it/s][1,0]<stderr>:#015 20%|█▉        | 61/307 [00:39<02:41,  1.52it/s][1,0]<stderr>:#015 20%|██        | 62/307 [00:39<02:41,  1.52it/s][1,0]<stderr>:#015 21%|██        | 63/307 [00:40<02:38,  1.54it/s][1,0]<stderr>:#015 21%|██        | 64/307 [00:41<02:38,  1.53it/s][1,0]<stderr>:#015 21%|██        | 65/307 [00:41<02:36,  1.55it/s][1,0]<stderr>:#015 21%|██▏       | 66/307 [00:42<02:35,  1.55it/s][1,0]<stderr>:#015 22%|██▏       | 67/307 [00:42<02:34,  1.55it/s][1,0]<stderr>:#015 22%|██▏       | 68/307 [00:43<02:34,  1.55it/s][1,0]<stderr>:#015 22%|██▏       | 69/307 [00:44<02:35,  1.53it/s][1,0]<stderr>:#015 23%|██▎       | 70/307 [00:44<02:33,  1.54it/s][1,0]<stderr>:#015 23%|██▎       | 71/307 [00:45<02:31,  1.55it/s][1,0]<stderr>:#015 23%|██▎       | 72/307 [00:46<02:33,  1.54it/s][1,0]<stderr>:#015 24%|██▍       | 73/307 [00:46<02:31,  1.54it/s][1,0]<stderr>:#015 24%|██▍       | 74/307 [00:47<02:32,  1.53it/s][1,0]<stderr>:#015 24%|██▍       | 75/307 [00:48<02:32,  1.52it/s][1,0]<stderr>:#015 25%|██▍       | 76/307 [00:48<02:31,  1.53it/s][1,0]<stderr>:#015 25%|██▌       | 77/307 [00:49<02:29,  1.54it/s][1,0]<stderr>:#015 25%|██▌       | 78/307 [00:50<02:30,  1.53it/s][1,0]<stderr>:#015 26%|██▌       | 79/307 [00:50<02:28,  1.54it/s][1,0]<stderr>:#015 26%|██▌       | 80/307 [00:51<02:27,  1.54it/s][1,0]<stderr>:#015 26%|██▋       | 81/307 [00:52<02:26,  1.54it/s][1,0]<stderr>:#015 27%|██▋       | 82/307 [00:52<02:25,  1.55it/s][1,0]<stderr>:#015 27%|██▋       | 83/307 [00:53<02:24,  1.55it/s][1,0]<stderr>:#015 27%|██▋       | 84/307 [00:53<02:24,  1.55it/s][1,0]<stderr>:#015 28%|██▊       | 85/307 [00:54<02:22,  1.55it/s][1,0]<stderr>:#015 28%|██▊       | 86/307 [00:55<02:21,  1.56it/s][1,0]<stderr>:#015 28%|██▊       | 87/307 [00:55<02:20,  1.57it/s][1,0]<stderr>:#015 29%|██▊       | 88/307 [00:56<02:20,  1.56it/s][1,0]<stderr>:#015 29%|██▉       | 89/307 [00:57<02:19,  1.56it/s][1,0]<stderr>:#015 29%|██▉       | 90/307 [00:57<02:17,  1.58it/s][1,0]<stderr>:#015 30%|██▉       | 91/307 [00:58<02:16,  1.58it/s][1,0]<stderr>:#015 30%|██▉       | 92/307 [00:59<02:16,  1.57it/s][1,0]<stderr>:#015 30%|███       | 93/307 [00:59<02:14,  1.59it/s][1,0]<stderr>:#015 31%|███       | 94/307 [01:00<02:14,  1.59it/s][1,0]<stderr>:#015 31%|███       | 95/307 [01:00<02:14,  1.58it/s][1,0]<stderr>:#015 31%|███▏      | 96/307 [01:01<02:16,  1.54it/s][1,0]<stderr>:#015 32%|███▏      | 97/307 [01:02<02:15,  1.55it/s][1,0]<stderr>:#015 32%|███▏      | 98/307 [01:02<02:13,  1.56it/s][1,0]<stderr>:#015 32%|███▏      | 99/307 [01:03<02:13,  1.56it/s][1,0]<stderr>:#015 33%|███▎      | 100/307 [01:04<02:12,  1.56it/s][1,0]<stderr>:#015 33%|███▎      | 101/307 [01:04<02:10,  1.57it/s][1,0]<stderr>:#015 33%|███▎      | 102/307 [01:05<02:13,  1.54it/s][1,0]<stderr>:#015 34%|███▎      | 103/307 [01:06<02:11,  1.55it/s][1,0]<stderr>:#015 34%|███▍      | 104/307 [01:06<02:10,  1.55it/s][1,0]<stderr>:#015 34%|███▍      | 105/307 [01:07<02:09,  1.56it/s][1,0]<stderr>:#015 35%|███▍      | 106/307 [01:08<02:08,  1.57it/s][1,0]<stderr>:#015 35%|███▍      | 107/307 [01:08<02:07,  1.57it/s][1,0]<stderr>:#015 35%|███▌      | 108/307 [01:09<02:07,  1.56it/s][1,0]<stderr>:#015 36%|███▌      | 109/307 [01:09<02:05,  1.58it/s][1,0]<stderr>:#015 36%|███▌      | 110/307 [01:10<02:08,  1.54it/s][1,0]<stderr>:#015 36%|███▌      | 111/307 [01:11<02:05,  1.56it/s][1,0]<stderr>:#015 36%|███▋      | 112/307 [01:11<02:06,  1.55it/s][1,0]<stderr>:#015 37%|███▋      | 113/307 [01:12<02:09,  1.50it/s][1,0]<stderr>:#015 37%|███▋      | 114/307 [01:13<02:07,  1.52it/s][1,0]<stderr>:#015 37%|███▋      | 115/307 [01:13<02:06,  1.51it/s][1,0]<stderr>:#015 38%|███▊      | 116/307 [01:14<02:05,  1.52it/s][1,0]<stderr>:#015 38%|███▊      | 117/307 [01:15<02:03,  1.54it/s][1,0]<stderr>:#015 38%|███▊      | 118/307 [01:15<02:00,  1.57it/s][1,0]<stderr>:#015 39%|███▉      | 119/307 [01:16<02:00,  1.57it/s][1,0]<stderr>:#015 39%|███▉      | 120/307 [01:17<01:59,  1.56it/s][1,0]<stderr>:#015 39%|███▉      | 121/307 [01:17<01:59,  1.56it/s][1,0]<stderr>:#015 40%|███▉      | 122/307 [01:18<01:59,  1.54it/s][1,0]<stderr>:#015 40%|████      | 123/307 [01:19<01:59,  1.54it/s][1,0]<stderr>:#015 40%|████      | 124/307 [01:19<01:57,  1.56it/s][1,0]<stderr>:#015 41%|████      | 125/307 [01:20<01:57,  1.54it/s][1,0]<stderr>:#015 41%|████      | 126/307 [01:20<01:55,  1.57it/s][1,0]<stderr>:#015 41%|████▏     | 127/307 [01:21<01:54,  1.57it/s][1,0]<stderr>:#015 42%|████▏     | 128/307 [01:22<01:53,  1.58it/s][1,0]<stderr>:#015 42%|████▏     | 129/307 [01:22<01:52,  1.58it/s][1,0]<stderr>:#015 42%|████▏     | 130/307 [01:23<01:52,  1.57it/s][1,0]<stderr>:#015 43%|████▎     | 131/307 [01:24<01:53,  1.55it/s][1,0]<stderr>:#015 43%|████▎     | 132/307 [01:24<01:52,  1.56it/s][1,0]<stderr>:#015 43%|████▎     | 133/307 [01:25<01:50,  1.57it/s][1,0]<stderr>:#015 44%|████▎     | 134/307 [01:26<01:51,  1.55it/s][1,0]<stderr>:#015 44%|████▍     | 135/307 [01:26<01:52,  1.53it/s][1,0]<stderr>:#015 44%|████▍     | 136/307 [01:27<01:50,  1.54it/s][1,0]<stderr>:#015 45%|████▍     | 137/307 [01:28<01:49,  1.56it/s][1,0]<stderr>:#015 45%|████▍     | 138/307 [01:28<01:50,  1.53it/s][1,0]<stderr>:#015 45%|████▌     | 139/307 [01:29<01:50,  1.53it/s][1,0]<stderr>:#015 46%|████▌     | 140/307 [01:30<01:48,  1.53it/s][1,0]<stderr>:#015 46%|████▌     | 141/307 [01:30<01:47,  1.54it/s][1,0]<stderr>:#015 46%|████▋     | 142/307 [01:31<01:47,  1.54it/s][1,0]<stderr>:#015 47%|████▋     | 143/307 [01:31<01:46,  1.54it/s][1,0]<stderr>:#015 47%|████▋     | 144/307 [01:32<01:47,  1.51it/s][1,0]<stderr>:#015 47%|████▋     | 145/307 [01:33<01:45,  1.53it/s][1,0]<stderr>:#015 48%|████▊     | 146/307 [01:33<01:44,  1.55it/s][1,0]<stderr>:#015 48%|████▊     | 147/307 [01:34<01:43,  1.54it/s][1,0]<stderr>:#015 48%|████▊     | 148/307 [01:35<01:42,  1.55it/s][1,0]<stderr>:#015 49%|████▊     | 149/307 [01:35<01:42,  1.55it/s][1,0]<stderr>:#015 49%|████▉     | 150/307 [01:36<01:40,  1.56it/s][1,0]<stderr>:#015 49%|████▉     | 151/307 [01:37<01:38,  1.58it/s][1,0]<stderr>:#015 50%|████▉     | 152/307 [01:37<01:38,  1.57it/s][1,0]<stderr>:#015 50%|████▉     | 153/307 [01:38<01:38,  1.56it/s][1,0]<stderr>:#015 50%|█████     | 154/307 [01:39<01:38,  1.56it/s][1,0]<stderr>:#015 50%|█████     | 155/307 [01:39<01:40,  1.51it/s][1,0]<stderr>:#015 51%|█████     | 156/307 [01:40<01:39,  1.52it/s][1,0]<stderr>:#015 51%|█████     | 157/307 [01:41<01:39,  1.51it/s][1,0]<stderr>:#015 51%|█████▏    | 158/307 [01:41<01:38,  1.52it/s][1,0]<stderr>:#015 52%|█████▏    | 159/307 [01:42<01:37,  1.52it/s][1,0]<stderr>:#015 52%|█████▏    | 160/307 [01:43<01:36,  1.53it/s][1,0]<stderr>:#015 52%|█████▏    | 161/307 [01:43<01:34,  1.54it/s][1,0]<stderr>:#015 53%|█████▎    | 162/307 [01:44<01:33,  1.54it/s][1,0]<stderr>:#015 53%|█████▎    | 163/307 [01:44<01:32,  1.56it/s][1,0]<stderr>:#015 53%|█████▎    | 164/307 [01:45<01:31,  1.57it/s][1,0]<stderr>:#015 54%|█████▎    | 165/307 [01:46<01:30,  1.57it/s][1,0]<stderr>:#015 54%|█████▍    | 166/307 [01:46<01:29,  1.58it/s][1,0]<stderr>:#015 54%|█████▍    | 167/307 [01:47<01:29,  1.56it/s][1,0]<stderr>:#015 55%|█████▍    | 168/307 [01:48<01:29,  1.55it/s][1,0]<stderr>:#015 55%|█████▌    | 169/307 [01:48<01:28,  1.56it/s][1,0]<stderr>:#015 55%|█████▌    | 170/307 [01:49<01:28,  1.54it/s][1,0]<stderr>:#015 56%|█████▌    | 171/307 [01:50<01:26,  1.57it/s][1,0]<stderr>:#015 56%|█████▌    | 172/307 [01:50<01:25,  1.58it/s][1,0]<stderr>:#015 56%|█████▋    | 173/307 [01:51<01:24,  1.59it/s][1,0]<stderr>:#015 57%|█████▋    | 174/307 [01:51<01:23,  1.59it/s][1,0]<stderr>:#015 57%|█████▋    | 175/307 [01:52<01:23,  1.58it/s][1,0]<stderr>:#015 57%|█████▋    | 176/307 [01:53<01:22,  1.58it/s][1,0]<stderr>:#015 58%|█████▊    | 177/307 [01:53<01:23,  1.56it/s][1,0]<stderr>:#015 58%|█████▊    | 178/307 [01:54<01:22,  1.57it/s][1,0]<stderr>:#015 58%|█████▊    | 179/307 [01:55<01:21,  1.57it/s][1,0]<stderr>:#015 59%|█████▊    | 180/307 [01:55<01:22,  1.54it/s][1,0]<stderr>:#015 59%|█████▉    | 181/307 [01:56<01:21,  1.54it/s][1,0]<stderr>:#015 59%|█████▉    | 182/307 [01:57<01:19,  1.56it/s][1,0]<stderr>:#015 60%|█████▉    | 183/307 [01:57<01:19,  1.57it/s][1,0]<stderr>:#015 60%|█████▉    | 184/307 [01:58<01:17,  1.58it/s][1,0]<stderr>:#015 60%|██████    | 185/307 [01:58<01:18,  1.56it/s][1,0]<stderr>:#015 61%|██████    | 186/307 [01:59<01:18,  1.54it/s][1,0]<stderr>:#015 61%|██████    | 187/307 [02:00<01:17,  1.54it/s][1,0]<stderr>:#015 61%|██████    | 188/307 [02:00<01:18,  1.52it/s][1,0]<stderr>:#015 62%|██████▏   | 189/307 [02:01<01:17,  1.52it/s][1,0]<stderr>:#015 62%|██████▏   | 190/307 [02:02<01:16,  1.53it/s][1,0]<stderr>:#015 62%|██████▏   | 191/307 [02:02<01:15,  1.54it/s][1,0]<stderr>:#015 63%|██████▎   | 192/307 [02:03<01:14,  1.54it/s][1,0]<stderr>:#015 63%|██████▎   | 193/307 [02:04<01:14,  1.52it/s][1,0]<stderr>:#015 63%|██████▎   | 194/307 [02:04<01:13,  1.53it/s][1,0]<stderr>:#015 64%|██████▎   | 195/307 [02:05<01:13,  1.53it/s][1,0]<stderr>:#015 64%|██████▍   | 196/307 [02:06<01:10,  1.56it/s][1,0]<stderr>:#015 64%|██████▍   | 197/307 [02:06<01:14,  1.48it/s][1,0]<stderr>:#015 64%|██████▍   | 198/307 [02:07<01:13,  1.48it/s][1,0]<stderr>:#015 65%|██████▍   | 199/307 [02:08<01:12,  1.49it/s][1,0]<stderr>:#015 65%|██████▌   | 200/307 [02:08<01:10,  1.51it/s][1,0]<stderr>:#015 65%|██████▌   | 201/307 [02:09<01:09,  1.53it/s][1,0]<stderr>:#015 66%|██████▌   | 202/307 [02:10<01:08,  1.53it/s][1,0]<stderr>:#015 66%|██████▌   | 203/307 [02:10<01:07,  1.54it/s][1,0]<stderr>:#015 66%|██████▋   | 204/307 [02:11<01:07,  1.52it/s][1,0]<stderr>:#015 67%|██████▋   | 205/307 [02:12<01:06,  1.54it/s][1,0]<stderr>:#015 67%|██████▋   | 206/307 [02:12<01:05,  1.55it/s][1,0]<stderr>:#015 67%|██████▋   | 207/307 [02:13<01:04,  1.56it/s][1,0]<stderr>:#015 68%|██████▊   | 208/307 [02:14<01:03,  1.56it/s][1,0]<stderr>:#015 68%|██████▊   | 209/307 [02:14<01:03,  1.55it/s][1,0]<stderr>:#015 68%|██████▊   | 210/307 [02:15<01:02,  1.56it/s][1,0]<stderr>:#015 69%|██████▊   | 211/307 [02:15<01:01,  1.56it/s][1,0]<stderr>:#015 69%|██████▉   | 212/307 [02:16<01:01,  1.55it/s][1,0]<stderr>:#015 69%|██████▉   | 213/307 [02:17<01:00,  1.54it/s][1,0]<stderr>:#015 70%|██████▉   | 214/307 [02:17<01:00,  1.54it/s][1,0]<stderr>:#015 70%|███████   | 215/307 [02:18<00:58,  1.57it/s][1,0]<stderr>:#015 70%|███████   | 216/307 [02:19<00:57,  1.57it/s][1,0]<stderr>:#015 71%|███████   | 217/307 [02:19<00:57,  1.57it/s][1,0]<stderr>:#015 71%|███████   | 218/307 [02:20<00:56,  1.57it/s][1,0]<stderr>:#015 71%|███████▏  | 219/307 [02:21<00:55,  1.58it/s][1,0]<stderr>:#015 72%|███████▏  | 220/307 [02:21<00:55,  1.57it/s][1,0]<stderr>:#015 72%|███████▏  | 221/307 [02:22<00:54,  1.57it/s][1,0]<stderr>:#015 72%|███████▏  | 222/307 [02:22<00:55,  1.54it/s][1,0]<stderr>:#015 73%|███████▎  | 223/307 [02:23<00:55,  1.52it/s][1,0]<stderr>:#015 73%|███████▎  | 224/307 [02:24<00:54,  1.54it/s][1,0]<stderr>:#015 73%|███████▎  | 225/307 [02:25<00:54,  1.50it/s][1,0]<stderr>:#015 74%|███████▎  | 226/307 [02:25<00:53,  1.53it/s][1,0]<stderr>:#015 74%|███████▍  | 227/307 [02:26<00:52,  1.51it/s][1,0]<stderr>:#015 74%|███████▍  | 228/307 [02:26<00:51,  1.54it/s][1,0]<stderr>:#015 75%|████\u001b[0m\n",
      "\u001b[34m███▍  | 229/307 [02:27<00:50,  1.53it/s][1,0]<stderr>:#015 75%|███████▍  | 230/307 [02:28<00:50,  1.53it/s][1,0]<stderr>:#015 75%|███████▌  | 231/307 [02:28<00:51,  1.49it/s][1,0]<stderr>:#015 76%|███████▌  | 232/307 [02:29<00:49,  1.51it/s][1,0]<stderr>:#015 76%|███████▌  | 233/307 [02:30<00:48,  1.52it/s][1,0]<stderr>:#015 76%|███████▌  | 234/307 [02:30<00:47,  1.53it/s][1,0]<stderr>:#015 77%|███████▋  | 235/307 [02:31<00:47,  1.53it/s][1,0]<stderr>:#015 77%|███████▋  | 236/307 [02:32<00:46,  1.53it/s][1,0]<stderr>:#015 77%|███████▋  | 237/307 [02:32<00:47,  1.49it/s][1,0]<stderr>:#015 78%|███████▊  | 238/307 [02:33<00:46,  1.48it/s][1,0]<stderr>:#015 78%|███████▊  | 239/307 [02:34<00:45,  1.48it/s][1,0]<stderr>:#015 78%|███████▊  | 240/307 [02:34<00:44,  1.50it/s][1,0]<stderr>:#015 79%|███████▊  | 241/307 [02:35<00:44,  1.49it/s][1,0]<stderr>:#015 79%|███████▉  | 242/307 [02:36<00:44,  1.48it/s][1,0]<stderr>:#015 79%|███████▉  | 243/307 [02:36<00:43,  1.48it/s][1,0]<stderr>:#015 79%|███████▉  | 244/307 [02:37<00:42,  1.49it/s][1,0]<stderr>:#015 80%|███████▉  | 245/307 [02:38<00:41,  1.51it/s][1,0]<stderr>:#015 80%|████████  | 246/307 [02:38<00:39,  1.54it/s][1,0]<stderr>:#015 80%|████████  | 247/307 [02:39<00:38,  1.55it/s][1,0]<stderr>:#015 81%|████████  | 248/307 [02:40<00:37,  1.56it/s][1,0]<stderr>:#015 81%|████████  | 249/307 [02:40<00:37,  1.55it/s][1,0]<stderr>:#015 81%|████████▏ | 250/307 [02:41<00:36,  1.55it/s][1,0]<stderr>:#015 82%|████████▏ | 251/307 [02:42<00:36,  1.54it/s][1,0]<stderr>:#015 82%|████████▏ | 252/307 [02:42<00:38,  1.43it/s][1,0]<stderr>:#015 82%|████████▏ | 253/307 [02:43<00:36,  1.47it/s][1,0]<stderr>:#015 83%|████████▎ | 254/307 [02:44<00:35,  1.49it/s][1,0]<stderr>:#015 83%|████████▎ | 255/307 [02:44<00:34,  1.53it/s][1,0]<stderr>:#015 83%|████████▎ | 256/307 [02:45<00:33,  1.51it/s][1,0]<stderr>:#015 84%|████████▎ | 257/307 [02:46<00:32,  1.53it/s][1,0]<stderr>:#015 84%|████████▍ | 258/307 [02:46<00:31,  1.54it/s][1,0]<stderr>:#015 84%|████████▍ | 259/307 [02:47<00:30,  1.55it/s][1,0]<stderr>:#015 85%|████████▍ | 260/307 [02:48<00:30,  1.56it/s][1,0]<stderr>:#015 85%|████████▌ | 261/307 [02:48<00:29,  1.56it/s][1,0]<stderr>:#015 85%|████████▌ | 262/307 [02:49<00:28,  1.56it/s][1,0]<stderr>:#015 86%|████████▌ | 263/307 [02:49<00:28,  1.55it/s][1,0]<stderr>:#015 86%|████████▌ | 264/307 [02:50<00:27,  1.54it/s][1,0]<stderr>:#015 86%|████████▋ | 265/307 [02:51<00:26,  1.56it/s][1,0]<stderr>:#015 87%|████████▋ | 266/307 [02:51<00:26,  1.57it/s][1,0]<stderr>:#015 87%|████████▋ | 267/307 [02:52<00:25,  1.57it/s][1,0]<stderr>:#015 87%|████████▋ | 268/307 [02:53<00:24,  1.57it/s][1,0]<stderr>:#015 88%|████████▊ | 269/307 [02:53<00:24,  1.52it/s][1,0]<stderr>:#015 88%|████████▊ | 270/307 [02:54<00:24,  1.53it/s][1,0]<stderr>:#015 88%|████████▊ | 271/307 [02:55<00:23,  1.53it/s][1,0]<stderr>:#015 89%|████████▊ | 272/307 [02:55<00:22,  1.55it/s][1,0]<stderr>:#015 89%|████████▉ | 273/307 [02:56<00:22,  1.54it/s][1,0]<stderr>:#015 89%|████████▉ | 274/307 [02:57<00:21,  1.56it/s][1,0]<stderr>:#015 90%|████████▉ | 275/307 [02:57<00:20,  1.57it/s][1,0]<stderr>:#015 90%|████████▉ | 276/307 [02:58<00:19,  1.58it/s][1,0]<stderr>:#015 90%|█████████ | 277/307 [02:58<00:19,  1.56it/s][1,0]<stderr>:#015 91%|█████████ | 278/307 [02:59<00:18,  1.58it/s][1,0]<stderr>:#015 91%|█████████ | 279/307 [03:00<00:17,  1.57it/s][1,0]<stderr>:#015 91%|█████████ | 280/307 [03:00<00:17,  1.55it/s][1,0]<stderr>:#015 92%|█████████▏| 281/307 [03:01<00:16,  1.57it/s][1,0]<stderr>:#015 92%|█████████▏| 282/307 [03:02<00:17,  1.43it/s][1,0]<stderr>:#015 92%|█████████▏| 283/307 [03:03<00:16,  1.46it/s][1,0]<stderr>:#015 93%|█████████▎| 284/307 [03:03<00:15,  1.49it/s][1,0]<stderr>:#015 93%|█████████▎| 285/307 [03:04<00:14,  1.51it/s][1,0]<stderr>:#015 93%|█████████▎| 286/307 [03:04<00:13,  1.54it/s][1,0]<stderr>:#015 93%|█████████▎| 287/307 [03:05<00:13,  1.52it/s][1,0]<stderr>:#015 94%|█████████▍| 288/307 [03:06<00:12,  1.51it/s][1,0]<stderr>:#015 94%|█████████▍| 289/307 [03:06<00:11,  1.53it/s][1,0]<stderr>:#015 94%|█████████▍| 290/307 [03:07<00:11,  1.54it/s][1,0]<stderr>:#015 95%|█████████▍| 291/307 [03:08<00:10,  1.55it/s][1,0]<stderr>:#015 95%|█████████▌| 292/307 [03:08<00:09,  1.54it/s][1,0]<stderr>:#015 95%|█████████▌| 293/307 [03:09<00:09,  1.55it/s][1,0]<stderr>:#015 96%|█████████▌| 294/307 [03:10<00:08,  1.55it/s][1,0]<stderr>:#015 96%|█████████▌| 295/307 [03:10<00:07,  1.55it/s][1,0]<stderr>:#015 96%|█████████▋| 296/307 [03:11<00:07,  1.55it/s][1,0]<stderr>:#015 97%|█████████▋| 297/307 [03:12<00:06,  1.53it/s][1,0]<stderr>:#015 97%|█████████▋| 298/307 [03:12<00:06,  1.50it/s][1,0]<stderr>:#015 97%|█████████▋| 299/307 [03:13<00:05,  1.47it/s][1,0]<stderr>:#015 98%|█████████▊| 300/307 [03:14<00:04,  1.49it/s][1,0]<stderr>:#015 98%|█████████▊| 301/307 [03:14<00:03,  1.52it/s][1,0]<stderr>:#015 98%|█████████▊| 302/307 [03:15<00:03,  1.51it/s][1,0]<stderr>:#015 99%|█████████▊| 303/307 [03:16<00:02,  1.51it/s][1,0]<stderr>:#015 99%|█████████▉| 304/307 [03:16<00:01,  1.52it/s][1,0]<stderr>:#015 99%|█████████▉| 305/307 [03:17<00:01,  1.53it/s][1,0]<stderr>:#015100%|█████████▉| 306/307 [03:18<00:00,  1.54it/s][1,0]<stderr>:#015100%|██████████| 307/307 [03:18<00:00,  1.56it/s][1,4]<stderr>:INFO:/opt/conda/lib/python3.6/site-packages/datasets/metric.py:Removing /root/.cache/huggingface/metrics/glue/mnli/default_experiment-1-0.arrow\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:INFO:/opt/conda/lib/python3.6/site-packages/datasets/metric.py:Removing /root/.cache/huggingface/metrics/glue/mnli/default_experiment-1-0.arrow\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:/opt/conda/lib/python3.6/site-packages/datasets/metric.py:Removing /root/.cache/huggingface/metrics/glue/mnli/default_experiment-1-0.arrow\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015100%|██████████| 307/307 [03:18<00:00,  1.54it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:/opt/conda/lib/python3.6/site-packages/datasets/metric.py:Removing /root/.cache/huggingface/metrics/glue/mnli/default_experiment-1-0.arrow\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:/opt/conda/lib/python3.6/site-packages/datasets/metric.py:Removing /root/.cache/huggingface/metrics/glue/mnli/default_experiment-1-0.arrow\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:INFO:/opt/conda/lib/python3.6/site-packages/datasets/metric.py:Removing /root/.cache/huggingface/metrics/glue/mnli/default_experiment-1-0.arrow\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:/opt/conda/lib/python3.6/site-packages/datasets/metric.py:Removing /root/.cache/huggingface/metrics/glue/mnli/default_experiment-1-0.arrow\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:907] 2022-05-02 14:01:50,579 >> ***** eval metrics *****\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2022-05-02 14:01:50,580 >>   epoch                     =       0.04\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2022-05-02 14:01:50,580 >>   eval_accuracy             =     0.3182\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2022-05-02 14:01:50,580 >>   eval_loss                 =     1.0992\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2022-05-02 14:01:50,580 >>   eval_mem_cpu_alloc_delta  =      103MB\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2022-05-02 14:01:50,580 >>   eval_mem_cpu_peaked_delta =        0MB\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2022-05-02 14:01:50,580 >>   eval_mem_gpu_alloc_delta  =        0MB\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2022-05-02 14:01:50,580 >>   eval_mem_gpu_peaked_delta =      146MB\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2022-05-02 14:01:50,580 >>   eval_runtime              = 0:03:19.47\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2022-05-02 14:01:50,580 >>   eval_samples              =       9815\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2022-05-02 14:01:50,580 >>   eval_samples_per_second   =     49.203\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:INFO:/opt/conda/lib/python3.6/site-packages/datasets/metric.py:Removing /root/.cache/huggingface/metrics/glue/mnli/default_experiment-1-0.arrow\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[INFO|trainer.py:516] 2022-05-02 14:01:50,652 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: premise, idx, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[INFO|trainer.py:2115] 2022-05-02 14:01:50,656 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[INFO|trainer.py:2117] 2022-05-02 14:01:50,656 >>   Num examples = 9832\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[INFO|trainer.py:2120] 2022-05-02 14:01:50,656 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:[INFO|trainer.py:516] 2022-05-02 14:01:50,671 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: premise, idx, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:[INFO|trainer.py:2115] 2022-05-02 14:01:50,675 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:[INFO|trainer.py:2117] 2022-05-02 14:01:50,676 >>   Num examples = 9832\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:[INFO|trainer.py:2120] 2022-05-02 14:01:50,676 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[INFO|trainer.py:516] 2022-05-02 14:01:50,708 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[INFO|trainer.py:2115] 2022-05-02 14:01:50,711 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[INFO|trainer.py:2117] 2022-05-02 14:01:50,712 >>   Num examples = 9832\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[INFO|trainer.py:2120] 2022-05-02 14:01:50,712 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[INFO|trainer.py:516] 2022-05-02 14:01:50,736 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: premise, hypothesis, idx.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[INFO|trainer.py:2115] 2022-05-02 14:01:50,741 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[INFO|trainer.py:2117] 2022-05-02 14:01:50,741 >>   Num examples = 9832\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[INFO|trainer.py:2120] 2022-05-02 14:01:50,742 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:516] 2022-05-02 14:01:50,750 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: hypothesis, idx, premise.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:2115] 2022-05-02 14:01:50,754 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:2117] 2022-05-02 14:01:50,754 >>   Num examples = 9832\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:2120] 2022-05-02 14:01:50,754 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|trainer.py:516] 2022-05-02 14:01:50,821 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: hypothesis, premise, idx.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|trainer.py:2115] 2022-05-02 14:01:50,824 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|trainer.py:2117] 2022-05-02 14:01:50,824 >>   Num examples = 9832\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|trainer.py:2120] 2022-05-02 14:01:50,825 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[INFO|trainer.py:516] 2022-05-02 14:01:50,842 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[INFO|trainer.py:2115] 2022-05-02 14:01:50,846 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[INFO|trainer.py:2117] 2022-05-02 14:01:50,847 >>   Num examples = 9832\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[INFO|trainer.py:2120] 2022-05-02 14:01:50,847 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:[INFO|trainer.py:516] 2022-05-02 14:01:50,872 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:[INFO|trainer.py:2115] 2022-05-02 14:01:50,875 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:[INFO|trainer.py:2117] 2022-05-02 14:01:50,876 >>   Num examples = 9832\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:[INFO|trainer.py:2120] 2022-05-02 14:01:50,876 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  0%|          | 0/308 [00:00<?, ?it/s][1,0]<stderr>:#015  1%|          | 2/308 [00:00<01:37,  3.15it/s][1,0]<stderr>:#015  1%|          | 3/308 [00:01<02:05,  2.42it/s][1,0]<stderr>:#015  1%|▏         | 4/308 [00:01<02:27,  2.05it/s][1,0]<stderr>:#015  2%|▏         | 5/308 [00:02<02:48,  1.80it/s][1,0]<stderr>:#015  2%|▏         | 6/308 [00:03<02:56,  1.71it/s][1,0]<stderr>:#015  2%|▏         | 7/308 [00:03<03:02,  1.65it/s][1,0]<stderr>:#015  3%|▎         | 8/308 [00:04<03:04,  1.62it/s][1,0]<stderr>:#015  3%|▎         | 9/308 [00:05<03:06,  1.61it/s][1,0]<stderr>:#015  3%|▎         | 10/308 [00:05<03:06,  1.60it/s][1,0]<stderr>:#015  4%|▎         | 11/308 [00:06<03:08,  1.58it/s][1,0]<stderr>:#015  4%|▍         | 12/308 [00:07<03:11,  1.55it/s][1,0]<stderr>:#015  4%|▍         | 13/308 [00:07<03:09,  1.56it/s][1,0]<stderr>:#015  5%|▍         | 14/308 [00:08<03:09,  1.55it/s][1,0]<stderr>:#015  5%|▍         | 15/308 [00:09<03:13,  1.52it/s][1,0]<stderr>:#015  5%|▌         | 16/308 [00:09<03:13,  1.51it/s][1,0]<stderr>:#015  6%|▌         | 17/308 [00:10<03:12,  1.51it/s][1,0]<stderr>:#015  6%|▌         | 18/308 [00:11<03:12,  1.51it/s][1,0]<stderr>:#015  6%|▌         | 19/308 [00:11<03:09,  1.53it/s][1,0]<stderr>:#015  6%|▋         | 20/308 [00:12<03:06,  1.54it/s][1,0]<stderr>:#015  7%|▋         | 21/308 [00:13<03:04,  1.56it/s][1,0]<stderr>:#015  7%|▋         | 22/308 [00:13<03:04,  1.55it/s][1,0]<stderr>:#015  7%|▋         | 23/308 [00:14<03:04,  1.54it/s][1,0]<stderr>:#015  8%|▊         | 24/308 [00:15<03:05,  1.53it/s][1,0]<stderr>:#015  8%|▊         | 25/308 [00:15<03:02,  1.55it/s][1,0]<stderr>:#015  8%|▊         | 26/308 [00:16<03:00,  1.56it/s][1,0]<stderr>:#015  9%|▉         | 27/308 [00:16<03:00,  1.55it/s][1,0]<stderr>:#015  9%|▉         | 28/308 [00:17<02:57,  1.57it/s][1,0]<stderr>:#015  9%|▉         | 29/308 [00:18<02:57,  1.57it/s][1,0]<stderr>:#015 10%|▉         | 30/308 [00:18<02:58,  1.56it/s][1,0]<stderr>:#015 10%|█         | 31/308 [00:19<02:58,  1.55it/s][1,0]<stderr>:#015 10%|█         | 32/308 [00:20<02:58,  1.54it/s][1,0]<stderr>:#015 11%|█         | 33/308 [00:20<02:56,  1.55it/s][1,0]<stderr>:#015 11%|█         | 34/308 [00:21<02:55,  1.56it/s][1,0]<stderr>:#015 11%|█▏        | 35/308 [00:22<02:58,  1.53it/s][1,0]<stderr>:#015 12%|█▏        | 36/308 [00:22<03:00,  1.50it/s][1,0]<stderr>:#015 12%|█▏        | 37/308 [00:23<03:01,  1.49it/s][1,0]<stderr>:#015 12%|█▏        | 38/308 [00:24<02:58,  1.51it/s][1,0]<stderr>:#015 13%|█▎        | 39/308 [00:24<03:00,  1.49it/s][1,0]<stderr>:#015 13%|█▎        | 40/308 [00:25<03:00,  1.49it/s][1,0]<stderr>:#015 13%|█▎        | 41/308 [00:26<02:55,  1.52it/s][1,0]<stderr>:#015 14%|█▎        | 42/308 [00:26<02:57,  1.50it/s][1,0]<stderr>:#015 14%|█▍        | 43/308 [00:27<02:53,  1.52it/s][1,0]<stderr>:#015 14%|█▍        | 44/308 [00:28<02:51,  1.54it/s][1,0]<stderr>:#015 15%|█▍        | 45/308 [00:28<02:50,  1.54it/s][1,0]<stderr>:#015 15%|█▍        | 46/308 [00:29<02:50,  1.54it/s][1,0]<stderr>:#015 15%|█▌        | 47/308 [00:30<02:50,  1.53it/s][1,0]<stderr>:#015 16%|█▌        | 48/308 [00:30<02:49,  1.53it/s][1,0]<stderr>:#015 16%|█▌        | 49/308 [00:31<02:47,  1.55it/s][1,0]<stderr>:#015 16%|█▌        | 50/308 [00:31<02:45,  1.56it/s][1,0]<stderr>:#015 17%|█▋        | 51/308 [00:32<02:44,  1.57it/s][1,0]<stderr>:#015 17%|█▋        | 52/308 [00:33<02:42,  1.58it/s][1,0]<stderr>:#015 17%|█▋        | 53/308 [00:33<02:41,  1.58it/s][1,0]<stderr>:#015 18%|█▊        | 54/308 [00:34<02:39,  1.59it/s][1,0]<stderr>:#015 18%|█▊        | 55/308 [00:35<02:43,  1.55it/s][1,0]<stderr>:#015 18%|█▊        | 56/308 [00:35<02:41,  1.56it/s][1,0]<stderr>:#015 19%|█▊        | 57/308 [00:36<02:41,  1.55it/s][1,0]<stderr>:#015 19%|█▉        | 58/308 [00:37<02:42,  1.54it/s][1,0]<stderr>:#015 19%|█▉        | 59/308 [00:37<02:45,  1.50it/s][1,0]<stderr>:#015 19%|█▉        | 60/308 [00:38<02:43,  1.52it/s][1,0]<stderr>:#015 20%|█▉        | 61/308 [00:39<02:44,  1.50it/s][1,0]<stderr>:#015 20%|██        | 62/308 [00:39<02:43,  1.51it/s][1,0]<stderr>:#015 20%|██        | 63/308 [00:40<02:38,  1.54it/s][1,0]<stderr>:#015 21%|██        | 64/308 [00:41<02:37,  1.55it/s][1,0]<stderr>:#015 21%|██        | 65/308 [00:41<02:36,  1.55it/s][1,0]<stderr>:#015 21%|██▏       | 66/308 [00:42<02:37,  1.54it/s][1,0]<stderr>:#015 22%|██▏       | 67/308 [00:42<02:38,  1.52it/s][1,0]<stderr>:#015 22%|██▏       | 68/308 [00:43<02:34,  1.55it/s][1,0]<stderr>:#015 22%|██▏       | 69/308 [00:44<02:33,  1.56it/s][1,0]<stderr>:#015 23%|██▎       | 70/308 [00:44<02:34,  1.54it/s][1,0]<stderr>:#015 23%|██▎       | 71/308 [00:45<02:32,  1.55it/s][1,0]<stderr>:#015 23%|██▎       | 72/308 [00:46<02:32,  1.55it/s][1,0]<stderr>:#015 24%|██▎       | 73/308 [00:46<02:30,  1.56it/s][1,0]<stderr>:#015 24%|██▍       | 74/308 [00:47<02:31,  1.54it/s][1,0]<stderr>:#015 24%|██▍       | 75/308 [00:48<02:34,  1.51it/s][1,0]<stderr>:#015 25%|██▍       | 76/308 [00:48<02:31,  1.53it/s][1,0]<stderr>:#015 25%|██▌       | 77/308 [00:49<02:29,  1.54it/s][1,0]<stderr>:#015 25%|██▌       | 78/308 [00:50<02:27,  1.56it/s][1,0]<stderr>:#015 26%|██▌       | 79/308 [00:50<02:26,  1.56it/s][1,0]<stderr>:#015 26%|██▌       | 80/308 [00:51<02:26,  1.56it/s][1,0]<stderr>:#015 26%|██▋       | 81/308 [00:52<02:27,  1.54it/s][1,0]<stderr>:#015 27%|██▋       | 82/308 [00:52<02:25,  1.55it/s][1,0]<stderr>:#015 27%|██▋       | 83/308 [00:53<02:25,  1.55it/s][1,0]<stderr>:#015 27%|██▋       | 84/308 [00:53<02:24,  1.55it/s][1,0]<stderr>:#015 28%|██▊       | 85/308 [00:54<02:24,  1.55it/s][1,0]<stderr>:#015 28%|██▊       | 86/308 [00:55<02:23,  1.55it/s][1,0]<stderr>:#015 28%|██▊       | 87/308 [00:55<02:21,  1.56it/s][1,0]<stderr>:#015 29%|██▊       | 88/308 [00:56<02:20,  1.57it/s][1,0]<stderr>:#015 29%|██▉       | 89/308 [00:57<02:21,  1.55it/s][1,0]<stderr>:#015 29%|██▉       | 90/308 [00:57<02:19,  1.57it/s][1,0]<stderr>:#015 30%|██▉       | 91/308 [00:58<02:17,  1.58it/s][1,0]<stderr>:#015 30%|██▉       | 92/308 [00:59<02:19,  1.55it/s][1,0]<stderr>:#015 30%|███       | 93/308 [00:59<02:17,  1.56it/s][1,0]<stderr>:#015 31%|███       | 94/308 [01:00<02:17,  1.56it/s][1,0]<stderr>:#015 31%|███       | 95/308 [01:00<02:16,  1.56it/s][1,0]<stderr>:#015 31%|███       | 96/308 [01:01<02:14,  1.57it/s][1,0]<stderr>:#015 31%|███▏      | 97/308 [01:02<02:13,  1.58it/s][1,0]<stderr>:#015 32%|███▏      | 98/308 [01:02<02:13,  1.57it/s][1,0]<stderr>:#015 32%|███▏      | 99/308 [01:03<02:13,  1.57it/s][1,0]<stderr>:#015 32%|███▏      | 100/308 [01:04<02:13,  1.56it/s][1,0]<stderr>:#015 33%|███▎      | 101/308 [01:04<02:12,  1.56it/s][1,0]<stderr>:#015 33%|███▎      | 102/308 [01:05<02:10,  1.58it/s][1,0]<stderr>:#015 33%|███▎      | 103/308 [01:06<02:10,  1.57it/s][1,0]<stderr>:#015 34%|███▍      | 104/308 [01:06<02:11,  1.56it/s][1,0]<stderr>:#015 34%|███▍      | 105/308 [01:07<02:10,  1.55it/s][1,0]<stderr>:#015 34%|███▍      | 106/308 [01:08<02:10,  1.55it/s][1,0]<stderr>:#015 35%|███▍      | 107/308 [01:08<02:10,  1.54it/s][1,0]<stderr>:#015 35%|███▌      | 108/308 [01:09<02:11,  1.53it/s][1,0]<stderr>:#015 35%|███▌      | 109/308 [01:09<02:08,  1.55it/s][1,0]<stderr>:#015 36%|███▌      | 110/308 [01:10<02:08,  1.54it/s][1,0]<stderr>:#015 36%|███▌      | 111/308 [01:11<02:09,  1.53it/s][1,0]<stderr>:#015 36%|███▋      | 112/308 [01:11<02:06,  1.55it/s][1,0]<stderr>:#015 37%|███▋      | 113/308 [01:12<02:05,  1.55it/s][1,0]<stderr>:#015 37%|███▋      | 114/308 [01:13<02:04,  1.56it/s][1,0]<stderr>:#015 37%|███▋      | 115/308 [01:13<02:05,  1.54it/s][1,0]<stderr>:#015 38%|███▊      | 116/308 [01:14<02:03,  1.55it/s][1,0]<stderr>:#015 38%|███▊      | 117/308 [01:15<02:03,  1.54it/s][1,0]<stderr>:#015 38%|███▊      | 118/308 [01:15<02:01,  1.56it/s][1,0]<stderr>:#015 39%|███▊      | 119/308 [01:16<01:59,  1.58it/s][1,0]<stderr>:#015 39%|███▉      | 120/308 [01:17<01:58,  1.59it/s][1,0]<stderr>:#015 39%|███▉      | 121/308 [01:17<01:58,  1.58it/s][1,0]<stderr>:#015 40%|███▉      | 122/308 [01:18<01:57,  1.59it/s][1,0]<stderr>:#015 40%|███▉      | 123/308 [01:18<01:57,  1.57it/s][1,0]<stderr>:#015 40%|████      | 124/308 [01:19<01:58,  1.55it/s][1,0]<stderr>:#015 41%|████      | 125/308 [01:20<01:57,  1.56it/s][1,0]<stderr>:#015 41%|████      | 126/308 [01:20<01:55,  1.57it/s][1,0]<stderr>:#015 41%|████      | 127/308 [01:21<01:54,  1.58it/s][1,0]<stderr>:#015 42%|████▏     | 128/308 [01:22<01:53,  1.59it/s][1,0]<stderr>:#015 42%|████▏     | 129/308 [01:22<01:52,  1.59it/s][1,0]<stderr>:#015 42%|████▏     | 130/308 [01:23<01:52,  1.58it/s][1,0]<stderr>:#015 43%|████▎     | 131/308 [01:24<01:53,  1.57it/s][1,0]<stderr>:#015 43%|████▎     | 132/308 [01:24<01:52,  1.56it/s][1,0]<stderr>:#015 43%|████▎     | 133/308 [01:25<01:53,  1.54it/s][1,0]<stderr>:#015 44%|████▎     | 134/308 [01:25<01:51,  1.56it/s][1,0]<stderr>:#015 44%|████▍     | 135/308 [01:26<01:49,  1.58it/s][1,0]<stderr>:#015 44%|████▍     | 136/308 [01:27<01:50,  1.56it/s][1,0]<stderr>:#015 44%|████▍     | 137/308 [01:27<01:48,  1.57it/s][1,0]<stderr>:#015 45%|████▍     | 138/308 [01:28<01:48,  1.57it/s][1,0]<stderr>:#015 45%|████▌     | 139/308 [01:29<01:46,  1.59it/s][1,0]<stderr>:#015 45%|████▌     | 140/308 [01:29<01:47,  1.57it/s][1,0]<stderr>:#015 46%|████▌     | 141/308 [01:30<01:46,  1.57it/s][1,0]<stderr>:#015 46%|████▌     | 142/308 [01:31<01:45,  1.58it/s][1,0]<stderr>:#015 46%|████▋     | 143/308 [01:31<01:44,  1.58it/s][1,0]<stderr>:#015 47%|████▋     | 144/308 [01:32<01:44,  1.57it/s][1,0]<stderr>:#015 47%|████▋     | 145/308 [01:32<01:44,  1.56it/s][1,0]<stderr>:#015 47%|████▋     | 146/308 [01:33<01:43,  1.57it/s][1,0]<stderr>:#015 48%|████▊     | 147/308 [01:34<01:41,  1.58it/s][1,0]<stderr>:#015 48%|████▊     | 148/308 [01:34<01:42,  1.56it/s][1,0]<stderr>:#015 48%|████▊     | 149/308 [01:35<01:41,  1.57it/s][1,0]<stderr>:#015 49%|████▊     | 150/308 [01:36<01:40,  1.57it/s][1,0]<stderr>:#015 49%|████▉     | 151/308 [01:36<01:38,  1.59it/s][1,0]<stderr>:#015 49%|████▉     | 152/308 [01:37<01:38,  1.59it/s][1,0]<stderr>:#015 50%|████▉     | 153/308 [01:38<01:40,  1.54it/s][1,0]<stderr>:#015 50%|█████     | 154/308 [01:38<01:38,  1.57it/s][1,0]<stderr>:#015 50%|█████     | 155/308 [01:39<01:38,  1.56it/s][1,0]<stderr>:#015 51%|█████     | 156/308 [01:39<01:37,  1.56it/s][1,0]<stderr>:#015 51%|█████     | 157/308 [01:40<01:35,  1.58it/s][1,0]<stderr>:#015 51%|█████▏    | 158/308 [01:41<01:36,  1.55it/s][1,0]<stderr>:#015 52%|█████▏    | 159/308 [01:41<01:37,  1.54it/s][1,0]<stderr>:#015 52%|█████▏    | 160/308 [01:42<01:35,  1.55it/s][1,0]<stderr>:#015 52%|█████▏    | 161/308 [01:43<01:34,  1.55it/s][1,0]<stderr>:#015 53%|█████▎    | 162/308 [01:43<01:35,  1.53it/s][1,0]<stderr>:#015 53%|█████▎    | 163/308 [01:44<01:34,  1.54it/s][1,0]<stderr>:#015 53%|█████▎    | 164/308 [01:45<01:33,  1.54it/s][1,0]<stderr>:#015 54%|█████▎    | 165/308 [01:45<01:32,  1.55it/s][1,0]<stderr>:#015 54%|█████▍    | 166/308 [01:46<01:31,  1.55it/s][1,0]<stderr>:#015 54%|█████▍    | 167/308 [01:47<01:32,  1.53it/s][1,0]<stderr>:#015 55%|█████▍    | 168/308 [01:47<01:31,  1.53it/s][1,0]<stderr>:#015 55%|█████▍    | 169/308 [01:48<01:31,  1.52it/s][1,0]<stderr>:#015 55%|█████▌    | 170/308 [01:49<01:30,  1.52it/s][1,0]<stderr>:#015 56%|█████▌    | 171/308 [01:49<01:30,  1.51it/s][1,0]<stderr>:#015 56%|█████▌    | 172/308 [01:50<01:29,  1.51it/s][1,0]<stderr>:#015 56%|█████▌    | 173/308 [01:51<01:28,  1.53it/s][1,0]<stderr>:#015 56%|█████▋    | 174/308 [01:51<01:26,  1.54it/s][1,0]<stderr>:#015 57%|█████▋    | 175/308 [01:52<01:27,  1.52it/s][1,0]<stderr>:#015 57%|█████▋    | 176/308 [01:53<01:25,  1.55it/s][1,0]<stderr>:#015 57%|█████▋    | 177/308 [01:53<01:23,  1.57it/s][1,0]<stderr>:#015 58%|█████▊    | 178/308 [01:54<01:22,  1.57it/s][1,0]<stderr>:#015 58%|█████▊    | 179/308 [01:54<01:22,  1.57it/s][1,0]<stderr>:#015 58%|█████▊    | 180/308 [01:55<01:22,  1.55it/s][1,0]<stderr>:#015 59%|█████▉    | 181/308 [01:56<01:22,  1.55it/s][1,0]<stderr>:#015 59%|█████▉    | 182/308 [01:56<01:21,  1.55it/s][1,0]<stderr>:#015 59%|█████▉    | 183/308 [01:57<01:19,  1.58it/s][1,0]<stderr>:#015 60%|█████▉    | 184/308 [01:58<01:18,  1.58it/s][1,0]<stderr>:#015 60%|██████    | 185/308 [01:58<01:18,  1.57it/s][1,0]<stderr>:#015 60%|██████    | 186/308 [01:59<01:19,  1.54it/s][1,0]<stderr>:#015 61%|██████    | 187/308 [02:00<01:19,  1.53it/s][1,0]<stderr>:#015 61%|██████    | 188/308 [02:00<01:18,  1.52it/s][1,0]<stderr>:#015 61%|██████▏   | 189/308 [02:01<01:17,  1.54it/s][1,0]<stderr>:#015 62%|██████▏   | 190/308 [02:02<01:16,  1.54it/s][1,0]<stderr>:#015 62%|██████▏   | 191/308 [02:02<01:15,  1.55it/s][1,0]<stderr>:#015 62%|██████▏   | 192/308 [02:03<01:15,  1.54it/s][1,0]<stderr>:#015 63%|██████▎   | 193/308 [02:03<01:14,  1.54it/s][1,0]<stderr>:#015 63%|██████▎   | 194/308 [02:04<01:13,  1.55it/s][1,0]<stderr>:#015 63%|██████▎   | 195/308 [02:05<01:13,  1.53it/s][1,0]<stderr>:#015 64%|██████▎   | 196/308 [02:05<01:14,  1.51it/s][1,0]<stderr>:#015 64%|██████▍   | 197/308 [02:06<01:13,  1.51it/s][1,0]<stderr>:#015 64%|██████▍   | 198/308 [02:07<01:11,  1.53it/s][1,0]<stderr>:#015 65%|██████▍   | 199/308 [02:07<01:11,  1.53it/s][1,0]<stderr>:#015 65%|██████▍   | 200/308 [02:08<01:11,  1.52it/s][1,0]<stderr>:#015 65%|██████▌   | 201/308 [02:09<01:11,  1.50it/s][1,0]<stderr>:#015 66%|██████▌   | 202/308 [02:09<01:10,  1.49it/s][1,0]<stderr>:#015 66%|██████▌   | 203/308 [02:10<01:10,  1.49it/s][1,0]<stderr>:#015 66%|██████▌   | 204/308 [02:11<01:08,  1.51it/s][1,0]<stderr>:#015 67%|██████▋   | 205/308 [02:11<01:07,  1.53it/s][1,0]<stderr>:#015 67%|██████▋   | 206/308 [02:12<01:07,  1.51it/s][1,0]<stderr>:#015 67%|██████▋   | 207/308 [02:13<01:06,  1.53it/s][1,0]<stderr>:#015 68%|██████▊   | 208/308 [02:13<01:04,  1.54it/s][1,0]<stderr>:#015 68%|██████▊   | 209/308 [02:14<01:03,  1.55it/s][1,0]<stderr>:#015 68%|██████▊   | 210/308 [02:15<01:03,  1.55it/s][1,0]<stderr>:#015 69%|██████▊   | 211/308 [02:15<01:03,  1.52it/s][1,0]<stderr>:#015 69%|██████▉   | 212/308 [02:16<01:02,  1.54it/s][1,0]<stderr>:#015 69%|██████▉   | 213/308 [02:17<01:01,  1.54it/s][1,0]<stderr>:#015 69%|██████▉   | 214/308 [02:17<01:00,  1.55it/s][1,0]<stderr>:#015 70%|██████▉   | 215/308 [02:18<00:59,  1.55it/s][1,0]<stderr>:#015 70%|███████   | 216/308 [02:18<00:59,  1.56it/s][1,0]<stderr>:#015 70%|███████   | 217/308 [02:19<00:59,  1.54it/s][1,0]<stderr>:#015 71%|███████   | 218/308 [02:20<00:57,  1.56it/s][1,0]<stderr>:#015 71%|███████   | 219/308 [02:20<00:57,  1.56it/s][1,0]<stderr>:#015 71%|███████▏  | 220/308 [02:21<00:56,  1.56it/s][1,0]<stderr>:#015 72%|███████▏  | 221/308 [02:22<00:56,  1.55it/s][1,0]<stderr>:#015 72%|███████▏  | 222/308 [02:22<00:55,  1.54it/s][1,0]<stderr>:#015 72%|███████▏  | 223/308 [02:23<00:54,  1.55it/s][1,0]<stderr>:#015 73%|███████▎  | 224/308 [02:24<00:55,  1.52it/s][1,0]<stderr>:#015 73%|███████▎  | 225/308 [02:24<00:53,  1.54it/s][1,0]<stderr>:#015 73%|███████▎  | 226/308 [02:25<00:52,  1.56it/s][1,0]<stderr>:#015 74%|███████▎  | 227/308 [02:26<00:52,  1.55it/s][1,0]<stderr>:#015 74%|███████▍  | 228/308 [02:26<00:51,  1.55it/s][1,0]<stderr>:#015 74%|██████\u001b[0m\n",
      "\u001b[34m█▍  | 229/308 [02:27<00:51,  1.53it/s][1,0]<stderr>:#015 75%|███████▍  | 230/308 [02:28<00:51,  1.52it/s][1,0]<stderr>:#015 75%|███████▌  | 231/308 [02:28<00:50,  1.54it/s][1,0]<stderr>:#015 75%|███████▌  | 232/308 [02:29<00:48,  1.56it/s][1,0]<stderr>:#015 76%|███████▌  | 233/308 [02:29<00:48,  1.56it/s][1,0]<stderr>:#015 76%|███████▌  | 234/308 [02:30<00:47,  1.57it/s][1,0]<stderr>:#015 76%|███████▋  | 235/308 [02:31<00:46,  1.56it/s][1,0]<stderr>:#015 77%|███████▋  | 236/308 [02:31<00:45,  1.58it/s][1,0]<stderr>:#015 77%|███████▋  | 237/308 [02:32<00:44,  1.58it/s][1,0]<stderr>:#015 77%|███████▋  | 238/308 [02:33<00:44,  1.58it/s][1,0]<stderr>:#015 78%|███████▊  | 239/308 [02:33<00:44,  1.57it/s][1,0]<stderr>:#015 78%|███████▊  | 240/308 [02:34<00:43,  1.56it/s][1,0]<stderr>:#015 78%|███████▊  | 241/308 [02:35<00:42,  1.56it/s][1,0]<stderr>:#015 79%|███████▊  | 242/308 [02:35<00:42,  1.54it/s][1,0]<stderr>:#015 79%|███████▉  | 243/308 [02:36<00:42,  1.52it/s][1,0]<stderr>:#015 79%|███████▉  | 244/308 [02:37<00:41,  1.54it/s][1,0]<stderr>:#015 80%|███████▉  | 245/308 [02:37<00:40,  1.56it/s][1,0]<stderr>:#015 80%|███████▉  | 246/308 [02:38<00:40,  1.55it/s][1,0]<stderr>:#015 80%|████████  | 247/308 [02:39<00:39,  1.54it/s][1,0]<stderr>:#015 81%|████████  | 248/308 [02:39<00:38,  1.54it/s][1,0]<stderr>:#015 81%|████████  | 249/308 [02:40<00:38,  1.55it/s][1,0]<stderr>:#015 81%|████████  | 250/308 [02:40<00:36,  1.58it/s][1,0]<stderr>:#015 81%|████████▏ | 251/308 [02:41<00:36,  1.57it/s][1,0]<stderr>:#015 82%|████████▏ | 252/308 [02:42<00:35,  1.56it/s][1,0]<stderr>:#015 82%|████████▏ | 253/308 [02:42<00:34,  1.57it/s][1,0]<stderr>:#015 82%|████████▏ | 254/308 [02:43<00:34,  1.57it/s][1,0]<stderr>:#015 83%|████████▎ | 255/308 [02:44<00:33,  1.57it/s][1,0]<stderr>:#015 83%|████████▎ | 256/308 [02:44<00:32,  1.58it/s][1,0]<stderr>:#015 83%|████████▎ | 257/308 [02:45<00:32,  1.58it/s][1,0]<stderr>:#015 84%|████████▍ | 258/308 [02:45<00:31,  1.57it/s][1,0]<stderr>:#015 84%|████████▍ | 259/308 [02:46<00:31,  1.58it/s][1,0]<stderr>:#015 84%|████████▍ | 260/308 [02:47<00:30,  1.58it/s][1,0]<stderr>:#015 85%|████████▍ | 261/308 [02:47<00:29,  1.57it/s][1,0]<stderr>:#015 85%|████████▌ | 262/308 [02:48<00:29,  1.56it/s][1,0]<stderr>:#015 85%|████████▌ | 263/308 [02:49<00:28,  1.57it/s][1,0]<stderr>:#015 86%|████████▌ | 264/308 [02:49<00:27,  1.57it/s][1,0]<stderr>:#015 86%|████████▌ | 265/308 [02:50<00:27,  1.56it/s][1,0]<stderr>:#015 86%|████████▋ | 266/308 [02:51<00:26,  1.56it/s][1,0]<stderr>:#015 87%|████████▋ | 267/308 [02:51<00:26,  1.56it/s][1,0]<stderr>:#015 87%|████████▋ | 268/308 [02:52<00:25,  1.57it/s][1,0]<stderr>:#015 87%|████████▋ | 269/308 [02:53<00:24,  1.57it/s][1,0]<stderr>:#015 88%|████████▊ | 270/308 [02:53<00:24,  1.57it/s][1,0]<stderr>:#015 88%|████████▊ | 271/308 [02:54<00:23,  1.57it/s][1,0]<stderr>:#015 88%|████████▊ | 272/308 [02:54<00:22,  1.57it/s][1,0]<stderr>:#015 89%|████████▊ | 273/308 [02:55<00:22,  1.58it/s][1,0]<stderr>:#015 89%|████████▉ | 274/308 [02:56<00:21,  1.56it/s][1,0]<stderr>:#015 89%|████████▉ | 275/308 [02:56<00:21,  1.56it/s][1,0]<stderr>:#015 90%|████████▉ | 276/308 [02:57<00:20,  1.56it/s][1,0]<stderr>:#015 90%|████████▉ | 277/308 [02:58<00:19,  1.58it/s][1,0]<stderr>:#015 90%|█████████ | 278/308 [02:58<00:19,  1.55it/s][1,0]<stderr>:#015 91%|█████████ | 279/308 [02:59<00:18,  1.58it/s][1,0]<stderr>:#015 91%|█████████ | 280/308 [03:00<00:17,  1.57it/s][1,0]<stderr>:#015 91%|█████████ | 281/308 [03:00<00:17,  1.57it/s][1,0]<stderr>:#015 92%|█████████▏| 282/308 [03:01<00:16,  1.57it/s][1,0]<stderr>:#015 92%|█████████▏| 283/308 [03:01<00:16,  1.55it/s][1,0]<stderr>:#015 92%|█████████▏| 284/308 [03:02<00:15,  1.54it/s][1,0]<stderr>:#015 93%|█████████▎| 285/308 [03:03<00:14,  1.55it/s][1,0]<stderr>:#015 93%|█████████▎| 286/308 [03:03<00:14,  1.56it/s][1,0]<stderr>:#015 93%|█████████▎| 287/308 [03:04<00:13,  1.58it/s][1,0]<stderr>:#015 94%|█████████▎| 288/308 [03:05<00:12,  1.54it/s][1,0]<stderr>:#015 94%|█████████▍| 289/308 [03:05<00:12,  1.55it/s][1,0]<stderr>:#015 94%|█████████▍| 290/308 [03:06<00:11,  1.55it/s][1,0]<stderr>:#015 94%|█████████▍| 291/308 [03:07<00:10,  1.55it/s][1,0]<stderr>:#015 95%|█████████▍| 292/308 [03:07<00:10,  1.54it/s][1,0]<stderr>:#015 95%|█████████▌| 293/308 [03:08<00:09,  1.54it/s][1,0]<stderr>:#015 95%|█████████▌| 294/308 [03:09<00:09,  1.53it/s][1,0]<stderr>:#015 96%|█████████▌| 295/308 [03:09<00:08,  1.51it/s][1,0]<stderr>:#015 96%|█████████▌| 296/308 [03:10<00:07,  1.52it/s][1,0]<stderr>:#015 96%|█████████▋| 297/308 [03:11<00:07,  1.54it/s][1,0]<stderr>:#015 97%|█████████▋| 298/308 [03:11<00:06,  1.54it/s][1,0]<stderr>:#015 97%|█████████▋| 299/308 [03:12<00:05,  1.51it/s][1,0]<stderr>:#015 97%|█████████▋| 300/308 [03:13<00:05,  1.54it/s][1,0]<stderr>:#015 98%|█████████▊| 301/308 [03:13<00:04,  1.53it/s][1,0]<stderr>:#015 98%|█████████▊| 302/308 [03:14<00:03,  1.54it/s][1,0]<stderr>:#015 98%|█████████▊| 303/308 [03:14<00:03,  1.55it/s][1,0]<stderr>:#015 99%|█████████▊| 304/308 [03:15<00:02,  1.55it/s][1,0]<stderr>:#015 99%|█████████▉| 305/308 [03:16<00:01,  1.55it/s][1,0]<stderr>:#015 99%|█████████▉| 306/308 [03:16<00:01,  1.57it/s][1,0]<stderr>:#015100%|█████████▉| 307/308 [03:17<00:00,  1.58it/s][1,0]<stderr>:#015100%|██████████| 308/308 [03:18<00:00,  1.57it/s][1,1]<stderr>:INFO:/opt/conda/lib/python3.6/site-packages/datasets/metric.py:Removing /root/.cache/huggingface/metrics/glue/mnli/default_experiment-1-0.arrow\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:INFO:/opt/conda/lib/python3.6/site-packages/datasets/metric.py:Removing /root/.cache/huggingface/metrics/glue/mnli/default_experiment-1-0.arrow\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:INFO:/opt/conda/lib/python3.6/site-packages/datasets/metric.py:Removing /root/.cache/huggingface/metrics/glue/mnli/default_experiment-1-0.arrow\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:INFO:/opt/conda/lib/python3.6/site-packages/datasets/metric.py:Removing /root/.cache/huggingface/metrics/glue/mnli/default_experiment-1-0.arrow\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:__main__:*** Predict ***\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:run_glue.py:502: FutureWarning: remove_columns_ is deprecated and will be removed in the next major version of datasets. Use Dataset.remove_columns instead.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  predict_dataset.remove_columns_(\"label\")\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:INFO:/opt/conda/lib/python3.6/site-packages/datasets/metric.py:Removing /root/.cache/huggingface/metrics/glue/mnli/default_experiment-1-0.arrow\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:INFO:__main__:*** Predict ***\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:run_glue.py:502: FutureWarning: remove_columns_ is deprecated and will be removed in the next major version of datasets. Use Dataset.remove_columns instead.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  predict_dataset.remove_columns_(\"label\")\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:INFO:__main__:*** Predict ***\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:run_glue.py:502: FutureWarning: remove_columns_ is deprecated and will be removed in the next major version of datasets. Use Dataset.remove_columns instead.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  predict_dataset.remove_columns_(\"label\")\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:/opt/conda/lib/python3.6/site-packages/datasets/metric.py:Removing /root/.cache/huggingface/metrics/glue/mnli/default_experiment-1-0.arrow\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015100%|██████████| 308/308 [03:18<00:00,  1.55it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:/opt/conda/lib/python3.6/site-packages/datasets/metric.py:Removing /root/.cache/huggingface/metrics/glue/mnli/default_experiment-1-0.arrow\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:/opt/conda/lib/python3.6/site-packages/datasets/metric.py:Removing /root/.cache/huggingface/metrics/glue/mnli/default_experiment-1-0.arrow\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:INFO:__main__:*** Predict ***\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:run_glue.py:502: FutureWarning: remove_columns_ is deprecated and will be removed in the next major version of datasets. Use Dataset.remove_columns instead.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  predict_dataset.remove_columns_(\"label\")\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:INFO:__main__:*** Predict ***\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:run_glue.py:502: FutureWarning: remove_columns_ is deprecated and will be removed in the next major version of datasets. Use Dataset.remove_columns instead.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  predict_dataset.remove_columns_(\"label\")\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[INFO|trainer.py:516] 2022-05-02 14:05:10,090 >> The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:[INFO|trainer.py:516] 2022-05-02 14:05:10,095 >> The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[INFO|trainer.py:2115] 2022-05-02 14:05:10,095 >> ***** Running Prediction *****\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[INFO|trainer.py:2117] 2022-05-02 14:05:10,095 >>   Num examples = 9796\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[INFO|trainer.py:2120] 2022-05-02 14:05:10,096 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:[INFO|trainer.py:2115] 2022-05-02 14:05:10,098 >> ***** Running Prediction *****\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:[INFO|trainer.py:2117] 2022-05-02 14:05:10,098 >>   Num examples = 9796\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:[INFO|trainer.py:2120] 2022-05-02 14:05:10,099 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:[INFO|trainer.py:516] 2022-05-02 14:05:10,135 >> The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: premise, idx, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:[INFO|trainer.py:2115] 2022-05-02 14:05:10,139 >> ***** Running Prediction *****\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:[INFO|trainer.py:2117] 2022-05-02 14:05:10,139 >>   Num examples = 9796\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:[INFO|trainer.py:2120] 2022-05-02 14:05:10,139 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:907] 2022-05-02 14:05:10,175 >> ***** eval metrics *****\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2022-05-02 14:05:10,176 >>   epoch                     =       0.04\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2022-05-02 14:05:10,176 >>   eval_accuracy             =     0.3182\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2022-05-02 14:05:10,176 >>   eval_loss                 =     1.0992\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2022-05-02 14:05:10,176 >>   eval_mem_cpu_alloc_delta  =       29MB\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2022-05-02 14:05:10,176 >>   eval_mem_cpu_peaked_delta =        0MB\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2022-05-02 14:05:10,176 >>   eval_mem_gpu_alloc_delta  =        0MB\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2022-05-02 14:05:10,176 >>   eval_mem_gpu_peaked_delta =      150MB\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2022-05-02 14:05:10,176 >>   eval_runtime              = 0:03:19.19\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2022-05-02 14:05:10,176 >>   eval_samples              =       9832\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2022-05-02 14:05:10,176 >>   eval_samples_per_second   =     49.358\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:__main__:*** Predict ***\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:run_glue.py:502: FutureWarning: remove_columns_ is deprecated and will be removed in the next major version of datasets. Use Dataset.remove_columns instead.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  predict_dataset.remove_columns_(\"label\")\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:__main__:*** Predict ***\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:run_glue.py:502: FutureWarning: remove_columns_ is deprecated and will be removed in the next major version of datasets. Use Dataset.remove_columns instead.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  predict_dataset.remove_columns_(\"label\")\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|trainer.py:516] 2022-05-02 14:05:10,227 >> The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: hypothesis, premise, idx.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|trainer.py:2115] 2022-05-02 14:05:10,230 >> ***** Running Prediction *****\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|trainer.py:2117] 2022-05-02 14:05:10,231 >>   Num examples = 9796\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|trainer.py:2120] 2022-05-02 14:05:10,231 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:__main__:*** Predict ***\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:run_glue.py:502: FutureWarning: remove_columns_ is deprecated and will be removed in the next major version of datasets. Use Dataset.remove_columns instead.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  predict_dataset.remove_columns_(\"label\")\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[INFO|trainer.py:516] 2022-05-02 14:05:10,322 >> The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: premise, idx, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[INFO|trainer.py:2115] 2022-05-02 14:05:10,326 >> ***** Running Prediction *****\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[INFO|trainer.py:2117] 2022-05-02 14:05:10,326 >>   Num examples = 9796\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[INFO|trainer.py:2120] 2022-05-02 14:05:10,327 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:516] 2022-05-02 14:05:10,375 >> The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: hypothesis, idx, premise.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:2115] 2022-05-02 14:05:10,379 >> ***** Running Prediction *****\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:2117] 2022-05-02 14:05:10,380 >>   Num examples = 9796\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:2120] 2022-05-02 14:05:10,380 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[INFO|trainer.py:516] 2022-05-02 14:05:10,432 >> The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[INFO|trainer.py:2115] 2022-05-02 14:05:10,436 >> ***** Running Prediction *****\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[INFO|trainer.py:2117] 2022-05-02 14:05:10,436 >>   Num examples = 9796\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[INFO|trainer.py:2120] 2022-05-02 14:05:10,436 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[INFO|trainer.py:516] 2022-05-02 14:05:10,456 >> The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: premise, hypothesis, idx.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[INFO|trainer.py:2115] 2022-05-02 14:05:10,460 >> ***** Running Prediction *****\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[INFO|trainer.py:2117] 2022-05-02 14:05:10,461 >>   Num examples = 9796\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[INFO|trainer.py:2120] 2022-05-02 14:05:10,461 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  0%|          | 0/307 [00:00<?, ?it/s][1,0]<stderr>:#015  1%|          | 2/307 [00:00<01:35,  3.19it/s][1,0]<stderr>:#015  1%|          | 3/307 [00:01<02:02,  2.48it/s][1,0]<stderr>:#015  1%|▏         | 4/307 [00:01<02:23,  2.11it/s][1,0]<stderr>:#015  2%|▏         | 5/307 [00:02<02:37,  1.92it/s][1,0]<stderr>:#015  2%|▏         | 6/307 [00:03<02:47,  1.80it/s][1,0]<stderr>:#015  2%|▏         | 7/307 [00:03<02:55,  1.71it/s][1,0]<stderr>:#015  3%|▎         | 8/307 [00:04<03:07,  1.60it/s][1,0]<stderr>:#015  3%|▎         | 9/307 [00:05<03:08,  1.58it/s][1,0]<stderr>:#015  3%|▎         | 10/307 [00:05<03:09,  1.56it/s][1,0]<stderr>:#015  4%|▎         | 11/307 [00:06<03:07,  1.58it/s][1,0]<stderr>:#015  4%|▍         | 12/307 [00:07<03:06,  1.58it/s][1,0]<stderr>:#015  4%|▍         | 13/307 [00:07<03:07,  1.57it/s][1,0]<stderr>:#015  5%|▍         | 14/307 [00:08<03:08,  1.55it/s][1,0]<stderr>:#015  5%|▍         | 15/307 [00:09<03:06,  1.56it/s][1,0]<stderr>:#015  5%|▌         | 16/307 [00:09<03:04,  1.58it/s][1,0]<stderr>:#015  6%|▌         | 17/307 [00:10<03:03,  1.58it/s][1,0]<stderr>:#015  6%|▌         | 18/307 [00:10<03:02,  1.58it/s][1,0]<stderr>:#015  6%|▌         | 19/307 [00:11<03:02,  1.58it/s][1,0]<stderr>:#015  7%|▋         | 20/307 [00:12<02:59,  1.60it/s][1,0]<stderr>:#015  7%|▋         | 21/307 [00:12<02:59,  1.59it/s][1,0]<stderr>:#015  7%|▋         | 22/307 [00:13<02:58,  1.60it/s][1,0]<stderr>:#015  7%|▋         | 23/307 [00:14<02:57,  1.60it/s][1,0]<stderr>:#015  8%|▊         | 24/307 [00:14<02:59,  1.58it/s][1,0]<stderr>:#015  8%|▊         | 25/307 [00:15<02:57,  1.59it/s][1,0]<stderr>:#015  8%|▊         | 26/307 [00:15<02:57,  1.58it/s][1,0]<stderr>:#015  9%|▉         | 27/307 [00:16<02:53,  1.61it/s][1,0]<stderr>:#015  9%|▉         | 28/307 [00:17<02:53,  1.60it/s][1,0]<stderr>:#015  9%|▉         | 29/307 [00:17<02:52,  1.61it/s][1,0]<stderr>:#015 10%|▉         | 30/307 [00:18<02:53,  1.60it/s][1,0]<stderr>:#015 10%|█         | 31/307 [00:19<02:51,  1.61it/s][1,0]<stderr>:#015 10%|█         | 32/307 [00:19<02:50,  1.61it/s][1,0]<stderr>:#015 11%|█         | 33/307 [00:20<02:55,  1.56it/s][1,0]<stderr>:#015 11%|█         | 34/307 [00:20<02:51,  1.59it/s][1,0]<stderr>:#015 11%|█▏        | 35/307 [00:21<02:50,  1.59it/s][1,0]<stderr>:#015 12%|█▏        | 36/307 [00:22<02:50,  1.59it/s][1,0]<stderr>:#015 12%|█▏        | 37/307 [00:22<02:49,  1.60it/s][1,0]<stderr>:#015 12%|█▏        | 38/307 [00:23<02:48,  1.59it/s][1,0]<stderr>:#015 13%|█▎        | 39/307 [00:24<02:52,  1.56it/s][1,0]<stderr>:#015 13%|█▎        | 40/307 [00:24<02:53,  1.54it/s][1,0]<stderr>:#015 13%|█▎        | 41/307 [00:25<02:55,  1.51it/s][1,0]<stderr>:#015 14%|█▎        | 42/307 [00:26<02:54,  1.52it/s][1,0]<stderr>:#015 14%|█▍        | 43/307 [00:26<02:52,  1.53it/s][1,0]<stderr>:#015 14%|█▍        | 44/307 [00:27<02:51,  1.53it/s][1,0]<stderr>:#015 15%|█▍        | 45/307 [00:28<02:50,  1.54it/s][1,0]<stderr>:#015 15%|█▍        | 46/307 [00:28<02:48,  1.55it/s][1,0]<stderr>:#015 15%|█▌        | 47/307 [00:29<02:45,  1.57it/s][1,0]<stderr>:#015 16%|█▌        | 48/307 [00:29<02:43,  1.59it/s][1,0]<stderr>:#015 16%|█▌        | 49/307 [00:30<02:43,  1.58it/s][1,0]<stderr>:#015 16%|█▋        | 50/307 [00:31<02:40,  1.60it/s][1,0]<stderr>:#015 17%|█▋        | 51/307 [00:31<02:41,  1.58it/s][1,0]<stderr>:#015 17%|█▋        | 52/307 [00:32<02:41,  1.58it/s][1,0]<stderr>:#015 17%|█▋        | 53/307 [00:33<02:40,  1.59it/s][1,0]<stderr>:#015 18%|█▊        | 54/307 [00:33<02:40,  1.57it/s][1,0]<stderr>:#015 18%|█▊        | 55/307 [00:34<02:43,  1.54it/s][1,0]<stderr>:#015 18%|█▊        | 56/307 [00:35<02:43,  1.53it/s][1,0]<stderr>:#015 19%|█▊        | 57/307 [00:35<02:41,  1.55it/s][1,0]<stderr>:#015 19%|█▉        | 58/307 [00:36<02:40,  1.55it/s][1,0]<stderr>:#015 19%|█▉        | 59/307 [00:36<02:40,  1.55it/s][1,0]<stderr>:#015 20%|█▉        | 60/307 [00:37<02:37,  1.57it/s][1,0]<stderr>:#015 20%|█▉        | 61/307 [00:38<02:33,  1.60it/s][1,0]<stderr>:#015 20%|██        | 62/307 [00:38<02:36,  1.56it/s][1,0]<stderr>:#015 21%|██        | 63/307 [00:39<02:35,  1.57it/s][1,0]<stderr>:#015 21%|██        | 64/307 [00:40<02:35,  1.57it/s][1,0]<stderr>:#015 21%|██        | 65/307 [00:40<02:34,  1.57it/s][1,0]<stderr>:#015 21%|██▏       | 66/307 [00:41<02:33,  1.57it/s][1,0]<stderr>:#015 22%|██▏       | 67/307 [00:42<02:31,  1.58it/s][1,0]<stderr>:#015 22%|██▏       | 68/307 [00:42<02:28,  1.61it/s][1,0]<stderr>:#015 22%|██▏       | 69/307 [00:43<02:29,  1.59it/s][1,0]<stderr>:#015 23%|██▎       | 70/307 [00:43<02:26,  1.62it/s][1,0]<stderr>:#015 23%|██▎       | 71/307 [00:44<02:28,  1.59it/s][1,0]<stderr>:#015 23%|██▎       | 72/307 [00:45<02:30,  1.56it/s][1,0]<stderr>:#015 24%|██▍       | 73/307 [00:45<02:30,  1.56it/s][1,0]<stderr>:#015 24%|██▍       | 74/307 [00:46<02:28,  1.57it/s][1,0]<stderr>:#015 24%|██▍       | 75/307 [00:47<02:26,  1.58it/s][1,0]<stderr>:#015 25%|██▍       | 76/307 [00:47<02:24,  1.60it/s][1,0]<stderr>:#015 25%|██▌       | 77/307 [00:48<02:24,  1.59it/s][1,0]<stderr>:#015 25%|██▌       | 78/307 [00:48<02:22,  1.61it/s][1,0]<stderr>:#015 26%|██▌       | 79/307 [00:49<02:24,  1.58it/s][1,0]<stderr>:#015 26%|██▌       | 80/307 [00:50<02:22,  1.59it/s][1,0]<stderr>:#015 26%|██▋       | 81/307 [00:50<02:21,  1.60it/s][1,0]<stderr>:#015 27%|██▋       | 82/307 [00:51<02:22,  1.58it/s][1,0]<stderr>:#015 27%|██▋       | 83/307 [00:52<02:22,  1.57it/s][1,0]<stderr>:#015 27%|██▋       | 84/307 [00:52<02:20,  1.58it/s][1,0]<stderr>:#015 28%|██▊       | 85/307 [00:53<02:20,  1.59it/s][1,0]<stderr>:#015 28%|██▊       | 86/307 [00:53<02:18,  1.59it/s][1,0]<stderr>:#015 28%|██▊       | 87/307 [00:54<02:19,  1.57it/s][1,0]<stderr>:#015 29%|██▊       | 88/307 [00:55<02:22,  1.54it/s][1,0]<stderr>:#015 29%|██▉       | 89/307 [00:55<02:21,  1.54it/s][1,0]<stderr>:#015 29%|██▉       | 90/307 [00:56<02:19,  1.56it/s][1,0]<stderr>:#015 30%|██▉       | 91/307 [00:57<02:19,  1.55it/s][1,0]<stderr>:#015 30%|██▉       | 92/307 [00:57<02:18,  1.55it/s][1,0]<stderr>:#015 30%|███       | 93/307 [00:58<02:19,  1.53it/s][1,0]<stderr>:#015 31%|███       | 94/307 [00:59<02:18,  1.54it/s][1,0]<stderr>:#015 31%|███       | 95/307 [00:59<02:15,  1.56it/s][1,0]<stderr>:#015 31%|███▏      | 96/307 [01:00<02:15,  1.56it/s][1,0]<stderr>:#015 32%|███▏      | 97/307 [01:01<02:14,  1.57it/s][1,0]<stderr>:#015 32%|███▏      | 98/307 [01:01<02:11,  1.59it/s][1,0]<stderr>:#015 32%|███▏      | 99/307 [01:02<02:10,  1.59it/s][1,0]<stderr>:#015 33%|███▎      | 100/307 [01:02<02:10,  1.59it/s][1,0]<stderr>:#015 33%|███▎      | 101/307 [01:03<02:10,  1.57it/s][1,0]<stderr>:#015 33%|███▎      | 102/307 [01:04<02:12,  1.55it/s][1,0]<stderr>:#015 34%|███▎      | 103/307 [01:04<02:10,  1.57it/s][1,0]<stderr>:#015 34%|███▍      | 104/307 [01:05<02:07,  1.59it/s][1,0]<stderr>:#015 34%|███▍      | 105/307 [01:06<02:06,  1.59it/s][1,0]<stderr>:#015 35%|███▍      | 106/307 [01:06<02:07,  1.57it/s][1,0]<stderr>:#015 35%|███▍      | 107/307 [01:07<02:07,  1.56it/s][1,0]<stderr>:#015 35%|███▌      | 108/307 [01:08<02:07,  1.56it/s][1,0]<stderr>:#015 36%|███▌      | 109/307 [01:08<02:05,  1.58it/s][1,0]<stderr>:#015 36%|███▌      | 110/307 [01:09<02:04,  1.58it/s][1,0]<stderr>:#015 36%|███▌      | 111/307 [01:09<02:04,  1.58it/s][1,0]<stderr>:#015 36%|███▋      | 112/307 [01:10<02:02,  1.59it/s][1,0]<stderr>:#015 37%|███▋      | 113/307 [01:11<02:02,  1.58it/s][1,0]<stderr>:#015 37%|███▋      | 114/307 [01:11<02:01,  1.58it/s][1,0]<stderr>:#015 37%|███▋      | 115/307 [01:12<02:00,  1.60it/s][1,0]<stderr>:#015 38%|███▊      | 116/307 [01:13<02:01,  1.58it/s][1,0]<stderr>:#015 38%|███▊      | 117/307 [01:13<02:00,  1.58it/s][1,0]<stderr>:#015 38%|███▊      | 118/307 [01:14<01:58,  1.59it/s][1,0]<stderr>:#015 39%|███▉      | 119/307 [01:15<01:58,  1.59it/s][1,0]<stderr>:#015 39%|███▉      | 120/307 [01:15<01:59,  1.57it/s][1,0]<stderr>:#015 39%|███▉      | 121/307 [01:16<01:58,  1.58it/s][1,0]<stderr>:#015 40%|███▉      | 122/307 [01:16<01:57,  1.57it/s][1,0]<stderr>:#015 40%|████      | 123/307 [01:17<01:58,  1.55it/s][1,0]<stderr>:#015 40%|████      | 124/307 [01:18<01:57,  1.56it/s][1,0]<stderr>:#015 41%|████      | 125/307 [01:18<01:55,  1.57it/s][1,0]<stderr>:#015 41%|████      | 126/307 [01:19<01:55,  1.57it/s][1,0]<stderr>:#015 41%|████▏     | 127/307 [01:20<01:55,  1.56it/s][1,0]<stderr>:#015 42%|████▏     | 128/307 [01:20<01:55,  1.55it/s][1,0]<stderr>:#015 42%|████▏     | 129/307 [01:21<01:55,  1.55it/s][1,0]<stderr>:#015 42%|████▏     | 130/307 [01:22<01:53,  1.56it/s][1,0]<stderr>:#015 43%|████▎     | 131/307 [01:22<01:52,  1.57it/s][1,0]<stderr>:#015 43%|████▎     | 132/307 [01:23<01:55,  1.51it/s][1,0]<stderr>:#015 43%|████▎     | 133/307 [01:24<01:54,  1.52it/s][1,0]<stderr>:#015 44%|████▎     | 134/307 [01:24<01:54,  1.51it/s][1,0]<stderr>:#015 44%|████▍     | 135/307 [01:25<01:52,  1.52it/s][1,0]<stderr>:#015 44%|████▍     | 136/307 [01:26<01:50,  1.54it/s][1,0]<stderr>:#015 45%|████▍     | 137/307 [01:26<01:50,  1.54it/s][1,0]<stderr>:#015 45%|████▍     | 138/307 [01:27<01:49,  1.55it/s][1,0]<stderr>:#015 45%|████▌     | 139/307 [01:27<01:48,  1.55it/s][1,0]<stderr>:#015 46%|████▌     | 140/307 [01:28<01:46,  1.57it/s][1,0]<stderr>:#015 46%|████▌     | 141/307 [01:29<01:44,  1.58it/s][1,0]<stderr>:#015 46%|████▋     | 142/307 [01:29<01:42,  1.60it/s][1,0]<stderr>:#015 47%|████▋     | 143/307 [01:30<01:41,  1.61it/s][1,0]<stderr>:#015 47%|████▋     | 144/307 [01:31<01:42,  1.59it/s][1,0]<stderr>:#015 47%|████▋     | 145/307 [01:31<01:43,  1.57it/s][1,0]<stderr>:#015 48%|████▊     | 146/307 [01:32<01:41,  1.58it/s][1,0]<stderr>:#015 48%|████▊     | 147/307 [01:32<01:41,  1.57it/s][1,0]<stderr>:#015 48%|████▊     | 148/307 [01:33<01:41,  1.56it/s][1,0]<stderr>:#015 49%|████▊     | 149/307 [01:34<01:40,  1.57it/s][1,0]<stderr>:#015 49%|████▉     | 150/307 [01:34<01:39,  1.57it/s][1,0]<stderr>:#015 49%|████▉     | 151/307 [01:35<01:38,  1.59it/s][1,0]<stderr>:#015 50%|████▉     | 152/307 [01:36<01:39,  1.56it/s][1,0]<stderr>:#015 50%|████▉     | 153/307 [01:36<01:38,  1.56it/s][1,0]<stderr>:#015 50%|█████     | 154/307 [01:37<01:39,  1.54it/s][1,0]<stderr>:#015 50%|█████     | 155/307 [01:38<01:36,  1.57it/s][1,0]<stderr>:#015 51%|█████     | 156/307 [01:38<01:35,  1.59it/s][1,0]<stderr>:#015 51%|█████     | 157/307 [01:39<01:33,  1.61it/s][1,0]<stderr>:#015 51%|█████▏    | 158/307 [01:39<01:34,  1.58it/s][1,0]<stderr>:#015 52%|█████▏    | 159/307 [01:40<01:35,  1.55it/s][1,0]<stderr>:#015 52%|█████▏    | 160/307 [01:41<01:34,  1.55it/s][1,0]<stderr>:#015 52%|█████▏    | 161/307 [01:41<01:33,  1.56it/s][1,0]<stderr>:#015 53%|█████▎    | 162/307 [01:42<01:32,  1.57it/s][1,0]<stderr>:#015 53%|█████▎    | 163/307 [01:43<01:31,  1.58it/s][1,0]<stderr>:#015 53%|█████▎    | 164/307 [01:43<01:31,  1.57it/s][1,0]<stderr>:#015 54%|█████▎    | 165/307 [01:44<01:33,  1.52it/s][1,0]<stderr>:#015 54%|█████▍    | 166/307 [01:45<01:31,  1.55it/s][1,0]<stderr>:#015 54%|█████▍    | 167/307 [01:45<01:30,  1.55it/s][1,0]<stderr>:#015 55%|█████▍    | 168/307 [01:46<01:28,  1.57it/s][1,0]<stderr>:#015 55%|█████▌    | 169/307 [01:47<01:27,  1.58it/s][1,0]<stderr>:#015 55%|█████▌    | 170/307 [01:47<01:26,  1.58it/s][1,0]<stderr>:#015 56%|█████▌    | 171/307 [01:48<01:25,  1.59it/s][1,0]<stderr>:#015 56%|█████▌    | 172/307 [01:48<01:24,  1.59it/s][1,0]<stderr>:#015 56%|█████▋    | 173/307 [01:49<01:25,  1.57it/s][1,0]<stderr>:#015 57%|█████▋    | 174/307 [01:50<01:26,  1.54it/s][1,0]<stderr>:#015 57%|█████▋    | 175/307 [01:50<01:25,  1.54it/s][1,0]<stderr>:#015 57%|█████▋    | 176/307 [01:51<01:23,  1.56it/s][1,0]<stderr>:#015 58%|█████▊    | 177/307 [01:52<01:22,  1.58it/s][1,0]<stderr>:#015 58%|█████▊    | 178/307 [01:52<01:21,  1.58it/s][1,0]<stderr>:#015 58%|█████▊    | 179/307 [01:53<01:21,  1.57it/s][1,0]<stderr>:#015 59%|█████▊    | 180/307 [01:54<01:22,  1.55it/s][1,0]<stderr>:#015 59%|█████▉    | 181/307 [01:54<01:20,  1.56it/s][1,0]<stderr>:#015 59%|█████▉    | 182/307 [01:55<01:19,  1.57it/s][1,0]<stderr>:#015 60%|█████▉    | 183/307 [01:55<01:19,  1.57it/s][1,0]<stderr>:#015 60%|█████▉    | 184/307 [01:56<01:20,  1.54it/s][1,0]<stderr>:#015 60%|██████    | 185/307 [01:57<01:20,  1.52it/s][1,0]<stderr>:#015 61%|██████    | 186/307 [01:57<01:18,  1.54it/s][1,0]<stderr>:#015 61%|██████    | 187/307 [01:58<01:18,  1.52it/s][1,0]<stderr>:#015 61%|██████    | 188/307 [01:59<01:16,  1.55it/s][1,0]<stderr>:#015 62%|██████▏   | 189/307 [01:59<01:15,  1.56it/s][1,0]<stderr>:#015 62%|██████▏   | 190/307 [02:00<01:15,  1.55it/s][1,0]<stderr>:#015 62%|██████▏   | 191/307 [02:01<01:16,  1.52it/s][1,0]<stderr>:#015 63%|██████▎   | 192/307 [02:01<01:14,  1.54it/s][1,0]<stderr>:#015 63%|██████▎   | 193/307 [02:02<01:13,  1.56it/s][1,0]<stderr>:#015 63%|██████▎   | 194/307 [02:03<01:12,  1.56it/s][1,0]<stderr>:#015 64%|██████▎   | 195/307 [02:03<01:11,  1.56it/s][1,0]<stderr>:#015 64%|██████▍   | 196/307 [02:04<01:11,  1.55it/s][1,0]<stderr>:#015 64%|██████▍   | 197/307 [02:05<01:10,  1.56it/s][1,0]<stderr>:#015 64%|██████▍   | 198/307 [02:05<01:09,  1.57it/s][1,0]<stderr>:#015 65%|██████▍   | 199/307 [02:06<01:08,  1.59it/s][1,0]<stderr>:#015 65%|██████▌   | 200/307 [02:06<01:07,  1.58it/s][1,0]<stderr>:#015 65%|██████▌   | 201/307 [02:07<01:06,  1.60it/s][1,0]<stderr>:#015 66%|██████▌   | 202/307 [02:08<01:04,  1.62it/s][1,0]<stderr>:#015 66%|██████▌   | 203/307 [02:08<01:04,  1.62it/s][1,0]<stderr>:#015 66%|██████▋   | 204/307 [02:09<01:04,  1.60it/s][1,0]<stderr>:#015 67%|██████▋   | 205/307 [02:10<01:04,  1.59it/s][1,0]<stderr>:#015 67%|██████▋   | 206/307 [02:10<01:03,  1.59it/s][1,0]<stderr>:#015 67%|██████▋   | 207/307 [02:11<01:02,  1.59it/s][1,0]<stderr>:#015 68%|██████▊   | 208/307 [02:11<01:03,  1.57it/s][1,0]<stderr>:#015 68%|██████▊   | 209/307 [02:12<01:03,  1.55it/s][1,0]<stderr>:#015 68%|██████▊   | 210/307 [02:13<01:03,  1.53it/s][1,0]<stderr>:#015 69%|██████▊   | 211/307 [02:13<01:02,  1.55it/s][1,0]<stderr>:#015 69%|██████▉   | 212/307 [02:14<01:00,  1.57it/s][1,0]<stderr>:#015 69%|██████▉   | 213/307 [02:15<00:58,  1.59it/s][1,0]<stderr>:#015 70%|██████▉   | 214/307 [02:15<00:59,  1.57it/s][1,0]<stderr>:#015 70%|███████   | 215/307 [02:16<00:58,  1.57it/s][1,0]<stderr>:#015 70%|███████   | 216/307 [02:17<00:57,  1.58it/s][1,0]<stderr>:#015 71%|███████   | 217/307 [02:17<00:57,  1.57it/s][1,0]<stderr>:#015 71%|███████   | 218/307 [02:18<00:57,  1.55it/s][1,0]<stderr>:#015 71%|███████▏  | 219/307 [02:19<00:57,  1.54it/s][1,0]<stderr>:#015 72%|███████▏  | 220/307 [02:19<00:57,  1.53it/s][1,0]<stderr>:#015 72%|███████▏  | 221/307 [02:20<00:56,  1.52it/s][1,0]<stderr>:#015 72%|███████▏  | 222/307 [02:20<00:55,  1.53it/s][1,0]<stderr>:#015 73%|███████▎  | 223/307 [02:21<00:54,  1.54it/s][1,0]<stderr>:#015 73%|███████▎  | 224/307 [02:22<00:54,  1.52it/s][1,0]<stderr>:#015 73%|███████▎  | 225/307 [02:22<00:53,  1.52it/s][1,0]<stderr>:#015 74%|███████▎  | 226/307 [02:23<00:52,  1.55it/s][1,0]<stderr>:#015 74%|███████▍  | 227/307 [02:24<00:51,  1.55it/s][1,0]<stderr>:#015 74%|███████▍  | 228/307 [02:24<00:51,  1.53it/s][1,0]<stderr>:#015 75%|████\u001b[0m\n",
      "\u001b[34m███▍  | 229/307 [02:25<00:50,  1.55it/s][1,0]<stderr>:#015 75%|███████▍  | 230/307 [02:26<00:49,  1.56it/s][1,0]<stderr>:#015 75%|███████▌  | 231/307 [02:26<00:49,  1.55it/s][1,0]<stderr>:#015 76%|███████▌  | 232/307 [02:27<00:47,  1.56it/s][1,0]<stderr>:#015 76%|███████▌  | 233/307 [02:28<00:47,  1.55it/s][1,0]<stderr>:#015 76%|███████▌  | 234/307 [02:28<00:46,  1.56it/s][1,0]<stderr>:#015 77%|███████▋  | 235/307 [02:29<00:46,  1.54it/s][1,0]<stderr>:#015 77%|███████▋  | 236/307 [02:30<00:46,  1.53it/s][1,0]<stderr>:#015 77%|███████▋  | 237/307 [02:30<00:45,  1.53it/s][1,0]<stderr>:#015 78%|███████▊  | 238/307 [02:31<00:45,  1.52it/s][1,0]<stderr>:#015 78%|███████▊  | 239/307 [02:32<00:44,  1.52it/s][1,0]<stderr>:#015 78%|███████▊  | 240/307 [02:32<00:43,  1.54it/s][1,0]<stderr>:#015 79%|███████▊  | 241/307 [02:33<00:43,  1.53it/s][1,0]<stderr>:#015 79%|███████▉  | 242/307 [02:33<00:41,  1.55it/s][1,0]<stderr>:#015 79%|███████▉  | 243/307 [02:34<00:40,  1.57it/s][1,0]<stderr>:#015 79%|███████▉  | 244/307 [02:35<00:39,  1.59it/s][1,0]<stderr>:#015 80%|███████▉  | 245/307 [02:35<00:38,  1.61it/s][1,0]<stderr>:#015 80%|████████  | 246/307 [02:36<00:38,  1.60it/s][1,0]<stderr>:#015 80%|████████  | 247/307 [02:37<00:38,  1.58it/s][1,0]<stderr>:#015 81%|████████  | 248/307 [02:37<00:37,  1.58it/s][1,0]<stderr>:#015 81%|████████  | 249/307 [02:38<00:37,  1.57it/s][1,0]<stderr>:#015 81%|████████▏ | 250/307 [02:38<00:36,  1.58it/s][1,0]<stderr>:#015 82%|████████▏ | 251/307 [02:39<00:34,  1.60it/s][1,0]<stderr>:#015 82%|████████▏ | 252/307 [02:40<00:34,  1.60it/s][1,0]<stderr>:#015 82%|████████▏ | 253/307 [02:40<00:33,  1.60it/s][1,0]<stderr>:#015 83%|████████▎ | 254/307 [02:41<00:33,  1.59it/s][1,0]<stderr>:#015 83%|████████▎ | 255/307 [02:42<00:32,  1.59it/s][1,0]<stderr>:#015 83%|████████▎ | 256/307 [02:42<00:32,  1.59it/s][1,0]<stderr>:#015 84%|████████▎ | 257/307 [02:43<00:31,  1.59it/s][1,0]<stderr>:#015 84%|████████▍ | 258/307 [02:43<00:30,  1.60it/s][1,0]<stderr>:#015 84%|████████▍ | 259/307 [02:44<00:29,  1.60it/s][1,0]<stderr>:#015 85%|████████▍ | 260/307 [02:45<00:29,  1.61it/s][1,0]<stderr>:#015 85%|████████▌ | 261/307 [02:45<00:28,  1.59it/s][1,0]<stderr>:#015 85%|████████▌ | 262/307 [02:46<00:28,  1.57it/s][1,0]<stderr>:#015 86%|████████▌ | 263/307 [02:47<00:28,  1.57it/s][1,0]<stderr>:#015 86%|████████▌ | 264/307 [02:47<00:27,  1.58it/s][1,0]<stderr>:#015 86%|████████▋ | 265/307 [02:48<00:26,  1.61it/s][1,0]<stderr>:#015 87%|████████▋ | 266/307 [02:48<00:25,  1.59it/s][1,0]<stderr>:#015 87%|████████▋ | 267/307 [02:49<00:25,  1.58it/s][1,0]<stderr>:#015 87%|████████▋ | 268/307 [02:50<00:24,  1.56it/s][1,0]<stderr>:#015 88%|████████▊ | 269/307 [02:50<00:24,  1.54it/s][1,0]<stderr>:#015 88%|████████▊ | 270/307 [02:51<00:24,  1.53it/s][1,0]<stderr>:#015 88%|████████▊ | 271/307 [02:52<00:23,  1.52it/s][1,0]<stderr>:#015 89%|████████▊ | 272/307 [02:52<00:22,  1.54it/s][1,0]<stderr>:#015 89%|████████▉ | 273/307 [02:53<00:21,  1.57it/s][1,0]<stderr>:#015 89%|████████▉ | 274/307 [02:54<00:21,  1.55it/s][1,0]<stderr>:#015 90%|████████▉ | 275/307 [02:54<00:20,  1.55it/s][1,0]<stderr>:#015 90%|████████▉ | 276/307 [02:55<00:20,  1.54it/s][1,0]<stderr>:#015 90%|█████████ | 277/307 [02:56<00:19,  1.54it/s][1,0]<stderr>:#015 91%|█████████ | 278/307 [02:56<00:18,  1.55it/s][1,0]<stderr>:#015 91%|█████████ | 279/307 [02:57<00:18,  1.54it/s][1,0]<stderr>:#015 91%|█████████ | 280/307 [02:58<00:17,  1.53it/s][1,0]<stderr>:#015 92%|█████████▏| 281/307 [02:58<00:16,  1.54it/s][1,0]<stderr>:#015 92%|█████████▏| 282/307 [02:59<00:16,  1.55it/s][1,0]<stderr>:#015 92%|█████████▏| 283/307 [03:00<00:15,  1.55it/s][1,0]<stderr>:#015 93%|█████████▎| 284/307 [03:00<00:14,  1.54it/s][1,0]<stderr>:#015 93%|█████████▎| 285/307 [03:01<00:14,  1.56it/s][1,0]<stderr>:#015 93%|█████████▎| 286/307 [03:01<00:13,  1.55it/s][1,0]<stderr>:#015 93%|█████████▎| 287/307 [03:02<00:13,  1.52it/s][1,0]<stderr>:#015 94%|█████████▍| 288/307 [03:03<00:12,  1.51it/s][1,0]<stderr>:#015 94%|█████████▍| 289/307 [03:03<00:11,  1.54it/s][1,0]<stderr>:#015 94%|█████████▍| 290/307 [03:04<00:11,  1.53it/s][1,0]<stderr>:#015 95%|█████████▍| 291/307 [03:05<00:10,  1.53it/s][1,0]<stderr>:#015 95%|█████████▌| 292/307 [03:05<00:09,  1.53it/s][1,0]<stderr>:#015 95%|█████████▌| 293/307 [03:06<00:09,  1.51it/s][1,0]<stderr>:#015 96%|█████████▌| 294/307 [03:07<00:08,  1.54it/s][1,0]<stderr>:#015 96%|█████████▌| 295/307 [03:07<00:07,  1.54it/s][1,0]<stderr>:#015 96%|█████████▋| 296/307 [03:08<00:07,  1.57it/s][1,0]<stderr>:#015 97%|█████████▋| 297/307 [03:09<00:06,  1.58it/s][1,0]<stderr>:#015 97%|█████████▋| 298/307 [03:09<00:05,  1.60it/s][1,0]<stderr>:#015 97%|█████████▋| 299/307 [03:10<00:05,  1.58it/s][1,0]<stderr>:#015 98%|█████████▊| 300/307 [03:10<00:04,  1.60it/s][1,0]<stderr>:#015 98%|█████████▊| 301/307 [03:11<00:03,  1.59it/s][1,0]<stderr>:#015 98%|█████████▊| 302/307 [03:12<00:03,  1.58it/s][1,0]<stderr>:#015 99%|█████████▊| 303/307 [03:12<00:02,  1.58it/s][1,0]<stderr>:#015 99%|█████████▉| 304/307 [03:13<00:01,  1.58it/s][1,0]<stderr>:#015 99%|█████████▉| 305/307 [03:14<00:01,  1.58it/s][1,0]<stderr>:#015100%|█████████▉| 306/307 [03:14<00:00,  1.59it/s][1,0]<stderr>:#015100%|██████████| 307/307 [03:15<00:00,  1.60it/s][1,0]<stderr>:INFO:__main__:***** Predict results mnli *****\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[INFO|trainer.py:516] 2022-05-02 14:08:26,758 >> The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[INFO|trainer.py:2115] 2022-05-02 14:08:26,762 >> ***** Running Prediction *****\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[INFO|trainer.py:2117] 2022-05-02 14:08:26,762 >>   Num examples = 9847\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[INFO|trainer.py:2120] 2022-05-02 14:08:26,762 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[INFO|trainer.py:516] 2022-05-02 14:08:26,780 >> The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[INFO|trainer.py:2115] 2022-05-02 14:08:26,784 >> ***** Running Prediction *****\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[INFO|trainer.py:2117] 2022-05-02 14:08:26,784 >>   Num examples = 9847\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[INFO|trainer.py:2120] 2022-05-02 14:08:26,784 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[INFO|trainer.py:516] 2022-05-02 14:08:26,807 >> The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: premise, idx, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[INFO|trainer.py:2115] 2022-05-02 14:08:26,811 >> ***** Running Prediction *****\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[INFO|trainer.py:2117] 2022-05-02 14:08:26,811 >>   Num examples = 9847\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[INFO|trainer.py:2120] 2022-05-02 14:08:26,811 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|trainer.py:516] 2022-05-02 14:08:26,827 >> The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: hypothesis, premise, idx.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|trainer.py:2115] 2022-05-02 14:08:26,831 >> ***** Running Prediction *****\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|trainer.py:2117] 2022-05-02 14:08:26,831 >>   Num examples = 9847\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[INFO|trainer.py:2120] 2022-05-02 14:08:26,831 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[INFO|trainer.py:516] 2022-05-02 14:08:26,856 >> The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: premise, hypothesis, idx.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[INFO|trainer.py:2115] 2022-05-02 14:08:26,860 >> ***** Running Prediction *****\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[INFO|trainer.py:2117] 2022-05-02 14:08:26,861 >>   Num examples = 9847\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[INFO|trainer.py:2120] 2022-05-02 14:08:26,861 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:[INFO|trainer.py:516] 2022-05-02 14:08:26,879 >> The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:[INFO|trainer.py:2115] 2022-05-02 14:08:26,882 >> ***** Running Prediction *****\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:[INFO|trainer.py:2117] 2022-05-02 14:08:26,882 >>   Num examples = 9847\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:[INFO|trainer.py:2120] 2022-05-02 14:08:26,882 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:[INFO|trainer.py:516] 2022-05-02 14:08:26,909 >> The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: premise, idx, hypothesis.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:[INFO|trainer.py:2115] 2022-05-02 14:08:26,914 >> ***** Running Prediction *****\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:[INFO|trainer.py:2117] 2022-05-02 14:08:26,914 >>   Num examples = 9847\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:[INFO|trainer.py:2120] 2022-05-02 14:08:26,914 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:516] 2022-05-02 14:08:26,914 >> The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: hypothesis, idx, premise.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:2115] 2022-05-02 14:08:26,919 >> ***** Running Prediction *****\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:2117] 2022-05-02 14:08:26,919 >>   Num examples = 9847\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:2120] 2022-05-02 14:08:26,919 >>   Batch size = 16\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015308it [03:16,  1.27it/s]                         [1,0]<stderr>:#015309it [03:17,  1.33it/s][1,0]<stderr>:#015310it [03:17,  1.39it/s][1,0]<stderr>:#015311it [03:18,  1.44it/s][1,0]<stderr>:#015312it [03:19,  1.48it/s][1,0]<stderr>:#015313it [03:19,  1.50it/s][1,0]<stderr>:#015314it [03:20,  1.50it/s][1,0]<stderr>:#015315it [03:21,  1.52it/s][1,0]<stderr>:#015316it [03:21,  1.54it/s][1,0]<stderr>:#015317it [03:22,  1.56it/s][1,0]<stderr>:#015318it [03:22,  1.56it/s][1,0]<stderr>:#015319it [03:23,  1.57it/s][1,0]<stderr>:#015320it [03:24,  1.54it/s][1,0]<stderr>:#015321it [03:24,  1.55it/s][1,0]<stderr>:#015322it [03:25,  1.55it/s][1,0]<stderr>:#015323it [03:26,  1.55it/s][1,0]<stderr>:#015324it [03:26,  1.56it/s][1,0]<stderr>:#015325it [03:27,  1.53it/s][1,0]<stderr>:#015326it [03:28,  1.56it/s][1,0]<stderr>:#015327it [03:28,  1.57it/s][1,0]<stderr>:#015328it [03:29,  1.58it/s][1,0]<stderr>:#015329it [03:30,  1.57it/s][1,0]<stderr>:#015330it [03:30,  1.59it/s][1,0]<stderr>:#015331it [03:31,  1.57it/s][1,0]<stderr>:#015332it [03:31,  1.59it/s][1,0]<stderr>:#015333it [03:32,  1.60it/s][1,0]<stderr>:#015334it [03:33,  1.61it/s][1,0]<stderr>:#015335it [03:33,  1.61it/s][1,0]<stderr>:#015336it [03:34,  1.61it/s][1,0]<stderr>:#015337it [03:35,  1.61it/s][1,0]<stderr>:#015338it [03:35,  1.58it/s][1,0]<stderr>:#015339it [03:36,  1.56it/s][1,0]<stderr>:#015340it [03:36,  1.56it/s][1,0]<stderr>:#015341it [03:37,  1.56it/s][1,0]<stderr>:#015342it [03:38,  1.55it/s][1,0]<stderr>:#015343it [03:38,  1.55it/s][1,0]<stderr>:#015344it [03:39,  1.54it/s][1,0]<stderr>:#015345it [03:40,  1.54it/s][1,0]<stderr>:#015346it [03:40,  1.56it/s][1,0]<stderr>:#015347it [03:41,  1.55it/s][1,0]<stderr>:#015348it [03:42,  1.58it/s][1,0]<stderr>:#015349it [03:42,  1.56it/s][1,0]<stderr>:#015350it [03:43,  1.57it/s][1,0]<stderr>:#015351it [03:44,  1.59it/s][1,0]<stderr>:#015352it [03:44,  1.60it/s][1,0]<stderr>:#015353it [03:45,  1.56it/s][1,0]<stderr>:#015354it [03:45,  1.57it/s][1,0]<stderr>:#015355it [03:46,  1.58it/s][1,0]<stderr>:#015356it [03:47,  1.56it/s][1,0]<stderr>:#015357it [03:47,  1.56it/s][1,0]<stderr>:#015358it [03:48,  1.57it/s][1,0]<stderr>:#015359it [03:49,  1.57it/s][1,0]<stderr>:#015360it [03:49,  1.57it/s][1,0]<stderr>:#015361it [03:50,  1.57it/s][1,0]<stderr>:#015362it [03:51,  1.57it/s][1,0]<stderr>:#015363it [03:51,  1.57it/s][1,0]<stderr>:#015364it [03:52,  1.58it/s][1,0]<stderr>:#015365it [03:52,  1.58it/s][1,0]<stderr>:#015366it [03:53,  1.58it/s][1,0]<stderr>:#015367it [03:54,  1.59it/s][1,0]<stderr>:#015368it [03:54,  1.58it/s][1,0]<stderr>:#015369it [03:55,  1.60it/s][1,0]<stderr>:#015370it [03:56,  1.59it/s][1,0]<stderr>:#015371it [03:56,  1.58it/s][1,0]<stderr>:#015372it [03:57,  1.58it/s][1,0]<stderr>:#015373it [03:57,  1.58it/s][1,0]<stderr>:#015374it [03:58,  1.58it/s][1,0]<stderr>:#015375it [03:59,  1.57it/s][1,0]<stderr>:#015376it [03:59,  1.60it/s][1,0]<stderr>:#015377it [04:00,  1.58it/s][1,0]<stderr>:#015378it [04:01,  1.59it/s][1,0]<stderr>:#015379it [04:01,  1.57it/s][1,0]<stderr>:#015380it [04:02,  1.57it/s][1,0]<stderr>:#015381it [04:03,  1.59it/s][1,0]<stderr>:#015382it [04:03,  1.59it/s][1,0]<stderr>:#015383it [04:04,  1.59it/s][1,0]<stderr>:#015384it [04:04,  1.59it/s][1,0]<stderr>:#015385it [04:05,  1.61it/s][1,0]<stderr>:#015386it [04:06,  1.58it/s][1,0]<stderr>:#015387it [04:06,  1.57it/s][1,0]<stderr>:#015388it [04:07,  1.57it/s][1,0]<stderr>:#015389it [04:08,  1.56it/s][1,0]<stderr>:#015390it [04:08,  1.54it/s][1,0]<stderr>:#015391it [04:09,  1.55it/s][1,0]<stderr>:#015392it [04:10,  1.56it/s][1,0]<stderr>:#015393it [04:10,  1.57it/s][1,0]<stderr>:#015394it [04:11,  1.59it/s][1,0]<stderr>:#015395it [04:11,  1.59it/s][1,0]<stderr>:#015396it [04:12,  1.58it/s][1,0]<stderr>:#015397it [04:13,  1.55it/s][1,0]<stderr>:#015398it [04:13,  1.56it/s][1,0]<stderr>:#015399it [04:14,  1.57it/s][1,0]<stderr>:#015400it [04:15,  1.58it/s][1,0]<stderr>:#015401it [04:15,  1.57it/s][1,0]<stderr>:#015402it [04:16,  1.58it/s][1,0]<stderr>:#015403it [04:17,  1.56it/s][1,0]<stderr>:#015404it [04:17,  1.55it/s][1,0]<stderr>:#015405it [04:18,  1.52it/s][1,0]<stderr>:#015406it [04:19,  1.54it/s][1,0]<stderr>:#015407it [04:19,  1.52it/s][1,0]<stderr>:#015408it [04:20,  1.52it/s][1,0]<stderr>:#015409it [04:20,  1.54it/s][1,0]<stderr>:#015410it [04:21,  1.56it/s][1,0]<stderr>:#015411it [04:22,  1.57it/s][1,0]<stderr>:#015412it [04:22,  1.59it/s][1,0]<stderr>:#015413it [04:23,  1.59it/s][1,0]<stderr>:#015414it [04:24,  1.58it/s][1,0]<stderr>:#015415it [04:24,  1.58it/s][1,0]<stderr>:#015416it [04:25,  1.58it/s][1,0]<stderr>:#015417it [04:26,  1.58it/s][1,0]<stderr>:#015418it [04:26,  1.59it/s][1,0]<stderr>:#015419it [04:27,  1.58it/s][1,0]<stderr>:#015420it [04:27,  1.55it/s][1,0]<stderr>:#015421it [04:28,  1.54it/s][1,0]<stderr>:#015422it [04:29,  1.57it/s][1,0]<stderr>:#015423it [04:29,  1.59it/s][1,0]<stderr>:#015424it [04:30,  1.57it/s][1,0]<stderr>:#015425it [04:31,  1.57it/s][1,0]<stderr>:#015426it [04:31,  1.59it/s][1,0]<stderr>:#015427it [04:32,  1.57it/s][1,0]<stderr>:#015428it [04:32,  1.59it/s][1,0]<stderr>:#015429it [04:33,  1.58it/s][1,0]<stderr>:#015430it [04:34,  1.59it/s][1,0]<stderr>:#015431it [04:34,  1.60it/s][1,0]<stderr>:#015432it [04:35,  1.59it/s][1,0]<stderr>:#015433it [04:36,  1.60it/s][1,0]<stderr>:#015434it [04:36,  1.57it/s][1,0]<stderr>:#015435it [04:37,  1.59it/s][1,0]<stderr>:#015436it [04:38,  1.60it/s][1,0]<stderr>:#015437it [04:38,  1.59it/s][1,0]<stderr>:#015438it [04:39,  1.60it/s][1,0]<stderr>:#015439it [04:39,  1.60it/s][1,0]<stderr>:#015440it [04:40,  1.58it/s][1,0]<stderr>:#015441it [04:41,  1.56it/s][1,0]<stderr>:#015442it [04:41,  1.57it/s][1,0]<stderr>:#015443it [04:42,  1.57it/s][1,0]<stderr>:#015444it [04:43,  1.56it/s][1,0]<stderr>:#015445it [04:43,  1.58it/s][1,0]<stderr>:#015446it [04:44,  1.59it/s][1,0]<stderr>:#015447it [04:44,  1.60it/s][1,0]<stderr>:#015448it [04:45,  1.58it/s][1,0]<stderr>:#015449it [04:46,  1.57it/s][1,0]<stderr>:#015450it [04:46,  1.59it/s][1,0]<stderr>:#015451it [04:47,  1.58it/s][1,0]<stderr>:#015452it [04:48,  1.58it/s][1,0]<stderr>:#015453it [04:48,  1.58it/s][1,0]<stderr>:#015454it [04:49,  1.57it/s][1,0]<stderr>:#015455it [04:50,  1.55it/s][1,0]<stderr>:#015456it [04:50,  1.54it/s][1,0]<stderr>:#015457it [04:51,  1.55it/s][1,0]<stderr>:#015458it [04:52,  1.54it/s][1,0]<stderr>:#015459it [04:52,  1.56it/s][1,0]<stderr>:#015460it [04:53,  1.55it/s][1,0]<stderr>:#015461it [04:53,  1.57it/s][1,0]<stderr>:#015462it [04:54,  1.55it/s][1,0]<stderr>:#015463it [04:55,  1.54it/s][1,0]<stderr>:#015464it [04:55,  1.56it/s][1,0]<stderr>:#015465it [04:56,  1.56it/s][1,0]<stderr>:#015466it [04:57,  1.57it/s][1,0]<stderr>:#015467it [04:57,  1.58it/s][1,0]<stderr>:#015468it [04:58,  1.59it/s][1,0]<stderr>:#015469it [04:59,  1.58it/s][1,0]<stderr>:#015470it [04:59,  1.58it/s][1,0]<stderr>:#015471it [05:00,  1.59it/s][1,0]<stderr>:#015472it [05:00,  1.60it/s][1,0]<stderr>:#015473it [05:01,  1.60it/s][1,0]<stderr>:#015474it [05:02,  1.58it/s][1,0]<stderr>:#015475it [05:02,  1.61it/s][1,0]<stderr>:#015476it [05:03,  1.59it/s][1,0]<stderr>:#015477it [05:04,  1.56it/s][1,0]<stderr>:#015478it [05:04,  1.57it/s][1,0]<stderr>:#015479it [05:05,  1.56it/s][1,0]<stderr>:#015480it [05:06,  1.57it/s][1,0]<stderr>:#015481it [05:06,  1.55it/s][1,0]<stderr>:#015482it [05:07,  1.55it/s][1,0]<stderr>:#015483it [05:07,  1.56it/s][1,0]<stderr>:#015484it [05:08,  1.56it/s][1,0]<stderr>:#015485it [05:09,  1.55it/s][1,0]<stderr>:#015486it [05:09,  1.56it/s][1,0]<stderr>:#015487it [05:10,  1.57it/s][1,0]<stderr>:#015488it [05:11,  1.57it/s][1,0]<stderr>:#015489it [05:11,  1.58it/s][1,0]<stderr>:#015490it [05:12,  1.58it/s][1,0]<stderr>:#015491it [05:13,  1.59it/s][1,0]<stderr>:#015492it [05:13,  1.58it/s][1,0]<stderr>:#015493it [05:14,  1.58it/s][1,0]<stderr>:#015494it [05:14,  1.57it/s][1,0]<stderr>:#015495it [05:15,  1.59it/s][1,0]<stderr>:#015496it [05:16,  1.59it/s][1,0]<stderr>:#015497it [05:16,  1.52it/s][1,0]<stderr>:#015498it [05:17,  1.55it/s][1,0]<stderr>:#015499it [05:18,  1.56it/s][1,0]<stderr>:#015500it [05:18,  1.54it/s][1,0]<stderr>:#015501it [05:19,  1.55it/s][1,0]<stderr>:#015502it [05:20,  1.52it/s][1,0]<stderr>:#015503it [05:20,  1.53it/s][1,0]<stderr>:#015504it [05:21,  1.54it/s][1,0]<stderr>:#015505it [05:22,  1.56it/s][1,0]<stderr>:#015506it [05:22,  1.54it/s][1,0]<stderr>:#015507it [05:23,  1.56it/s][1,0]<stderr>:#015508it [05:23,  1.57it/s][1,0]<stderr>:#015509it [05:24,  1.58it/s][1,0]<stderr>:#015510it [05:25,  1.58it/s][1,0]<stderr>:#015511it [05:25,  1.57it/s][1,0]<stderr>:#015512it [05:26,  1.56it/s][1,0]<stderr>:#015513it [05:27,  1.57it/s][1,0]<stderr>:#015514it [05:27,  1.59it/s][1,0]<stderr>:#015515it [05:28,  1.60it/s][1,0]<stderr>:#015516it [05:28,  1.60it/s][1,0]<stderr>:#015517it [05:29,  1.61it/s][1,0]<stderr>:#015518it [05:30,  1.62it/s][1,0]<stderr>:#015519it [05:30,  1.58it/s][1,0]<stderr>:#015520it [05:31,  1.59it/s][1,0]<stderr>:#015521it [05:32,  1.58it/s][1,0]<stderr>:#015522it [05:32,  1.58it/s][1,0]<stderr>:#015523it [05:33,  1.59it/s][1,0]<stderr>:#015524it [05:34,  1.58it/s][1,0]<stderr>:#015525it [05:34,  1.60it/s][1,0]<stderr>:#015526it [05:35,  1.60it/s][1,0]<stderr>:#015527it [05:35,  1.58it/s][1,0]<stderr>:#015528it [05:36,  1.56it/s][1,0]<stderr>:#015529it [05:37,  1.57it/s][1,0]<stderr>:#015530it [05:37,  1.58it/s][1,0]<stderr>:#015531it [05:38,  1.59it/s][1,0]<stderr>:#015532it [05:39,  1.59it/s][1,0]<stderr>:#015533it [05:39,  1.61it/s][1,0]<stderr>:#015534it [05:40,  1.60it/s][1,0]<stderr>:#015535it [05:40,  1.61it/s][1,0]<stderr>:#015536it [05:41,  1.57it/s][1,0]<stderr>:#015537it [05:42,  1.57it/s][1,0]<stderr>:#015538it [05:42,  1.58it/s][1,0]<stderr>:#015539it [05:43,  1.57it/s][1,0]<stderr>:#015540it [05:44,  1.54it/s][1,0]<stderr>:#015541it [05:44,  1.55it/s][1,0]<stderr>:#015542it [05:45,  1.54it/s][1,0]<stderr>:#015543it [05:46,  1.54it/s][1,0]<stderr>:#015544it [05:46,  1.55it/s][1,0]<stderr>:#015545it [05:47,  1.56it/s][1,0]<stderr>:#015546it [05:48,  1.54it/s][1,0]<stderr>:#015547it [05:48,  1.54it/s][1,0]<stderr>:#015548it [05:49,  1.55it/s][1,0]<stderr>:#015549it [05:50,  1.56it/s][1,0]<stderr>:#015550it [05:50,  1.54it/s][1,0]<stderr>:#015551it [05:51,  1.56it/s][1,0]<stderr>:#015552it [05:51,  1.57it/s][1,0]<stderr>:#015553it [05:52,  1.56it/s][1,0]<stderr>:#015554it [05:53,  1.55it/s][1,0]<stderr>:#015555it [05:53,  1.55it/s][1,0]<stderr>:#015556it [05:54,  1.57it/s][1,0]<stderr>:#015557it [05:55,  1.57it/s][1,0]<stderr>:#015558it [05:55,  1.58it/s][1,0]<stderr>:#015559it [05:56,  1.59it/s][1,0]<stderr>:#015560it [05:57,  1.56it/s][1,0]<stderr>:#015561it [05:57,  1.57it/s][1,0]<stderr>:#015562it [05:58,  1.58it/s][1,0]<stderr>:#015563it [05:58,  1.58it/s][1,0]<stderr>:#015564it [05:59,  1.57it/s][1,0]<stderr>:#015565it [06:00,  1.56it/s][1,0]<stderr>:#015566it [06:00,  1.58it/s][1,0]<stderr>:#015567it [06:01,  1.55it/s][1,0]<stderr>:#015568it [06:02,  1.56it/s][1,0]<stderr>:#015569it [06:02,  1.58it/s][1,0]<stderr>:#015570it [06:03,  1.58it/s][1,0]<stderr>:#015571it [06:04,  1.57it/s][1,0]<stderr>:#015572it [06:04,  1.57it/s][1,0]<stderr>:#015573it [06:05,  1.57it/s][1,0]<stderr>:#015574it [06:05,  1.57it/s][1,0]<stderr>:#015575it [06:06,  1.54it/s][1,0]<stderr>:#015576it [06:07,  1.56it/s][1,0]<stderr>:#015577it [06:07,  1.58it/s][1,0]<stderr>:#015578it [06:08,  1.59it/s][1,0]<stderr>:#015579it [06:09,  1.54it/s][1,0]<stderr>:#015580it [06:09,  1.53it/s][1,0]<stderr>:#015581it [06:10,  1.55it/s][1,0]<stderr>:#015582it [06:11,  1.55it/s][1,0]<stderr>:#015583it [06:11,  1.54it/s][1,0]<stderr>:#015584it [06:12,  1.54it/s][1,0]<stderr>:#015585it [06:13,  1.55it/s][1,0]<stderr>:#015586it [06:13,  1.57it/s][1,0]<stderr>:#015587it [06:14,  1.57it/s][1,0]<stderr>:#015588it [06:14,  1.55it/s][1,0]<stderr>:#015589it [06:15,  1.56it/s][1,0]<stderr>:#015590it [06:16,  1.58it/s][1,0]<stderr>:#015591it [06:16,  1.57it/s][1,0]<stderr>:#015592it [06:17,  1.55it/s][1,0]<stderr>:#015593it [06:18,  1.56it/s][1,0]<stderr>:#015594it [06:18,  1.58it/s][1,0]<stderr>:#015595it [06:19,  1.58it/s][1,0]<stderr>:#015596it [06:20,  1.59it/s][1,0]<stderr>:#015597it [06:20,  1.59it/s][1,0]<stderr>:#015598it [06:21,  1.58it/s][1,0]<stderr>:#015599it [06:21,  1.58it/s][1,0]<stderr>:#015600it [06:22,  1.58it/s][1,0]<stderr>:#015601it [06:23,  1.59it/s][1,0]<stderr>:#015602it [06:23,  1.59it/s][1,0]<stderr>:#015603it [06:24,  1.59it/s][1,0]<stderr>:#015604it [06:25,  1.58it/s][1,0]<stderr>:#015605it [06:25,  1.58it/s][1,0]<stderr>:#015606it [06:26,  1.59it/s][1,0]<stderr>:#015607it [06:26,  1.60it/s][1,0]<stderr>:#015608it [06:27,  1.58it/s][1,0]<stderr>:#015609it [06:28,  1.57it/s][1,0]<stderr>:#015610it [06:28,  1.58it/s][1,0]<stderr>:#015611it [06:29,  1.57it/s][1,0]<stderr>:#015612it [06:30,  1.57it/s][1,0]<stderr>:#015613it [06:30,  1.58it/s][1,0]<stderr>:#015614it [06:31,  1.58it/s][1,0]<stderr>:#015615it [06:32,  1.58it/s][1,0]<stderr>:INFO:__main__:***** Predict results mnli-mm *****\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015615it [06:32,  1.57it/s]\u001b[0m\n",
      "\n",
      "2022-05-02 14:24:14 Completed - Training job completed\n",
      "Training seconds: 2561\n",
      "Billable seconds: 2561\n"
     ]
    }
   ],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the endpoint\n",
    "\n",
    "To deploy our endpoint, we call `deploy()` on our HuggingFace estimator object, passing in our desired number of instances and instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = huggingface_estimator.deploy(1,\"ml.g4dn.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use the returned predictor object to call the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_input= {\"inputs\":\"I love using the new Inference DLC.\"}\n",
    "\n",
    "predictor.predict(sentiment_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we delete the endpoint again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimator Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# container image used for training job\n",
    "print(f\"container image used for training job: \\n{huggingface_estimator.image_uri}\\n\")\n",
    "\n",
    "# s3 uri where the trained model is located\n",
    "print(f\"s3 uri where the trained model is located: \\n{huggingface_estimator.model_data}\\n\")\n",
    "\n",
    "# latest training job name for this estimator\n",
    "print(f\"latest training job name for this estimator: \\n{huggingface_estimator.latest_training_job.name}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access the logs of the training job\n",
    "huggingface_estimator.sagemaker_session.logs_for_job(huggingface_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attach to old training job to an estimator \n",
    "\n",
    "In Sagemaker you can attach an old training job to an estimator to continue training, get results etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "# job which is going to be attached to the estimator\n",
    "old_training_job_name=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-01-15 19:31:50 Starting - Preparing the instances for training\n",
      "2021-01-15 19:31:50 Downloading - Downloading input data\n",
      "2021-01-15 19:31:50 Training - Training image download completed. Training in progress.\n",
      "2021-01-15 19:31:50 Uploading - Uploading generated training model\n",
      "2021-01-15 19:31:50 Completed - Training job completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s3://philipps-sagemaker-bucket-eu-central-1/huggingface-sdk-extension-2021-01-15-19-14-13-725/output/model.tar.gz'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attach old training job\n",
    "huggingface_estimator_loaded = Estimator.attach(old_training_job_name)\n",
    "\n",
    "# get model output s3 from training job\n",
    "huggingface_estimator_loaded.model_data"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
